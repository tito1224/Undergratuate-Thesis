---
title: "Final"
author: "Tito"
date: "2023-02-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Results

```{r}
# in case issues with installation of gExtras:
# https://stackoverflow.com/questions/42807247/installing-package-cannot-open-file-permission-denied
# https://stackoverflow.com/questions/60451908/tidyverse-not-loaded-it-says-namespace-vctrs-0-2-0-is-already-loaded-but

library(tidyverse)
library(gtools)
library(data.table)
library(matrixStats)
library(RMark)
library(stringi)
library(rlang)
library(gt)
library(gtExtras)
library(webshot2)
library(knitr)
library(readxl)
```

Load functions from the other R scripts.
```{r}
source("./Functions/generatePointCount.R")
source("./Functions/runSimulationFunctions.R")
```


Load data - don't touch this. Temporarily set the working directory to load the data - will automatically return to current working directory after this chunk runs. The data in this folder is for 100 unique scenarios, with 1000 replicates. The scenarios are shown later on in the code and is called the "params" dataframe. The scenarios used to generate this data can also be found in the "./Functions/runSimulation.R" file. 
```{r}
setwd("./DigitalAllianceOutput/Huggins")
dataFinalTrial = list.files(pattern = ".rds") # load rds files
dataFinalTrial = mixedsort(dataFinalTrial)%>% # use mixedsort so it appears in ascending order 
  map(readRDS)

# initialize empty dataframes to store the data
dfSummarisedFinalH= data.frame()
dfEstimatesFinalH = data.frame()
dfHistoryFinalH = data.frame()
#counter = 1

# loop through the rds files and retrive data 
for (item in dataFinalTrial){
  tempSummarisedResults = item[[1]] # retrieve the summary stats; this df is useless because extreme values have not been removed; ignore
  tempCalculations = item[[2]] # load estimates for each run (should be 200,000 rows - 100,000 estimates of N, and 100,000 estimates of p)
  tempHistory= item[[3]] # load the detection histories used to generate the histories 

  
  # add data points to final dataframe
  dfSummarisedFinalH = rbind(dfSummarisedFinalH,tempSummarisedResults)
  dfEstimatesFinalH = rbind(dfEstimatesFinalH, tempCalculations)
  dfHistoryFinalH = rbind(dfHistoryFinalH, tempHistory)
  #counter = counter + 1
}
```

# Outliers

Some code to add identifiers to the data. Combination number will identify the variables used to create the scenario - or just "scenario". Simulation number will identify the run number - or datapoints actually generated. There are 1000 simulation numbers for every combination number! The variable `params` is created in one of the code chunks below, and shows the variables associated with each combination number.

```{r}
# id is combinationNumber_simulationNumber
dfEstimatesFinalH$rowid = 1:nrow(dfEstimatesFinalH)
dfEstimatesFinalH$id = paste0(dfEstimatesFinalH$combinationNumber,"_",dfEstimatesFinalH$simulationNumber)
dfHistoryFinalH$id = paste0(dfHistoryFinalH$combinationNumber,"_",dfHistoryFinalH$simulationNumber)
dfEstimatesFinalHUnClean = copy(dfEstimatesFinalH)
```

26 cases did not run because no detections were made, which is fair.
```{r}
dfOutliers = dfEstimatesFinalHUnClean %>%
  filter(variable == "N") %>%
  filter(estimate < 0 | is.na(estimate))
nOutliers = nrow(dfOutliers)
nEstimates = nrow(dfEstimatesFinalHUnClean[which(dfEstimatesFinalHUnClean$variable=="N"),])
print(nOutliers)
print(nEstimates)
print(nOutliers/nEstimates)
dfOutliers
```

The params dataset represents the scenarios/environment used to simulate data for each combination number. Add it to the dataset of final estimates to help calculate summary statistics.
```{r}
# this code is what was used in the ./Functions/runSimulation.R file to generate scenarios to input into the HPC 
nRuns = 1000
lstNi = c(10,20)
lstP = c(0.1,0.2,0.3,0.4,0.5)
#lstAlpha = c(0)
lstAlpha =c(0,0.05,0.1,0.2,0.3)
lstMaxMin = c(5,10)
lstFormula = c("~1")
lstMixtures = c(1)
lstSeed = c("NULL")
strModel = "Huggins"

params = expand.grid(nRuns,lstNi,lstP, lstAlpha, lstMaxMin,lstFormula,lstMixtures,lstSeed,strModel)
colnames(params) = c("nRuns","N","p_1m","alpha","maxMinute","strFormula","lstMixtures","lstSeed","strModel")
params$Scenario = 1:nrow(params)

# merge the params dataframe with the dataframe of estimates to help with calculating summary statistics 
dfEstimatesFinalH = left_join(dfEstimatesFinalH,params,by=c("combinationNumber"="Scenario")) 

# some code to help with calculating summary statistics -> full summary not done yet, but these are intermediate steps needed 
# to determine things like coverage probability, MSE, etc
dfEstimatesFinalH$bCoverage = ifelse(dfEstimatesFinalH$trueValue >= dfEstimatesFinalH$lcl & dfEstimatesFinalH$trueValue <= dfEstimatesFinalH$ucl,1,0)
dfEstimatesFinalH$squaredError = (dfEstimatesFinalH$estimate - dfEstimatesFinalH$trueValues)^2
dfEstimatesFinalH$width = dfEstimatesFinalH$ucl - dfEstimatesFinalH$lcl
  
```

Create a dataframe of summarized variables. This is before outliers have been removed.
```{r}
dfSummarisedFinalH = dfEstimatesFinalH %>%
  filter(variable == "N")%>% # only focus on N? or also use p??
  group_by(p_1m,alpha,N,maxMinute,strFormula,combinationNumber)%>%
  summarise(AvgNhat = mean(estimate,na.rm=TRUE),
            AvgNhatSE= mean(se,na.rm=TRUE),
            sdNhat = sd(estimate,na.rm=TRUE),
            mse = mean(squaredError,na.rm=TRUE),
            bias_SE_Nhat = AvgNhatSE - sdNhat,
            coverage = sum(bCoverage,na.rm=TRUE)/sum(!is.na(estimate)), # changed this from n() to sum(!is.na()) because I have some NA values for my estimates
            avgWidth = mean(width,na.rm=TRUE),
            avgAIC = mean(AIC,na.rm=TRUE))
```

Lack of removing extreme estimates is seen here - work to remove outliers is done in the next section!
```{r}
head(dfSummarisedFinalH)
```

Quick check to see which scenarios caused the capture histories where no detections were made. These are scenarios with low detection probabilities, and short time intervals for the most part, and also a low number of total individuals (N) in the population.
```{r}
lstNoDetect = unique(dfOutliers$combinationNumber)
dfSummarisedFinalH[dfSummarisedFinalH$combinationNumber%in% lstNoDetect,]
```

## Likelihood

One thing that can cause huge estimates is when for every individual detected, they are only detected once across all capture occasions. The number of individuals detected does not have to equal the number of individuals in the population. The reason this happens is because of how the likelihood is structured. I have written some code below to generate likelihoods based on capture occasions, and it will illustrate how maximizing the conditional log-likelihood leads to erroneous values.

The likelihood in general is

$L(N,p|\underline{n}) = \frac{N!}{\prod_{k=0}^{K} n_k}\pi_k^{n_k}$

-   K is the total number of capture occasions, and k represents the k-th capture occasion
-   N is the total population
-   $\pi_k = p^k(1-p)^{K-k}$ is the probability that an individual is captured k times
-   p represents the per minute detection probability
-   $n_k$ represents the number of individuals captured k times

The conditional likelihood is:

$L(p|n_{obs}) = \frac{\sum_{k=1}^{K} n_k}{\prod_{k=1}^{K} n_k}\prod_{k=1}^{K} \left(\frac{(1-p)^{K-k}p^k}{1-(1-p)^K}\right)^{n_k}$

-   It is not possible to maximize the full likelihood because we do not know what the value of N (true population) is

-   Instead we maximize the conditional likelihood

-   Once $\hat{p}$ has been found, we use the Horvitz-Thompson estimator to find the estimate of $\hat{N}$:

$\hat{N} = \frac{\sum_{k=1}^{K}n_k}{1-(1-p)^K}$

$L(p|n_{obs}) \propto \left(\frac{(1-p)^{K-1}p}{1-(1-p)^K}\right)^{n_1}$

-   Finding the derivative of the conditional log likelihood and setting it equal to 0 gives us the expression:

$1-(1-p)^K - Kp=0$

-   This doesn't have a solution for values of p between 0 and 1. Instead as p goes to 0, the log likelihood becomes bigger, so the estimate of $\hat{p}$ ends up being a very small number
-   Using a small estimate of $\hat{p}$ with the Horvitz-Thompson estimator means we get large estimates of $\hat{N}$


```{r}
# N = total individuals in the population
# n_k = number of individuals counted k times; will be a vector that corresponds to the length of maxMinutes (ie capture occasions)
# p represents the 1 min detection probability
# K = # of capture occasions
funcConditionalLikelihood= function(p,n_k,K){
  
  # check to make sure number of individuals counted k times corresponds to the length of capture occasions
  if(length(n_k)!=K){
    return("length of n_k must equal K")
  }
  
  intTotalIndividualsCounted = sum(n_k)
  
  # calculate the factorial of the number of individuals counted k times
  lstFactorialn_k = numeric()
  for(i in n_k){
    tempval = factorial(i)
    lstFactorialn_k= c(lstFactorialn_k,tempval)
  }
  intFactorialn_k = prod(lstFactorialn_k)
 
  
  # calculate probabilities to use for individuals counted once
  lstFactorial_prob = numeric()
  for(i in 1:length(n_k)){
    tempprob = ((((1-p)^(K-i))*(p^i))/(1-((1-p)^K)))^(n_k[i])
    lstFactorial_prob = c(lstFactorial_prob,tempprob)
  }
  numFactorial_prob = prod(lstFactorial_prob)
  
  
  condLikelihood = log((intTotalIndividualsCounted/intFactorialn_k)*numFactorial_prob)

  return(condLikelihood)
  
}
```

The graph below shows us that in order to maximize the log likelihood, p needs to be extremely close to 0. The estimator for N (after taking the derivative of the conditional log likelihood wrt to p and setting equal to 0 to find an estimate of $\hat{p}$) is the Horvitz-Thompson estimator.
```{r}
#funcConditionalLikelihood(c(2,1,3), 0.1, 3)

ll_data = tibble(p = seq(0.1,1,0.001))
log_likelihood = sapply(ll_data$p, funcConditionalLikelihood, c(3,0,0,0,0),5)
ll_data$log_likelihood = log_likelihood

ggplot(ll_data)+
  geom_line(aes(x= p, y = log_likelihood))+ 
  theme_bw()+ 
  labs(title = "Conditional Log-Likelihood for 3 Individuals Detected Only Once", subtitle = "Capture Occasions = 5", x= "p", y = "Log-Likelihood")

#ggsave("./Images/likelihhod.jpg")
```

## Find Single Detections

This tells us that in 94% of the scenarios generated, at least one individual was detected more than once.
```{r}
colnamesUse= paste0("Minute",1:10)
dfHistoryFinalH$counts = rowSums(dfHistoryFinalH[,colnamesUse],na.rm=TRUE)
dfHistoryFinalH$bCounts = ifelse(dfHistoryFinalH$counts > 1,">1","=1")

# find out for each id (scenario_simulationNumber) how many individuals were counted only once and how many were recaptured
nMultDetect = dfHistoryFinalH %>%
  filter(counts>1)
nMultDetect = length(unique(nMultDetect$id))
nMultDetect
nMultDetect/nEstimates
```

Doing this just to make sure I'm filtering data right. Shows that in 5.38% of the generated scenarios, there are datasets with only 1 detection per individual. Pretty small number, so it is fair to remove them.
```{r}
nMultDetect = dfHistoryFinalH %>%
  filter(counts>1)

nSingleDetection = dfHistoryFinalH %>%
  filter(counts==1 & !(id %in% nMultDetect$id))

nSingleDetection = length(unique(nSingleDetection$id))
nSingleDetection
nSingleDetection/nEstimates
```

Store for usage later on.
```{r}
dfMultDetect = dfHistoryFinalH %>%
  filter(counts>1)

dfSingleDetection = dfHistoryFinalH %>%
  filter(counts==1 & !(id %in% dfMultDetect$id))
```

## Exploratory Analysis

Add some important data to the dfHistory dataframe for further analysis
```{r}
dfHistoryFinalH = left_join(dfHistoryFinalH,dfSummarisedFinalH[,1:6], by= "combinationNumber")
```

```{r}
head(dfHistoryFinalH)
```

Make a dataframe to store the number of **unique** individuals detected for each simulation number
```{r}
dfPointCounts = dfHistoryFinalH%>%
  group_by(p_1m,alpha,N,maxMinute,id)%>%
  summarise(PointCounts = n()) # find number of people counted at least once
head(dfPointCounts%>%
       arrange(desc(PointCounts)))
```

```{r}
dfPointCounts%>%
  filter(N == 20)%>%
ggplot()+
  geom_boxplot(aes(x=as.factor(p_1m), y = PointCounts, fill = as.factor(alpha)),coef=6)+
  geom_hline(aes(yintercept = 20),color="darkblue",linetype="dotted")+
  facet_wrap(~maxMinute)+
  theme_bw()+
  labs(title = "Apparent Number of Birds Encountered", subtitle = "Facetted by capture ocassions. Population N = 20.", x= "Per Minute Detection Probability", y = "Point Counts")+ 
  scale_fill_discrete(name = "Alpha")

ggsave("C:/Users/titom/OneDrive/Documents/Rcode/Undergratuate-Thesis/Images/PointCount_N_20.jpg")
```

```{r}
dfPointCounts%>%
  filter(N == 10)%>%
ggplot()+
  geom_boxplot(aes(x=as.factor(p_1m), y = PointCounts, fill = as.factor(alpha)),coef=100)+
  geom_hline(aes(yintercept = 10),color="darkblue",linetype="dotted")+
  facet_wrap(~maxMinute)+
  theme_bw()+
  labs(title = "Apparent Number of Birds Encountered", subtitle = "Facetted by capture ocassions. Population N = 10.", x= "Per Minute Detection Probability", y = "Point Counts")+ 
  scale_fill_discrete(name = "Alpha")

ggsave("C:/Users/titom/OneDrive/Documents/Rcode/Undergratuate-Thesis/Images/PointCount_N_10.jpg")
```

# Clean Data

```{r}
# filter out cases that did not run
dfEstimatesFinalH = dfEstimatesFinalHUnClean %>%
  filter(variable == "N") %>%
  filter(estimate>=0) %>%
  filter(!(id %in% dfSingleDetection$id))

dfEstimatesProbability = dfEstimatesFinalHUnClean %>% # these are probability estimates; not really needed for this project
  filter(variable == "p g1 t1") %>%
  filter(estimate>=0) %>%
  filter(!(id %in% dfSingleDetection$id))
```


```{r}
#add p,alpha,N, etc to help with graphing
dfSummarisedFinalH = dfSummarisedFinalH[!(duplicated(dfSummarisedFinalH$combinationNumber)),]
dfEstimatesFinalH = left_join(dfEstimatesFinalH, dfSummarisedFinalH[,1:6], by= "combinationNumber")
dfEstimatesProbability  =  left_join(dfEstimatesProbability, dfSummarisedFinalH[,1:6], by= "combinationNumber")
```

```{r}
head(dfEstimatesFinalH)
```

Make sure there are approximately equal data points across all scenarios. The scenarios with less than 500 data points by a significant amount are the ones with lowest detection probability and high alpha and/or low N. Makes sense because we end up with more cases of single detections here.
```{r}
dfEstimatesFinalH %>%
  filter(variable == "N")%>%
  group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
  summarise(Count= n())%>%
  arrange(Count)
```

# Plots

```{r}
dfEstimatesPop = dfEstimatesFinalH %>%
  filter(N==20)
```

Make sure they all have around the same data points - 600+ data points per scenario.
```{r}
dfEstimatesPop %>%
  filter(variable == "N")%>%
  group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
  summarise(Count= n())%>%
  arrange(Count)

```

Looking at how estimates varies as alpha increases when p_1m is fixed and time is fixed.

-   When the time interval is 5 mins, and detection probability is low, our estimates are closer to the true value in general; Even when we have a higher alpha, the shift in median estimate isn't as pronounced compared to when time = 10. This makes sense as small occasions means we have less time to make more mistakes, and small detection probability means we have detected less individuals to make mistakes on.

-   when alpha is 0.05, and time = 5, we have pretty accurate results, no matter how high p1m is.

-   With both time = 5 and time = 10, I notice that the IQR when alpha is high becomes smaller. **Why should this be the case?**

-   When time = 5, not much of a difference in median estimate for alpha =0.2 when p is 0.2 vs when p = 0.3. The marked difference is in the spread of values

-   When time = 10, why are the worst estimates when p = 0.1 and alpha= 0.2? Shouldn't the worst estimates be when p = 0.3 and alpha is 0.2?

    -   Actually because we have a small p and high alpha, we have a higher occurence of more single detections, which lower our detection probability and increase the estimate of N. This is why we see more outliers for this scenario. When detection probability increases, then we that IQR decrease

-   Another interesting observation is that when time = 10, the median estimate is the same for each value of alpha, no matter what the p value is. But once again, that IQR tightens up as the p increases

-   a small probability of errors already skyrockets the estimates --\> even when we only have five capture occasions.

-   once we reach alpha = 10% for time = 10, no matter what p_1m is, median estimate is basically the same -\> only change is that alpha becomes smaller...

```{r}
dfEstimatesPop %>%
ggplot()+
  geom_boxplot(aes(x=as.factor(p_1m), y = estimate, fill = as.factor(alpha)),coef=100)+ 
  geom_hline(aes(yintercept = trueValues),color = "darkblue",linetype="dotted")+
  labs(title = "Population Estimates", subtitle = "Facetted by number of capture occasions. Population N = 20.")+
  facet_wrap(~maxMinute)+ 
  scale_fill_discrete(name = "Alpha")+ 
  labs(x= "Per Minute Detection Probability",y = "Estimate")+
  theme_bw()

#ggsave("./Images/Population_Estimates_20.jpg")
```

```{r}
dfEstimatesFinalH %>%
  filter(N==10) %>%
ggplot()+
  geom_boxplot(aes(x=as.factor(p_1m), y = estimate, fill = as.factor(alpha)),coef=100)+ 
  geom_hline(aes(yintercept = trueValues),color = "darkblue",linetype="dotted")+
  labs(title = "Population Estimates", subtitle = "Facetted by number of capture occasions. Population N = 10.")+
  facet_wrap(~maxMinute)+ 
  scale_fill_discrete(name = "Alpha")+ 
  labs(x= "Per Minute Detection Probability",y = "Estimate")+
  theme_bw()
#ggsave("./Images/Population_Estimates_10.jpg")
```

How many individuals are detected, once, twice, etc for N = 20 and maxMinute = 10. Used to make the graph of the distribution of detections per Individual.
```{r}
dfCounts = dfHistoryFinalH %>%
  filter(N==20 & maxMinute == 10 & p_1m != 0.3)%>%
  group_by(p_1m,alpha,maxMinute,counts)%>%
  summarise(c =n())%>%
  mutate(total = sum(c),
         percent = round(c/total*100,2))
head(dfCounts)
```

```{r}
ggplot(dfCounts,aes(x=counts, y = percent, color = as.factor(alpha)))+ 
  #geom_point()+
  geom_line()+
  facet_wrap(~p_1m)+
  theme_bw()+ 
  scale_color_discrete(name="Alpha")+
  labs(title = "Distribution of Detections per Individual", subtitle= "Facetted by detection probability. Population N = 20. Capture occasions = 10.", x = "Detections per Individual",y = "% of Individuals Detected ")

ggsave("C:/Users/titom/OneDrive/Documents/Rcode/Undergratuate-Thesis/Images/detection_individuals.jpg")
```

```{r}
dfCounts = dfHistoryFinalH %>%
  filter(N==20 & maxMinute == 5 & p_1m != 0.3)%>%
  group_by(p_1m,alpha,maxMinute,counts)%>%
  summarise(c =n())%>%
  mutate(total = sum(c),
         percent = round(c/total*100,2))

ggplot(dfCounts,aes(x=counts, y = percent, color = as.factor(alpha)))+ 
  geom_line()+
  facet_wrap(~p_1m)+
  theme_bw()+ 
  scale_color_discrete(name="Alpha")+
  labs(title = "Distribution of Number of Detections Across Capture Histories", subtitle= "Facetted by Detection Probability. Capture Occasion = 5.", x = "Detections per Individual",y = "% of Individuals Detected ")
```


See how number of single detections vary across capture occasions. Just focus on number of people counted once.
```{r}
dfCounts = dfHistoryFinalH %>%
  filter(N==20 & p_1m != 0.3)%>%
  group_by(p_1m,alpha,maxMinute,counts)%>%
  summarise(c =n())%>%
  mutate(total = sum(c),
         percent = round(c/total*100,2))%>%
  filter(counts == 1)
dfCounts%>%
  ggplot()+
  geom_line(aes(x=p_1m,y=percent,color = as.factor(alpha)))+
  facet_wrap(~maxMinute)+
  theme_bw()
```


```{r}
dfCounts = dfHistoryFinalH %>%
  filter(N==20 & maxMinute == 5 & p_1m != 0.3)%>%
  group_by(p_1m,alpha,maxMinute,counts)%>%
  summarise(c = n())%>% # of times someone was counted once, twice, etc
  mutate(total = sum(c))
```

Distribution of detection probabilities

```{r}
dfEstimatesProbability%>%
  filter(alpha == 0.3 & maxMinute == 10)%>%
  ggplot()+ 
  geom_density(aes(x=estimate, color = as.factor(p_1m)))+ 
  theme_bw()
 
```

For use in a table in the report. Shows how sd becomes smaller as p_1m becomes larger. 
```{r}
dfCompareEstimates = dfEstimatesFinalH%>%
  filter(alpha==0.3  & p_1m %in% c(0.1,0.2,0.3,0.4,0.5) & N == 20, maxMinute == 10)%>%
  group_by(maxMinute,p_1m)%>%
  summarise(`Average Estimate` = round(mean(estimate),1),
            `Standard Deviation`=round(sd(estimate),1))%>%
  rename(`Capture Occasions` = maxMinute)
dfCompareEstimates

```
Also for use in report. Showing how SD becomes smaller as p_1m becomes bigger
```{r}
dfEstimatesFinalH%>%
  filter(alpha==0.3  & p_1m %in% c(0.1,0.5) & N == 20)%>%
  group_by(maxMinute,p_1m)%>%
  summarise(`Average Estimate` = round(mean(estimate),1),
            `Standard Deviation`=round(sd(estimate),1))%>%
  rename(`Capture Occasions` = maxMinute)
```



# Tables
This is a summary table of all scenarios. 100 rows because I have 100 scenarios for my data. 
```{r}
# boolean for if a value is in the confidence interval or not
# rounding the lower confidence level because sometimes the estimate is something like 19.999 and the lower confidence interval is 20 
# in a case like that, the estimate really should be included in the confidence interval - the 0.0001 difference is a technicality 
# also ran into issues like then when maximinizing the full likelihood - with conditional likelihood it really does not happen that much
dfEstimatesFinalH$lcl = round(dfEstimatesFinalH$lcl) 
dfEstimatesFinalH$bCoverage = ifelse(dfEstimatesFinalH$trueValues >= dfEstimatesFinalH$lcl & dfEstimatesFinalH$trueValues <= dfEstimatesFinalH$ucl,1,0)

# find cases where bCoverage = 0, and check to make sure it's not a tolerance issue; if trueValue = lcl or ucl, change bCoverage boolean to 1
lclTrue = mapply(function(x, y) {isTRUE(all.equal(x, y))}, dfEstimatesFinalH$trueValues, dfEstimatesFinalH$lcl)
uclTrue = mapply(function(x, y) {isTRUE(all.equal(x, y))}, dfEstimatesFinalH$trueValues, dfEstimatesFinalH$ucl)
dfEstimatesFinalH[which(lclTrue),"bCoverage"] = 1
dfEstimatesFinalH[which(uclTrue),"bCoverage"] = 1
```

```{r}
dfEstimatesFinalH$squaredError = (dfEstimatesFinalH$estimate - dfEstimatesFinalH$trueValues)^2
dfEstimatesFinalH$width = dfEstimatesFinalH$ucl - dfEstimatesFinalH$lcl
  
dfSummaryStatsH = dfEstimatesFinalH %>%
    filter(variable == "N")%>% # only focus on N? or also use p??
    group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
    summarise(AvgNhat = mean(estimate,na.rm=TRUE),
              medianNhat= median(estimate,na.rm=TRUE),
              AvgNhatSE= mean(se,na.rm=TRUE),
              sdNhat = sd(estimate,na.rm=TRUE),
              biasNhat = AvgNhat - N,
              mse = mean(squaredError,na.rm=TRUE),
              bias_SE_Nhat = AvgNhatSE - sdNhat,
              coverage = sum(bCoverage,na.rm=TRUE)/sum(!is.na(estimate)), # changed this from n() to sum(!is.na()) because I have some NA values for my estimates
              avgWidth = mean(width,na.rm=TRUE),
              avgAIC = mean(AIC,na.rm=TRUE),
              relNhatBias = round(biasNhat/N*100,2),
              avgUCL= mean(ucl,na.rm=TRUE),
              avgLCL = mean(lcl, na.rm=TRUE),
              AvgNhatVar= mean(se^2,na.rm=TRUE))
dfSummaryStatsH = dfSummaryStatsH[!duplicated(dfSummaryStatsH),]
dfSummaryStatsH
```

```{r}
kable(dfSummaryStatsH)
```


## Plots on Table

Standard error as detection probability changes

```{r}
dfSummaryStatsH%>%
  filter(N==20)%>%
ggplot()+
  geom_point(aes(x=p_1m, y = AvgNhatSE, color = as.factor(maxMinute)))+
  geom_line(aes(x=p_1m, y = AvgNhatSE, color = as.factor(maxMinute)))+
  facet_grid(~alpha,space="free")+
  theme_bw()+
  labs(title = "Average Population Standard Error", subtitle = "N = 20. Facetted by Alpha",x="Per Minute Detection Probability",y="Average SE")+
  scale_color_discrete(name = "Capture Occasions")
ggsave("C:/Users/titom/OneDrive/Documents/Rcode/Undergratuate-Thesis/Images/avg_se_20.jpg",width=10,height=5)
```

```{r}
dfSummaryStatsH%>%
  filter(N==10)%>%
ggplot()+
  geom_point(aes(x=p_1m, y = AvgNhatSE, color = as.factor(maxMinute)))+
  geom_line(aes(x=p_1m, y = AvgNhatSE, color = as.factor(maxMinute)))+
  facet_grid(~alpha,space="free")+
  theme_bw()+
  labs(title = "Average Population Standard Error", subtitle = "N = 10. Facetted by Alpha",x="Per Minute Detection Probability",y="Average SE")+
  scale_color_discrete(name = "Capture Occasions")
ggsave("C:/Users/titom/OneDrive/Documents/Rcode/Undergratuate-Thesis/Images/avg_se_10.jpg",width=10,height=5)
```

```{r}
dfSummaryStatsH%>%
  filter(N==20 & maxMinute == 10)%>%
ggplot()+
  geom_point(aes(x=p_1m, y = AvgNhatSE, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = AvgNhatSE, color = as.factor(alpha)))+
  theme_bw()+
  labs(title = "Average Population Standard Error", subtitle = "N = 20. Facetted by Alpha",x="Per Minute Detection Probability",y="Average SE")+
  scale_color_discrete(name = "Capture Occasions")
```


The order of the average n values is as expected - as alpha increases, estimates of n should increase. I'm curious about the trend though. Namely, shouldn't the average n always be increasing as we increase p? Or at least kind of stabilize. When time = 5 I see this, but when time = 10, I'm confused about it. Maybe high detection probability makes up for the fact that we are miscounting and reduces bias?
```{r}
dfSummaryStatsH %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()+ 
  labs(title  = "Average Population Estimates", subtitle = "N = 20",x= "Per Minute Detection Probability",y = "Average N Estimate")+
  scale_color_discrete(name = "Alpha")
```

```{r}
dfSummaryStatsH %>%
  filter(N==20 & maxMinute == 10)%>%
ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  facet_wrap(~as.factor(alpha),scales = "free")+
  #facet_grid(~as.factor(alpha),space="free")+
  theme_bw()+ 
  labs(title  = "Average Population Estimates With Confidence Intervals", subtitle = "Population N = 20. Capture occasions = 10.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Alpha")

#ggsave("./Images/avg_pop_est.jpg",width=10,height=5)
```


```{r}
dfSummaryStatsH %>%
  filter(N==20 & maxMinute == 10)%>%
ggplot(aes(x=p_1m, y = relNhatBias, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  #geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  #facet_grid(~as.factor(alpha),space="free")+
  theme_bw()+ 
  labs(title  = "Average Population Estimates With Confidence Intervals", subtitle = "Population N = 20. Capture occasions = 10.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Alpha")
```



```{r}
dfSummaryStatsH %>%
  filter(N==10 & maxMinute == 5)%>%
ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  facet_wrap(~as.factor(alpha),scales = "free")+
  #facet_grid(~as.factor(alpha),space="free")+
  theme_bw()+ 
  labs(title  = "Average Population Estimates With Confidence Intervals", subtitle = "Population N = 10. Capture occasions = 5.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Alpha")

#ggsave("./Images/avg_pop_est_10_5.jpg",width=10,height=5)
```


```{r}
dfSummaryStatsH %>%
  filter(N==20 & maxMinute == 5)%>%
ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  facet_wrap(~as.factor(alpha),scales = "free")+
  #facet_grid(~as.factor(alpha),space="free")+
  theme_bw()+ 
  labs(title  = "Average Population Estimates With Confidence Intervals", subtitle = "Population N = 20. Capture occasions = 5.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Alpha")

#ggsave("./Images/avg_pop_est_5.jpg",width=10,height=5)
```



```{r}
dfSummaryStatsH %>%
  filter(N==10 & maxMinute == 10)%>%
ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  facet_wrap(~as.factor(alpha),scales = "free")+
  #facet_grid(~as.factor(alpha),space="free")+
  theme_bw()+ 
  labs(title  = "Average Population Estimates With Confidence Intervals", subtitle = "Population N = 10. Capture occasions = 10.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Alpha")

#ggsave("./Images/avg_pop_est_10.jpg",width=10,height=5)
```

For use in presentation/report. Just showing some numbers and comparing how key summary stats change
```{r}
dfSummaryStatsH%>%
  filter(p_1m %in% c(0.1,0.5) & alpha %in% c(0,0.05)& N==20 & maxMinute == 10)%>%
  select(p_1m,alpha,AvgNhat,relNhatBias,AvgNhatSE,mse,coverage)
```

```{r}
dfInteract = dfEstimatesFinalH%>%
  filter(N=="20" & maxMinute==10)
interaction.plot(x.factor = dfInteract$p_1m,
                 trace.factor = dfInteract$alpha,
                 response = dfInteract$estimate)

dfInteract = dfEstimatesFinalH%>%
  filter(N=="20" & maxMinute==5)
interaction.plot(x.factor = dfInteract$p_1m,
                 trace.factor = dfInteract$alpha,
                 response = dfInteract$estimate)

dfInteract = dfEstimatesFinalH%>%
  filter(N=="10" & maxMinute==10)
interaction.plot(x.factor = dfInteract$p_1m,
                 trace.factor = dfInteract$alpha,
                 response = dfInteract$estimate)
dfInteract = dfEstimatesFinalH%>%
  filter(N=="10" & maxMinute==5)
interaction.plot(x.factor = dfInteract$p_1m,
                 trace.factor = dfInteract$alpha,
                 response = dfInteract$estimate)
```


```{r}
dfInteract = dfEstimatesFinalH%>%
  filter(N=="20"& p_1m==0.1)
interaction.plot(x.factor = dfInteract$maxMinute,
                 trace.factor = dfInteract$DataEncounters,
                 response = dfInteract$estimate)
```

Significant interaction between capture occasions and detection probability on mean estimate.

```{r}
dfInteract = dfEstimatesFinalH%>%
  filter(N==20)
interaction.plot(x.factor = dfInteract$maxMinute,
                 trace.factor = dfInteract$p_1m,
                 response = dfInteract$estimate)
```
```{r}
dfEstimatesFinalH%>%
  filter(N == 20)%>%
  group_by(maxMinute, p_1m, alpha)%>%
  summarise(avgEstimate = mean(estimate,na.rm=TRUE))%>%
  ggplot()+
  geom_line(aes(x=maxMinute,y=avgEstimate,color=as.factor(p_1m)))+
  facet_wrap(~alpha,scales="free")+
  theme_bw()+
  labs(title = "Interaction Between Alpha and Detection Probability",subtitle = "Facetted by Alpha",x = "Number of Capture Occasions",y = "Mean Population Estimate")+
  scale_color_discrete(name = "Detection Probability")

#ggsave("./Images/interac.jpg")
```

All significant.

```{r}
dfAnova=dfEstimatesFinalH%>%
  filter(N == 20 & alpha == 0.1)
aov1 = aov(estimate ~ as.factor(maxMinute)*as.factor(p_1m), data = dfAnova)
summary(aov1)
```


## Why SE decrease

Why does SE decrease as p_1m increases? Because we have more data points to work with? Is that also why it decreases so dramatically when alpha = 0.3? Combo of more true individuals detected + more new individuals added to the sample

```{r}
dfSummaryStatsH %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = AvgNhatSE, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()+ 
  labs(title  = "Standard Error of Population Estimates", subtitle = "N = 20",x= "1 Min Detection Probability (p_1m)",y = "Average SE of Estimate")+
  scale_color_discrete(name = "Alpha")
```

```{r}
dfSummaryStatsH %>%
  filter(N==10)%>%
ggplot(aes(x=p_1m, y = AvgNhatSE, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()+ 
  labs(title  = "Standard Error of Population Estimates", subtitle = "N = 10",x= "1 Min Detection Probability (p_1m)",y = "Average SE of Estimate")+
  scale_color_discrete(name = "Alpha")
```

## MSE

It makes sense that MSE is largest when alpha is largest because we are introducing more individuals into the data than there should be. As probability of detection increases, for a fixed alpha, I assumed mse should be increasing. This is because we are detecting more individuals, which means more opportunities for us to miscount.

-   **So if this is the case, for when alpha = 0.3, why is mse decreasing?**

- ANS: think of $MSE(\hat{\theta}) = Var(\hat{\theta})+ Bias(\hat{\theta})^2$. Bias is the same but standard error ($\sqrt{Var({\hat{\theta})}}$) keeps falling (from what we have seen in the graphs earlier. So that explains the reason we see this convergence to a value (which is bias squared).

```{r}
dfSummaryStatsH %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = mse, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()+ 
  labs(title  = "Mean Squared Error of Population Estimates", subtitle = "Facetted by capture occasions. Population N = 20. ",x= "Per Minute Detection Probability",y = "Average MSE of Estimates")+
  scale_color_discrete(name = "Alpha")

#ggsave("./Images/mse.jpg")
```

```{r}
dfSummaryStatsH %>%
  filter(N==20 & p_1m %in% c(0.1,0.5)&alpha %in% c(0,0.05) & maxMinute == 10)
```

```{r}
dfSummaryStatsH %>%
  filter(N==10)%>%
ggplot(aes(x=p_1m, y = mse, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()+ 
  labs(title  = "Mean Squared Error of Population Estimates", subtitle = "Facetted by capture occasions. Population N = 10. ",x= "Per Minute Detection Probability",y = "Average MSE of Estimates")+
  scale_color_discrete(name = "Alpha")

#ggsave("./Images/mse_10.jpg")
```



Relationship between bias and variance 
```{r}
dfSummaryStatsH%>%
  filter(N==20 & maxMinute==10)%>%
  ggplot()+ 
  geom_point(aes(x=p_1m, y = biasNhat^2, color = "Average Squared Bias" ,shape=as.factor(alpha)))+
  geom_point(aes(x=p_1m, y = AvgNhatVar, color = "Average Variance of N" ,shape=as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = biasNhat^2, color = "Average Squared Bias" ))+
  geom_line(aes(x=p_1m, y = AvgNhatVar, color = "Average Variance of N" ))+
  #geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  facet_wrap(~as.factor(alpha),scales = "free")+
  #facet_grid(~as.factor(alpha),space="free")+
  theme_bw()+ 
  labs(title  = "Bias and Variance Across Detection Probability", subtitle = "Facetted by Alpha. Population N = 20. Capture occasions = 10.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Terms")+
  scale_shape_discrete(name = "Alpha")

#ggsave("./Images/bias_var_20.jpg",width=10,height=5)
```

```{r}
dfSummaryStatsH%>%
  filter(N==20 & maxMinute==5)%>%
  ggplot()+ 
  geom_point(aes(x=p_1m, y = biasNhat^2, color = "Average Squared Bias" ,shape=as.factor(alpha)))+
  geom_point(aes(x=p_1m, y = AvgNhatVar, color = "Average Variance of N" ,shape=as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = biasNhat^2, color = "Average Squared Bias" ))+
  geom_line(aes(x=p_1m, y = AvgNhatVar, color = "Average Variance of N" ))+
  #geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  facet_wrap(~as.factor(alpha),scales = "free")+
  #facet_grid(~as.factor(alpha),space="free")+
  theme_bw()+ 
  labs(title  = "Bias and Variance Across Detection Probability", subtitle = "Facetted by Alpha. Population N = 20. Capture occasions = 5.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Terms")+
  scale_shape_discrete(name = "Alpha")

#ggsave("./Images/bias_var_5.jpg",width=10,height=5)
```

## Coverage

Coverage for when min= 5 is as expected; lower coverage as alpha increases due to miscounting. What is interesting however is that even for a small alpha value, given a high enough detection probability, the coverage will go to 0. When time = 10, the rate at which coverage drops to 0 is much faster.

```{r}
dfSummaryStatsH %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = coverage, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_light()+ 
  labs(title  = "Coverage Probability", subtitle = "Facetted by capture occasions. Population N = 20.", x= "Per Minute Detection Probability",y = "Coverage Probability")+
  scale_color_discrete(name = "Alpha")+
  theme_bw()

#ggsave("./Images/coverage.jpg")
```

```{r}
dfEstimatesFinalH%>%
  filter(alpha == 0 & p_1m == 0.5 & N == 20 & maxMinute == 10)
```

to verify that cov probabilities make sense, note that cov probability when p = 0.5 and time = 5 is equivalent to cov probability when p = 0.3 andtime = 10. check prob of only being detected once to confirm that they have the same prob of being detected once?

```{r}
dfSummaryStatsH %>%
  filter(N==10)%>%
ggplot(aes(x=p_1m, y = coverage, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_light()+ 
  labs(title  = "Coverage Probability", subtitle = "Facetted by capture occasions. Population N = 10.", x= "Per Minute Detection Probability",y = "Coverage Probability")+
  scale_color_discrete(name = "Alpha")+
  theme_bw()

#ggsave(./Images/coverage10.jpg")
```

### Check Coverage

This section was for when I was still maximizing the full likelihood. Even though the lcl would say 20, and my estimate would say 20, because they were floats, my estimate of 20.00000001 was said to note be 20 -> fixed this in the earlier section (called Tables) by rounding and checking for tolerance values. This is just the code I used to play around with finding if a value was within the tolerance limit or not. Can ignore this section!!
```{r}
dfEstimatesFinalH%>%
  filter(variable == "N")%>% # only focus on N? or also use p??
  group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
  summarise(c = sum(!is.na(estimate)),
            cv = sum(bCoverage))

testCov = dfEstimatesFinalH%>%
  filter(combinationNumber==99)
testCov
```

```{r}
# i was having issues with testing if the floating point values are the same
## https://stackoverflow.com/questions/9508518/why-are-these-numbers-not-equal 
testCov$trueValues[10]>=testCov$lcl[10]
all.equal(testCov$trueValues[10],testCov$lcl[10])
```

```{r}
dfSummaryStatsH %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = bias_SE_Nhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()
```

```{r}
dfSummaryStatsH %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = relNhatBias, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_light()+ 
  labs(title  = "Relative Bias", subtitle = "Facetted by Capture Occasions. N = 20.", x= "1 Min Detection Probability (p_1m)",y = "Relative Bias %")+
  scale_color_discrete(name = "Alpha")+
  theme_bw()
```
