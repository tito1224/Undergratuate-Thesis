---
title: "Final Closure Analysis"
author: "Tito"
date: "2023-07-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Capture History
```{r}
library(tidyverse)
library(gtools)
library(data.table)
library(matrixStats)
library(RMark)
library(stringi)
library(rlang)
library(gt)
library(gtExtras)
library(webshot2)
library(knitr)
library(readxl)
```

Load functions from the other R scripts.
```{r}
source("./Functions/generatePointCount.R")
source("./Functions/runSimulationFunctions.R")
source("testClosure.R")
```

# Load Data

## Load CJS Results
```{r}
strPath = "./DigitalAllianceOutput/Closure3"
setwd(strPath)
dataFinalTrial = list.files(pattern = ".rds") # load rds files
dataFinalTrial = dataFinalTrial%>%  
  map(readRDS)

# initialize empty dataframes to store the data
dfClosure_CJS = data.frame()
#counter = 1

# loop through the rds files and retrive data 
for (item in dataFinalTrial){
  dfClosure_CJS = rbind(dfClosure_CJS, item)
  #counter = counter + 1
}
```


```{r}
# this code is what was used in the ./Functions/runSimulation.R file to generate scenarios to input into the HPC 
nRuns = 1000
lstNi = c(10,20)
lstP = c(0.1,0.2,0.3,0.4,0.5)
#lstAlpha = c(0)
lstAlpha =c(0,0.05,0.1,0.2,0.3)
lstMaxMin = c(5,10)
lstFormula = c("~1")
lstMixtures = c(1)
lstSeed = c("NULL")
strModel = "Huggins"

params = expand.grid(nRuns,lstNi,lstP, lstAlpha, lstMaxMin,lstFormula,lstMixtures,lstSeed,strModel)
colnames(params) = c("nRuns","N","p_1m","alpha","maxMinute","strFormula","lstMixtures","lstSeed","strModel")
params$Scenario = 1:nrow(params)

# merge the params dataframe with the dataframe of estimates to help with calculating summary statistics 
dfClosure_CJS$combinationNumber = as.integer(str_split_fixed(dfClosure_CJS$id, "_",2)[,1])
dfClosure_CJS = left_join(dfClosure_CJS,params,by=c("combinationNumber"="Scenario")) 
dfClosure_CJS$paramSimple= ifelse(grepl('Phi', dfClosure_CJS$param)==TRUE, "phi","p")
dfClosure_CJS$pExactOne = ifelse(dfClosure_CJS$p_CJS == 1, 1, 0)
head(dfClosure_CJS)
```


## Load Otis Results

```{r}
dfOriginalData = loadData()
```

Compute Otis test statistics
```{r}
dfOtisTest = otisDetectClosure(dfOriginalData)
```

There are times when p_k_Otis is nan because the varqi = 0 -> this is due to the fact that the individual is captured at each capture occasion
```{r}
dfOtisTest %>%
  filter(is.na(p_k_Otis))
```



```{r}
setwd("./DigitalAllianceOutput/Huggins")
dataFinalTrial = list.files(pattern = ".rds") # load rds files
dataFinalTrial = mixedsort(dataFinalTrial)%>% # use mixedsort so it appears in ascending order 
  map(readRDS)

# initialize empty dataframes to store the data
dfEstimatesFinalH = data.frame()
#counter = 1

# loop through the rds files and retrive data 
for (item in dataFinalTrial){
  tempCalculations = item[[2]] # load estimates for each run (should be 200,000 rows - 100,000 estimates of N, and 100,000 estimates of p)
  
  # add data points to final dataframe
  dfEstimatesFinalH = rbind(dfEstimatesFinalH, tempCalculations)
}

dfEstimatesFinalH$id = paste0(dfEstimatesFinalH$combinationNumber,"_",dfEstimatesFinalH$simulationNumber)
# this code is what was used in the ./Functions/runSimulation.R file to generate scenarios to input into the HPC 
nRuns = 1000
lstNi = c(10,20)
lstP = c(0.1,0.2,0.3,0.4,0.5)
#lstAlpha = c(0)
lstAlpha =c(0,0.05,0.1,0.2,0.3)
lstMaxMin = c(5,10)
lstFormula = c("~1")
lstMixtures = c(1)
lstSeed = c("NULL")
strModel = "Huggins"

params = expand.grid(nRuns,lstNi,lstP, lstAlpha, lstMaxMin,lstFormula,lstMixtures,lstSeed,strModel)
colnames(params) = c("nRuns","N","p_1m","alpha","maxMinute","strFormula","lstMixtures","lstSeed","strModel")
params$Scenario = 1:nrow(params)

# merge the params dataframe with the dataframe of estimates to help with calculating summary statistics 
dfEstimatesFinalH = left_join(dfEstimatesFinalH,params,by=c("combinationNumber"="Scenario")) 

# some code to help with calculating summary statistics -> full summary not done yet, but these are intermediate steps needed 
# to determine things like coverage probability, MSE, etc
dfEstimatesFinalH$bCoverage = ifelse(dfEstimatesFinalH$trueValue >= dfEstimatesFinalH$lcl & dfEstimatesFinalH$trueValue <= dfEstimatesFinalH$ucl,1,0)
dfEstimatesFinalH$squaredError = (dfEstimatesFinalH$estimate - dfEstimatesFinalH$trueValues)^2
dfEstimatesFinalH$width = dfEstimatesFinalH$ucl - dfEstimatesFinalH$lcl
```
### Clean Estimates 

Filter out all cases where individuals were detected only once. 

```{r}
dfMultDetect = dfOriginalData %>%
  filter(counts>1)

dfSingleDetection = dfOriginalData %>%
  filter(counts==1 & !(id %in% dfMultDetect$id))

dfEstimatesProbability = dfEstimatesFinalH %>% # these are probability estimates;
  filter(variable == "p g1 t1") %>%
  filter(estimate>=0) %>%
  filter(!(id %in% dfSingleDetection$id))

lstBadID = unique(dfOriginalData[which(!(dfOriginalData$id %in% dfEstimatesProbability$id)),]$id)

dfOtisTest = dfOtisTest%>%
  filter(!(id %in% lstBadID))

dfClosure_CJS = dfClosure_CJS%>%
  filter(!(id%in% lstBadID))
```

## Combine Test Results

```{r}
dfClosure_CJS_Summary = dfClosure_CJS%>%
  filter(paramSimple == "p")%>%
  select(id, p_CJS,CJS_estimate,bConstrained)

dfOtisTest_Summary = dfOtisTest%>%
  select(id, N,p_1m,alpha,maxMinute,pOverall_Otis)%>%
  filter(!duplicated(id))


dfEstimatesProbability = dfEstimatesProbability%>%
  rename(HugginsEstimate = estimate)

dfClosure = left_join(dfClosure_CJS_Summary,dfOtisTest_Summary,by="id")
dfClosure =left_join(dfClosure, dfEstimatesProbability[,c("id","HugginsEstimate","se","bCoverage","squaredError","width","lcl","ucl")])
dfClosure$combinationNumber = as.integer(str_split_fixed(dfClosure$id, "_",2)[,1])
head(dfClosure)
```


# Graph Results

## Otis

### Wait Time (Qi)

Not that informative when maxMin = 5. But overall same pattern. We see that when alpha is large, the median wait time is considerably lower. Exploratory analysis indicates that using wait time as metric to detect closure is indeed a good idea. 
```{r}
# if qi <0 it means there was only one detection

dfOtisTest%>%
  filter(N == 20 & maxMinute == 10)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = qi, color = as.factor(alpha)))+ 
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()+
  labs(title = "Distribution of Time Between First and Last Detection",subtitle="N = 20; 10 capture occasions." ,x = "Per minute detection probability", y = "Waiting Time")
```



### Conditional Test Statistic (C_k)

Now I will look at the distribution of the p values associated with the test statistic C_k across various scenarios. 


When N = 20 and only five capture occasions, Otis test does not reject closure on average. Note that low population and/or low capture occasion means that we use only 1-2 individuals to calculate the conditional test statistic - this does not seem to be a good idea.  
```{r}
dfOtisTest%>%
  filter(N == 20 & maxMinute == 5)%>%
  distinct(id,counts,.keep_all = TRUE)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = p_k_Otis, color = as.factor(alpha)))+ 
   geom_hline(aes(yintercept=0.05), color = "grey30")+
  scale_color_discrete(name="Alpha")+
  labs(title ="Results of Conditional Hypothesis Test", subtitle = "N = 20; 5 capture occasions.",x="Per minute detetion probability", y = "p_k")+
  theme_bw()

dfOtisTest%>%
  filter(N == 20 & maxMinute == 10)%>%
  distinct(id,counts,.keep_all = TRUE)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = p_k_Otis, color = as.factor(alpha)))+ 
   geom_hline(aes(yintercept=0.05), color = "grey30")+
  scale_color_discrete(name="Alpha")+
  labs(title ="Results of Conditional Hypothesis Test", subtitle = "N = 20; 10 capture occasions.",x="Per minute detetion probability", y = "p_k")+
  theme_bw()
```


```{r}
unique(dfOtisTest$counts)
```



The issue with the p_k_Otis value is that this is all based on only 3 or 4 individuals which is not great and feels like it is subject to a lot of variability. We can see there are circumstances where it fails to reject closure (even though it should) because not enough individuals are detected > 1 times. 

```{r}
dfOtisTest%>%
  filter(N ==20 & maxMinute == 10 & alpha == 0.1 & p_1m == 0.3)%>%
  group_by(N,p_1m, alpha,maxMinute,id,counts,p_k_Otis)%>%
  summarise(nIndividualsUsed = n())%>%
  group_by(N,p_1m, alpha,maxMinute,counts)%>%
  summarise(meanIndidivualsUsed= mean(nIndividualsUsed), 
            medianPvalue = median(p_k_Otis),
            meanPvalue = mean(p_k_Otis))
```


### Overall Test Statistic (C)

The smaller the test statistic C is, the more evidence against closure. Still requires a lot of evidence to reject the closure assumption - only when p = 0.3 or higher do we begin to see some reasonableness. But really p must equal 0.5 in order to actually reject closure @ the smallest alpha. Test is done right because when alpha = 0, closure is never rejected, even when p_1m is small.
```{r}
# dfOtisTest%>%
#   filter(N == 20 & maxMinute == 5)%>%
# ggplot()+ 
#   geom_boxplot(aes(x= as.factor(p_1m), y = C, color = as.factor(alpha)))+ 
#   geom_hline(yintercept = qnorm(0.05, lower.tail=TRUE),color = "grey20",size=0.8)+
#   scale_color_discrete(name="Alpha")+
#   theme_bw()
# 
# dfOtisTest%>%
#   filter(N == 20 & maxMinute == 10)%>%
# ggplot()+ 
#   geom_boxplot(aes(x= as.factor(p_1m), y = C, color = as.factor(alpha)))+ 
#   geom_hline(yintercept = qnorm(0.05, lower.tail=TRUE),color = "grey20",size=0.8)+
#   scale_color_discrete(name="Alpha")+
#   theme_bw()
```


```{r}
dfOtisTest%>%
  filter(N == 20 & maxMinute == 10)%>%
  distinct(id,.keep_all = TRUE)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = pOverall_Otis, color = as.factor(alpha)))+ 
   geom_hline(aes(yintercept=0.05), color = "grey30")+
  scale_color_discrete(name="Alpha")+
  theme_bw()+
  labs(title = "Results of Otis Closure Test", subtitle = "N=20; 10 capture occassions.", x = "Per minute detection probability", y ="p Otis")
```

```{r}
dfOtisTest%>%
  filter(N == 20 & maxMinute == 5)%>%
  distinct(id,.keep_all = TRUE)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = pOverall_Otis, color = as.factor(alpha)))+ 
   geom_hline(aes(yintercept=0.05), color = "grey30")+
  scale_color_discrete(name="Alpha")+
  theme_bw()+
  labs(title = "Results of Otis Closure Test", subtitle = "N=20; 5 capture occassions.", x = "Per minute detection probability", y ="p Otis")
```


### Power of Otis Test

A Type I error occurs if we reject the null hypothesis H0 (in favor of the alternative hypothesis HA) when the null hypothesis is true. We denote alpha = probability of Type 1 error 

A Type II error occurs if we fail to reject the null hypothesis when the alternative hypothesis is true. We denote B = probability of type 2 error.

The power of a hypothesis test is the probability of making the correct decision if the alternative hypothesis is true. That is, the power of a hypothesis test is the probability of rejecting the null hypothesis H0 when the alternative hypothesis is is the hypothesis that is true

```{r}
dfOtisTest_Power = dfOtisTest%>%
  mutate(Type_k = case_when(
    p_k_Otis < 0.05 & alpha == 0 ~ "Type 1",
    p_k_Otis > 0.05 & alpha != 0 ~ "Type 2",
    p_k_Otis < 0.05 & alpha !=0 ~ "Power",
    p_k_Otis > 0.05 & alpha == 0 ~ "True"
  )
         )


dfOtisTest_Power = dfOtisTest_Power%>%
  mutate(Type = case_when(
    pOverall_Otis < 0.05 & alpha == 0 ~ "Type 1",
    pOverall_Otis > 0.05 & alpha != 0 ~ "Type 2",
    pOverall_Otis < 0.05 & alpha !=0 ~ "Power",
    pOverall_Otis > 0.05 & alpha == 0 ~ "True"
  )
         )
```



Makes sense to do it by combination number I believe. Graph verifies that not a single case was rejected for combination number 1.
ASK ABT THIS PART


```{r}
dfOtisPercent = dfOtisTest_Power%>%
  filter(!is.na(p_k_Otis))%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  summarise(percentDatasets= round(n()/1000,5)*100 )
dfOtisPercent 
```




```{r}
dfOtisTest_Rate = dfOtisTest_Power%>%
  distinct(id,counts,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  mutate(totalCases = n())%>%
  group_by(N,p_1m,alpha,maxMinute,combinationNumber,totalCases,Type_k)%>%
  summarise(nType=n())%>%
  ungroup()%>%
  mutate(Percent=round(nType/totalCases*100,2))
dfOtisTest_Rate = left_join(dfOtisTest_Rate, dfOtisPercent,by="combinationNumber")
dfOtisTest_Rate
```



```{r}
dfOtisTest%>%
  filter(N == 10 & maxMinute == 5)%>%
  distinct(id,counts,.keep_all = TRUE)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = p_k_Otis, color = as.factor(alpha)))+ 
   geom_hline(aes(yintercept=0.05), color = "grey30")+
  scale_color_discrete(name="Alpha")+
  labs(title ="Results of Conditional Hypothesis Test", subtitle = "N = 10; 5 capture occasions.",x="Per minute detetion probability", y = "p_k")+
  theme_bw()
```

Type 1 error stays largely within the bounds of the significance level. Max power is only slightly higher than 75% even when detection probability is high. 
```{r}
dfOtisTest_Rate%>%
  filter(N == 20 & maxMinute == 10 & !is.na(Type_k) & Type_k == "Power")%>%
  ggplot(aes(x=p_1m, y = Percent, color = as.factor(alpha), label = percentDatasets))+
  geom_point()+
  geom_line()+
  geom_text(hjust=0,vjust=-0.3)+
  #geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  #scale_linetype_manual(name = "Significance level", values = c(5), 
  #                    guide = guide_legend(override.aes = list(color = c("black"))))+
  #facet_wrap(~maxMinute)+
  theme_bw()+
  labs(title = "Power of Conditional Otis Test Across Scenarios", subtitle = "N = 20; 10 capture occasions",x="Per minute detection probability",y="Power")+
  scale_color_discrete(name="Alpha")
```



```{r}
dfOtisTest_Rate%>%
  filter(N == 20 & maxMinute == 5 & !is.na(Type_k) & Type_k == "Power")%>%
  ggplot(aes(x=p_1m, y = Percent, color = as.factor(alpha), label = percentDatasets))+
  geom_point()+
  geom_line()+
  geom_text(hjust=0,vjust=-0.3)+
  #geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  #scale_linetype_manual(name = "Significance level", values = c(5), 
  #                    guide = guide_legend(override.aes = list(color = c("black"))))+
  #facet_wrap(~maxMinute)+
  theme_bw()+
  labs(title = "Power of Conditional Otis Test Across Scenarios", subtitle = "N = 20; 5 capture occasions",x="Per minute detection probability",y="Power")+
  scale_color_discrete(name="Alpha")
```


```{r}
dfOtisTest_Rate%>%
  filter(N == 20  & !is.na(Type_k) & Type_k == "Power")%>%
  ggplot(aes(x=p_1m, y = Percent, color = as.factor(alpha), label = percentDatasets))+
  geom_point()+
  geom_line()+
  #geom_text(hjust=0,vjust=-0.3)+
  #geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  #scale_linetype_manual(name = "Significance level", values = c(5), 
  #                    guide = guide_legend(override.aes = list(color = c("black"))))+
  facet_wrap(~maxMinute)+
  theme_bw()+
  labs(title = "Power of Conditional Otis Test Across Scenarios", subtitle = "N = 20; 5 capture occasions",x="Per minute detection probability",y="Power")+
  scale_color_discrete(name="Alpha")
```


We can see that less capture occasions means lower power - and more occurrence of Type 2 errors.
```{r}
dfOtisTest_Rate%>%
  filter(N == 20 & maxMinute == 5 & !is.na(Type_k) & Type_k != "True")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percent, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percent, color = as.factor(alpha)))+
 geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  scale_linetype_manual(name = "Significance level", values = c(5), 
                      guide = guide_legend(override.aes = list(color = c("black"))))+
   scale_color_discrete(name="Alpha")+
  facet_wrap(~Type_k)+
  theme_bw()+
  labs(title = "Error Rates Across Scenarios", subtitle = "N = 20; 5 capture occasions")
```


testing that numbers make sense
```{r}
#  dftest = dfOtisTest_Power%>%
#   distinct(id,counts,.keep_all = TRUE)%>%
#   filter(combinationNumber == 51)%>%
#   select(id,counts,combinationNumber)%>%
#   group_by(counts)%>%
#   summarise(vals = n())
# dftest
# sum(dftest$vals)

```


Now let's do the same for the pOverall values. If at least one p_k can be calculated for the conditional test, it means that p can be calculated for the overall test. So same idea that the % of datasets used applies here. 
```{r}
dfOtisTest_Rate_Overall = dfOtisTest_Power%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  mutate(totalCases = n())%>%
  group_by(N,p_1m,alpha,maxMinute,combinationNumber,totalCases,Type)%>%
  summarise(nType=n())%>%
  ungroup()%>%
  mutate(Percent_Overall=round(nType/totalCases*100,2))
dfOtisTest_Rate_Overall = left_join(dfOtisTest_Rate_Overall, dfOtisPercent,by="combinationNumber")
dfOtisTest_Rate_Overall
```


```{r}
dfOtisTest_Rate_Overall%>%
  filter(N == 20 & maxMinute == 10 & !is.na(Type) & Type == "Power")%>%
  ggplot(aes(x=p_1m, y = Percent_Overall, color = as.factor(alpha),label=percentDatasets))+
  geom_point()+
  geom_line()+
  geom_text(hjust=0,vjust=0)+
  #geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  #scale_linetype_manual(name = "Significance level", values = c(5), 
  #                    guide = guide_legend(override.aes = list(color = c("black"))))+
   scale_color_discrete(name="Alpha")+
  facet_wrap(~Type)+
  theme_bw()+
  labs(title = "Power Across Scenarios With General Otis Test", subtitle = "N = 20; 10 capture occasions",x="Per minute detection probability",y="Power")
```

```{r}
dfOtisTest_Rate_Overall%>%
  filter(N == 20 & maxMinute == 5 & !is.na(Type) & Type == "Power")%>%
  ggplot(aes(x=p_1m, y = Percent_Overall, color = as.factor(alpha),label=percentDatasets))+
  geom_point()+
  geom_line()+
  geom_text(hjust=0,vjust=0)+
  #geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  #scale_linetype_manual(name = "Significance level", values = c(5), 
  #                    guide = guide_legend(override.aes = list(color = c("black"))))+
   scale_color_discrete(name="Alpha")+
  facet_wrap(~Type)+
  theme_bw()+
  labs(title = "Power Across Scenarios With General Otis Test", subtitle = "N = 20; 5 capture occasions",x="Per minute detection probability",y="Power")
```

Even when the population is low, the power of the test is very strong as long as we have 10 capture occasions. So # of capture occasions affects power of the test more than the population size. 

remove significance of 5% on the Power and Type 2 part - only keep for Type 1
```{r}
dfOtisTest_Rate_Overall%>%
  filter(N == 10 & maxMinute == 10 & !is.na(Type) & Type == "Power")%>%
  ggplot(aes(x=p_1m, y = Percent_Overall, color = as.factor(alpha),label=percentDatasets))+
  geom_point()+
  geom_line()+
  geom_text(hjust=0,vjust=0)+
  #geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  #scale_linetype_manual(name = "Significance level", values = c(5), 
  #                    guide = guide_legend(override.aes = list(color = c("black"))))+
   scale_color_discrete(name="Alpha")+
  facet_wrap(~Type)+
  theme_bw()+
  labs(title = "Power Across Scenarios With General Otis Test", subtitle = "N = 10; 10 capture occasions",x="Per minute detection probability",y="Power")
```

### Power of Modified Otis Test
For each id, use 0.05/(t-2) to test for significance. If any of them reject, then overall we reject the null hypothesis. 

if t = 10, the probability of seeing at least one significant result due to chance is: 1-(1-0.05)^(8) = 0.337
if t = 5, the probability of seeing at least one significant result due to chance is:1-(1-0.05)^(3) = 0.142625

But this is assuming we always have t-2 tests -> and in the actual data we have less than t-2 tests... 


```{r}
head(dfOtisTest_Power)

dfOtisTest_Power%>%
  distinct(id,counts,.keep_all = TRUE)%>%
  group_by(id)%>%
  mutate(nTests = n(),
         bOverallReject = if_else(any(p_k_Otis < 0.05/n()),"Reject","Fail to Reject"))
```


```{r}
dfOtisTest_Rate_Overall_v2Org = dfOtisTest_Power%>%
  ungroup()%>%
  select(id, N,p_1m,alpha,maxMinute,combinationNumber, counts, p_k_Otis) %>%
  unique() %>%
  group_by(id, N,p_1m,alpha,maxMinute,combinationNumber)%>%
  filter(!is.na(p_k_Otis))%>%
  summarize(bOverallReject = if_else(any(p_k_Otis < 0.05/n()),"Reject","Fail to Reject"))

# dfOtisTest_Rate_Overall_v2Org = dfOtisTest_Power%>%
#   ungroup()%>%
#   distinct(id,counts,.keep_all = TRUE)%>%
#   filter(!is.na(p_k_Otis))%>%
#   group_by(id)%>%
#   mutate(nTests = n(),
#          bOverallReject = if_else(any(p_k_Otis < 0.05/nTests),"Reject","Fail to Reject"))


dfOtisTest_Rate_Overall_v2Org = dfOtisTest_Rate_Overall_v2Org%>%
  mutate(TypeModified = case_when(
    bOverallReject == "Reject" & alpha == 0 ~ "Type 1",
    bOverallReject == "Fail to Reject" & alpha != 0 ~ "Type 2",
    bOverallReject == "Reject"  & alpha !=0 ~ "Power",
    bOverallReject == "Fail to Reject" & alpha == 0 ~ "True"
  )
         )
```

```{r}
dfOtisTest_Rate_Overall_v2 = dfOtisTest_Rate_Overall_v2Org%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  mutate(totalCases = n())%>%
  group_by(N,p_1m,alpha,maxMinute,combinationNumber,totalCases,TypeModified)%>%
  summarise(nType=n())%>%
  ungroup()%>%
  mutate(Percent_Modified=round(nType/totalCases*100,2))
dfOtisTest_Rate_Overall_v2 = left_join(dfOtisTest_Rate_Overall_v2, dfOtisPercent,by="combinationNumber")
dfOtisTest_Rate_Overall_v2
```



The type 1 error rate is not more than 1%..
```{r}
dfOtisTest_Rate_Overall_v2%>%
  filter(N == 20 & maxMinute == 10 & !is.na(TypeModified) & TypeModified != "True")%>%
  ggplot(aes(x=p_1m, y = Percent_Modified, color = as.factor(alpha)))+
  geom_point()+
  geom_line()+
  #geom_text(hjust=0,vjust=0)+
  geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  scale_linetype_manual(name = "Significance level", values = c(5), 
                      guide = guide_legend(override.aes = list(color = c("black"))))+
   scale_color_discrete(name="Alpha")+
  facet_wrap(~TypeModified)+
  theme_bw()+
  labs(title = "Error Rates Across Scenarios With Modified Otis Test", subtitle = "N = 20; 10 capture occasions",x="Per minute detection probability",y="Power")
```

1% error rate -> more than the expected too. I 
```{r}
0.05/(10-2)
dfOtisTest_Rate_Overall_v2%>%
  filter(N == 20 & maxMinute == 10 & !is.na(TypeModified) & TypeModified == "Type 1")
```

```{r}
dfOtisTest_Rate_Overall_v2Org%>%
  filter(combinationNumber == 52)
```
```{r}
dfOtisTest_Power%>%
  filter(id == "52_116")
```


```{r}
dfPowerOtisCombined= left_join(dfOtisTest_Rate, dfOtisTest_Rate_Overall[,c("combinationNumber","Type","Percent_Overall")], by= c("combinationNumber","Type_k"="Type"))%>%
  rename("Conditional Test"="Percent",
         "Full Test" = "Percent_Overall")


dfPowerOtisCombined= left_join(dfPowerOtisCombined, dfOtisTest_Rate_Overall_v2[,c("combinationNumber","TypeModified","Percent_Modified")], by= c("combinationNumber","Type_k"="TypeModified"))%>%
  rename("Modified Test" = "Percent_Modified")
```

```{r}
head(dfPowerOtisCombined)
```

Side by side comparison shows that the overall test is stronger
```{r}
dfPowerOtisCombined%>%
  pivot_longer(cols= c("Conditional Test","Full Test","Modified Test"), names_to = "ModelType", values_to ="Percentages")%>%
  filter(N == 20 & maxMinute == 10 & !is.na(Type_k) & Type_k == "Power")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 10 capture occasions",x="Per minute detection probability",y="Power")+
   scale_color_discrete(name="Alpha")
```


## CJS

### Estimates

Curious about how my estimates of p differ from the true values

```{r}
dfClosure_CJS%>%
  filter(N==20 & maxMinute == 10)
```


```{r}
dfClosure_CJS_Estimate= dfClosure_CJS%>%
  filter(N==20 & maxMinute == 10 & paramSimple=="p")%>%
  mutate(bConstrained=if_else(bConstrained == 0,"Unconstrained","Constrained"))

ggplot(dfClosure_CJS_Estimate)+ 
  geom_boxplot(aes(x=as.factor(p_1m), y = CJS_estimate, color = as.factor(alpha)))+ 
  facet_wrap(~bConstrained)+
  theme_bw()+
  labs(title = "Estimates of Per Minute Detection Probability",subtitle = "N = 20;10 capture occassions.",x="Per minute detection probability",y="Estimate")+
  scale_color_discrete(name="Alpha")
```

Compare estimates from constrained model to Huggin's estimates. 

```{r}
dfClosure %>%
  filter(bConstrained == 1 & N == 20 & maxMinute == 10)%>%
  pivot_longer(cols = c("CJS_estimate","HugginsEstimate"), names_to = "ModelType", values_to = "pEstimate")%>%
  mutate(ModelType= ifelse(ModelType == "CJS_estimate","CJS","Huggins"))%>%
  ggplot()+ 
  geom_boxplot(aes(x=as.factor(p_1m), y = pEstimate, color = as.factor(alpha)))+ 
  facet_wrap(~ModelType)+
  theme_bw()+
  scale_color_discrete(name = "Alpha")+
  labs(title = "Estimates of Detection Probability Across Scenarios",subtitle = "N = 20; 10 capture occasions",x="Per minute detection probability", y = "Estimate")
```



Now estimates of "alpha" - perfect
```{r}
dfClosure_CJS%>%
  filter(N==20 & maxMinute == 10 & paramSimple=="phi" & bConstrained==0)%>%
  mutate(alphaHat = 1-CJS_estimate)%>%
ggplot()+ 
  geom_boxplot(aes(x=as.factor(p_1m), y = alphaHat, color = as.factor(alpha)))+ 
  theme_bw()+
  scale_color_discrete(name = "Alpha")+
  labs(title = "Estimates of Movement Probability Across Scenarios",subtitle = "N = 20; 10 capture occasions",x="Per minute detection probability", y = "Estimate")
```

```{r}
dfClosure_CJS%>%
  filter(paramSimple=="phi" & N==20 & maxMinute == 10 & bConstrained==0)%>%
  group_by(p_1m,alpha,N,maxMinute,bConstrained)%>%
  summarise(median = median(1-CJS_estimate))
```


Summary stats to look at

```{r}
head(dfClosure)
```

Start with Huggins summary stats
```{r}
dfSummaryStatsH = dfClosure %>%
  distinct(id,.keep_all = TRUE)%>%
    group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
    summarise(AvgPhat = mean(HugginsEstimate,na.rm=TRUE),
              medianPhat= median(HugginsEstimate,na.rm=TRUE),
              AvgPhatSE= mean(se,na.rm=TRUE),
              sdPhat = sd(HugginsEstimate,na.rm=TRUE),
              biasPhat = AvgPhat - p_1m,
              mse = mean(squaredError,na.rm=TRUE),
              bias_SE_Phat = AvgPhatSE - sdPhat,
              coverage = sum(bCoverage,na.rm=TRUE)/sum(!is.na(HugginsEstimate)), # changed this from n() to sum(!is.na()) because I have some NA values for my estimates
              avgWidth = mean(width,na.rm=TRUE),
              relPhatBias = round(biasPhat/p_1m*100,2),
              modelType = "Huggins",
              avgUCL= mean(ucl,na.rm=TRUE),
              avgLCL = mean(lcl, na.rm=TRUE),
              AvgPhatVar= mean(se^2,na.rm=TRUE))
dfSummaryStatsH = dfSummaryStatsH[!duplicated(dfSummaryStatsH),]
dfSummaryStatsH%>%
  arrange(combinationNumber)%>%
  select(p_1m,alpha,N,maxMinute,combinationNumber,AvgPhat,mse,AvgPhatSE,relPhatBias,coverage)
```

Summary stats for unconstrained model
```{r}
dfSummaryStatsC =dfClosure_CJS%>%
  filter(paramSimple=="p"& bConstrained == 1)
dfSummaryStatsC$bCoverage = ifelse(dfSummaryStatsC$p_1m >= dfSummaryStatsC$lcl & dfSummaryStatsC$p_1m <= dfSummaryStatsC$ucl,1,0)
dfSummaryStatsC$squaredError = (dfSummaryStatsC$CJS_estimate - dfSummaryStatsC$p_1m)^2
dfSummaryStatsC$width = dfSummaryStatsC$ucl - dfSummaryStatsC$lcl

dfSummaryStatsC = dfSummaryStatsC %>%
  distinct(id,.keep_all = TRUE)%>%
    group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
    summarise(AvgPhat = mean(CJS_estimate,na.rm=TRUE),
              medianPhat= median(CJS_estimate,na.rm=TRUE),
              AvgPhatSE= mean(se,na.rm=TRUE),
              sdPhat = sd(CJS_estimate,na.rm=TRUE),
              biasPhat = AvgPhat - p_1m,
              mse = mean(squaredError,na.rm=TRUE),
              bias_SE_Phat = AvgPhatSE - sdPhat,
              coverage = sum(bCoverage,na.rm=TRUE)/sum(!is.na(CJS_estimate)), # changed this from n() to sum(!is.na()) because I have some NA values for my estimates
              avgWidth = mean(width,na.rm=TRUE),
              relPhatBias = round(biasPhat/p_1m*100,2),
              modelType = "CJS Constrained",
              avgUCL= mean(ucl,na.rm=TRUE),
              avgLCL = mean(lcl, na.rm=TRUE),
              AvgPhatVar= mean(se^2,na.rm=TRUE))
dfSummaryStatsC = dfSummaryStatsC[!duplicated(dfSummaryStatsC),]
dfSummaryStatsC%>%
  arrange(combinationNumber)%>%
  select(p_1m,alpha,N,maxMinute,combinationNumber,AvgPhat,mse,AvgPhatSE,relPhatBias,coverage)


```

Summary stats for unconstrained model 

Avg p hat values are significantly biased - esp when alpha is low...
```{r}
dfSummaryStatsU =dfClosure_CJS%>%
  filter(paramSimple=="p"& bConstrained == 0)
dfSummaryStatsU$bCoverage = ifelse(dfSummaryStatsU$p_1m >= dfSummaryStatsU$lcl & dfSummaryStatsU$p_1m <= dfSummaryStatsU$ucl,1,0)
dfSummaryStatsU$squaredError = (dfSummaryStatsU$CJS_estimate - dfSummaryStatsU$p_1m)^2
dfSummaryStatsU$width = dfSummaryStatsU$ucl - dfSummaryStatsU$lcl

dfSummaryStatsU = dfSummaryStatsU %>%
  distinct(id,.keep_all = TRUE)%>%
    group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
    summarise(AvgPhat = mean(CJS_estimate,na.rm=TRUE),
              medianPhat= median(CJS_estimate,na.rm=TRUE),
              AvgPhatSE= mean(se,na.rm=TRUE),
              sdPhat = sd(CJS_estimate,na.rm=TRUE),
              biasPhat = AvgPhat - p_1m,
              mse = mean(squaredError,na.rm=TRUE),
              bias_SE_Phat = AvgPhatSE - sdPhat,
              coverage = sum(bCoverage,na.rm=TRUE)/sum(!is.na(CJS_estimate)), # changed this from n() to sum(!is.na()) because I have some NA values for my estimates
              avgWidth = mean(width,na.rm=TRUE),
              relPhatBias = round(biasPhat/p_1m*100,2),
              modelType = "CJS Unconstrained",
              avgUCL= mean(ucl,na.rm=TRUE),
              avgLCL = mean(lcl, na.rm=TRUE),
              AvgPhatVar= mean(se^2,na.rm=TRUE))
dfSummaryStatsU = dfSummaryStatsU[!duplicated(dfSummaryStatsU),]
dfSummaryStatsU%>%
  arrange(combinationNumber)%>%
  select(p_1m,alpha,N,maxMinute,combinationNumber,AvgPhat,mse,AvgPhatSE,relPhatBias,coverage)

```

Lots of estimates of 1...
```{r}
dfClosure_CJS%>%
  filter(paramSimple=="p"& bConstrained == 0 & combinationNumber == 1)

dfClosure_CJS%>%
  filter(id == "1_1")
dfOriginalData%>%
  filter(id == "1_1")
```


```{r}
dfCompareEstimates = rbind(dfSummaryStatsH, dfSummaryStatsC, dfSummaryStatsU)
head(dfCompareEstimates)
```

CJS unconstrained model not the best when N is low and capture ocasions is low.

Unconstrained model is horrible when capture occasions is 5... not sure why though
so basically unconstrained model is great when we have high population & lots of movement, else its pretty bad - overestimates detection probability even when there is no movement.
```{r}
dfCompareEstimates %>%
  filter(N==20 & maxMinute== 10)%>%
ggplot(aes(x=p_1m, y = AvgPhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Average Probability Estimates", subtitle = "N = 20; 10 capture occcasions",x= "Per Minute Detection Probability",y = "Average Probability Estimate")+
  scale_color_discrete(name = "Alpha")


dfCompareEstimates %>%
  filter(N==10 & maxMinute== 10)%>%
ggplot(aes(x=p_1m, y = AvgPhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Average Probability Estimates", subtitle = "N = 10; 10 capture occcasions",x= "Per Minute Detection Probability",y = "Average Probability Estimate")+
  scale_color_discrete(name = "Alpha")

dfCompareEstimates %>%
  filter(N==20 & maxMinute== 5)%>%
ggplot(aes(x=p_1m, y = AvgPhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Average Probability Estimates", subtitle = "N = 20; 5 capture occcasions",x= "Per Minute Detection Probability",y = "Average Probability Estimate")+
  scale_color_discrete(name = "Alpha")

dfCompareEstimates %>%
  filter(N==10 & maxMinute== 5)%>%
ggplot(aes(x=p_1m, y = AvgPhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Average Probability Estimates", subtitle = "N = 10; 5 capture occcasions",x= "Per Minute Detection Probability",y = "Average Probability Estimate")+
  scale_color_discrete(name = "Alpha")
```


Rel bias
```{r}
dfCompareEstimates %>%
  filter(N==20 & maxMinute== 10)%>%
ggplot(aes(x=p_1m, y = relPhatBias, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Relative Bias", subtitle = "N = 20; 10 capture occcasions",x= "Per Minute Detection Probability",y = "Bias")+
  scale_color_discrete(name = "Alpha")


dfCompareEstimates %>%
  filter(N==10 & maxMinute== 10)%>%
ggplot(aes(x=p_1m, y = relPhatBias, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Relative Bias", subtitle = "N = 10; 10 capture occcasions",x= "Per Minute Detection Probability",y = "Bias")+
  scale_color_discrete(name = "Alpha")

dfCompareEstimates %>%
  filter(N==20 & maxMinute== 5)%>%
ggplot(aes(x=p_1m, y = relPhatBias, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Relative Bias", subtitle = "N = 20; 5 capture occcasions",x= "Per Minute Detection Probability",y = "Bias")+
  scale_color_discrete(name = "Alpha")

dfCompareEstimates %>%
  filter(N==10 & maxMinute== 5)%>%
ggplot(aes(x=p_1m, y = relPhatBias, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Relative Bias", subtitle = "N = 10; 5 capture occcasions",x= "Per Minute Detection Probability",y = "Bias")+
  scale_color_discrete(name = "Alpha")
```

SE
```{r}
dfCompareEstimates %>%
  filter(N==20 & maxMinute== 10)%>%
ggplot(aes(x=p_1m, y = AvgPhatSE, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Average Probability SE", subtitle = "N = 20; 10 capture occcasions",x= "Per Minute Detection Probability",y = "Average Probability SE")+
  scale_color_discrete(name = "Alpha")


dfCompareEstimates %>%
  filter(N==10 & maxMinute== 10)%>%
ggplot(aes(x=p_1m, y = AvgPhatSE, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Average Probability SE", subtitle = "N = 10; 10 capture occcasions",x= "Per Minute Detection Probability",y = "Average Probability SE")+
  scale_color_discrete(name = "Alpha")

dfCompareEstimates %>%
  filter(N==20 & maxMinute== 5)%>%
ggplot(aes(x=p_1m, y = AvgPhatSE, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Average Probability SE", subtitle = "N = 20; 5 capture occcasions",x= "Per Minute Detection Probability",y = "Average Probability SE")+
  scale_color_discrete(name = "Alpha")

dfCompareEstimates %>%
  filter(N==10 & maxMinute== 5)%>%
ggplot(aes(x=p_1m, y = AvgPhatSE, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Average Probability SE", subtitle = "N = 10; 5 capture occcasions",x= "Per Minute Detection Probability",y = "Average Probability SE")+
  scale_color_discrete(name = "Alpha")
```

Coverage

```{r}
dfCompareEstimates %>%
  filter(N==20 & maxMinute== 10)%>%
ggplot(aes(x=p_1m, y = coverage, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Coverage", subtitle = "N = 20; 10 capture occcasions",x= "Per Minute Detection Probability",y = "Bias")+
  scale_color_discrete(name = "Alpha")


dfCompareEstimates %>%
  filter(N==10 & maxMinute== 10)%>%
ggplot(aes(x=p_1m, y = coverage, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Coverage", subtitle = "N = 10; 10 capture occcasions",x= "Per Minute Detection Probability",y = "Bias")+
  scale_color_discrete(name = "Alpha")

dfCompareEstimates %>%
  filter(N==20 & maxMinute== 5)%>%
ggplot(aes(x=p_1m, y = coverage, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Coverage", subtitle = "N = 20; 5 capture occcasions",x= "Per Minute Detection Probability",y = "Bias")+
  scale_color_discrete(name = "Alpha")

dfCompareEstimates %>%
  filter(N==10 & maxMinute== 5)%>%
ggplot(aes(x=p_1m, y = coverage, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~modelType)+
  theme_bw()+ 
  labs(title  = "Coverage", subtitle = "N = 10; 5 capture occcasions",x= "Per Minute Detection Probability",y = "Bias")+
  scale_color_discrete(name = "Alpha")
```


### Closure Test


- weird p values (ie id == 1_1)
- weird chat (ie id == 8_476)
- confirm how LRT is supposed to work D_S - D_L where S is a subset of L 
- 1_1 vs 62_2 shows how the order of results isn't always that the unconstrained model appears second

I used deviance to figure out what the p value should be. Theoretically I should be using c_hat to adjust, but for now I will keep it as it is. 
```{r}
dfClosure_CJS%>%
  filter(N == 20 & maxMinute == 10 & !duplicated(id) )%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = p_CJS, color = as.factor(alpha)))+ 
   geom_hline(aes(yintercept=0.05), color = "grey30")+
  scale_color_discrete(name="Alpha")+
  theme_bw()+
  labs(title = "Results of LRT (Closed model vs CJS)", subtitle = "Significance level of 0.05", x = "Per minute detection probability", y = "p LRT")
```




### Power

```{r}
head(dfClosure_CJS)
```


```{r}
dfCJS_Power = dfClosure_CJS%>%
  filter(paramSimple == "p")%>%
  mutate(Type = case_when(
    p_CJS < 0.05 & alpha == 0 ~ "Type 1",
    p_CJS > 0.05 & alpha != 0 ~ "Type 2",
    p_CJS < 0.05 & alpha !=0 ~ "Power",
    p_CJS > 0.05 & alpha == 0 ~ "True"
  )
         )


```


```{r}
dfCJS_Power_Rate = dfCJS_Power%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  mutate(totalCases = n())%>%
  group_by(N,p_1m,alpha,maxMinute,combinationNumber,totalCases,Type)%>%
  summarise(nType=n())%>%
  ungroup()%>%
  mutate(Percent_CJS=round(nType/totalCases*100,2))
dfCJS_Power_Rate
```



```{r}
head(dfCJS_Power)
dfCJS_Power[is.na(dfCJS_Power$p_CJS),]
dfCJS_Power%>%
  filter(p_CJS==1)
```

So there IS a difference - just negligible
```{r}
dfOtisTest_Power%>%
  filter(!is.na(p_k_Otis)& combinationNumber== 3)%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  summarise(n = n(),
    percentDatasets= round(n()/1000,5)*100 )


dfCJS_Power%>%
  filter(combinationNumber == 3)%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  summarise(n=n(),
            percentDatasets= round(n()/1000,5)*100 )
```


```{r}
dfOtisTest_Power%>%
  filter(is.na(p_k_Otis))

dfOtisTest_Power%>%
  filter(id == "3_411")

dfCJS_Power%>%
  filter(id == "3_411")
```



```{r}
dfCJSPercent = dfCJS_Power%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  summarise(percentDatasets= round(n()/1000,5)*100 )
dfCJSPercent%>%
  arrange(percentDatasets)
dfOtisPercent %>%
  arrange(percentDatasets)
```



Very strong power... let's see what happens when capture occasions fall
```{r}
cutoff = data.frame( x = c(-Inf, Inf), y = 5, cutoff = factor(5) )
dfCJS_Power_Rate%>%
  filter(N == 20 & maxMinute == 10 & !is.na(Type) & Type != "True")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percent_CJS, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percent_CJS, color = as.factor(alpha)))+
  geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  scale_linetype_manual(name = "Significance level", values = c(5), 
                      guide = guide_legend(override.aes = list(color = c("black"))))+
  scale_color_discrete(name = "Alpha")+
  facet_wrap(~Type)+
  theme_bw()+
  labs(title = "Error Rates Across Scenarios", subtitle = "N = 20; 10 capture occasions")
```

```{r}
cutoff = data.frame( x = c(-Inf, Inf), y = 5, cutoff = factor(5) )
dfCJS_Power_Rate%>%
  filter(N == 20 & maxMinute == 5 & !is.na(Type) & Type != "True")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percent_CJS, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percent_CJS, color = as.factor(alpha)))+
   geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  scale_linetype_manual(name = "Significance level", values = c(5), 
                      guide = guide_legend(override.aes = list(color = c("black"))))+
  scale_color_discrete(name = "Alpha")+
  facet_wrap(~Type)+
  theme_bw()+
  labs(title = "Error Rates Across Scenarios", subtitle = "N = 20; 5 capture occasions")
```



```{r}
dfCJS_Power
```



```{r}
dfPowerOtisCombined
```


```{r}
dfPowerCombined= left_join(dfCJS_Power_Rate, dfPowerOtisCombined[,c("combinationNumber","Type_k","Conditional Test","Full Test","Modified Test"),], by= c("combinationNumber","Type"="Type_k"))%>%
  rename("LRT" = "Percent_CJS",
         "Conditional Otis Test" = "Conditional Test",
         "Full Otis Test" = "Full Test",
         "Modified Otis Test"="Modified Test")
```



```{r}
dfPowerCombined
```




```{r}
dfPowerCombined%>%
  pivot_longer(cols= c("Conditional Otis Test","Modified Otis Test","Full Otis Test","LRT"), names_to = "ModelType", values_to ="Percentages")%>%
  filter(N == 20 & maxMinute == 10 & !is.na(Type) & Type == "Power")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  scale_color_discrete(name = "Alpha")+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 10 capture occasions",y="Power",x="Per minute detection probability")
```

```{r}
dfPowerCombined%>%
   pivot_longer(cols= c("Conditional Otis Test","Modified Otis Test","Full Otis Test","LRT"), names_to = "ModelType", values_to ="Percentages")%>%
  filter(N == 20 & maxMinute == 5 & !is.na(Type) & Type == "Power")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  theme_bw()+
  scale_color_discrete(name="Alpha")+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 5 capture occasions")
```

# Boot Strapping 

Use the case when alpha = 0 for each value of p to recompute the critical value. We are doing this because we know that the asymptotics break down. 

Basically, find the value where we reject H0 when alpha = 0 5% of the time. 

So basically find the 5th percentile of p values for each per minute detection probability (when alpha = 0). Using this new cutoff, we can recompute power!


```{r}
dfOtisCutoff = dfOtisTest%>%
  filter(alpha == 0)%>%
  distinct(id,counts,.keep_all = TRUE)%>%
  group_by(combinationNumber,N,p_1m,maxMinute)%>%
  summarise(CV_Conditional_Otis = quantile(p_k_Otis,probs=0.05,na.rm=TRUE,names=TRUE),
            CV_Overall_Otis =  quantile(pOverall_Otis,probs=0.05,na.rm=TRUE,names=TRUE))

dfCJSCutoff = dfClosure_CJS%>%
  filter(alpha == 0)%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber,N,p_1m,maxMinute)%>%
  summarise(CV_LRT = quantile(p_CJS ,probs=0.05,na.rm=TRUE,names=TRUE))

dfCutoff = left_join(dfOtisCutoff, dfCJSCutoff,by= c("combinationNumber","N","maxMinute","p_1m"))
head(dfCutoff)
```



## Otis 

```{r}
dfOtisTest_Power_Boot = left_join(dfOtisTest, dfCutoff[,c("N","p_1m","maxMinute","CV_Conditional_Otis","CV_Overall_Otis")],by=c("N","p_1m","maxMinute"))
```

Ask about "=" sign
```{r}
dfOtisTest_Power_Boot = dfOtisTest_Power_Boot%>%
  mutate(Type_k = case_when(
    p_k_Otis <= CV_Conditional_Otis & alpha == 0 ~ "Type 1",
    p_k_Otis > CV_Conditional_Otis & alpha != 0 ~ "Type 2",
    p_k_Otis <= CV_Conditional_Otis & alpha !=0 ~ "Power",
    p_k_Otis > CV_Conditional_Otis & alpha == 0 ~ "True"
  )
         )


dfOtisTest_Power_Boot = dfOtisTest_Power_Boot%>%
  mutate(Type = case_when(
    pOverall_Otis <= CV_Overall_Otis & alpha == 0 ~ "Type 1",
    pOverall_Otis > CV_Overall_Otis & alpha != 0 ~ "Type 2",
    pOverall_Otis <= CV_Overall_Otis & alpha !=0 ~ "Power",
    pOverall_Otis > CV_Overall_Otis & alpha == 0 ~ "True"
  )
         )
```


```{r}
dfOtisTest_Rate_Boot = dfOtisTest_Power_Boot%>%
  distinct(id,counts,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  mutate(totalCases = n())%>%
  group_by(N,p_1m,alpha,maxMinute,combinationNumber,totalCases,Type_k)%>%
  summarise(nType=n())%>%
  ungroup()%>%
  mutate(Percent=round(nType/totalCases*100,2))
dfOtisTest_Rate_Boot = left_join(dfOtisTest_Rate_Boot, dfOtisPercent,by="combinationNumber")
dfOtisTest_Rate_Boot
```


```{r}
dfOtisTest_Rate_Boot%>%
  filter(N == 20 & maxMinute == 10 & !is.na(Type_k) & Type_k == "Power")%>%
  ggplot(aes(x=p_1m, y = Percent, color = as.factor(alpha), label = percentDatasets))+
  geom_point()+
  geom_line()+
  geom_text(hjust=0,vjust=-0.3)+
  #geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  #scale_linetype_manual(name = "Significance level", values = c(5), 
  #                    guide = guide_legend(override.aes = list(color = c("black"))))+
  #facet_wrap(~maxMinute)+
  theme_bw()+
  labs(title = "Power of Conditional Otis Test Across Scenarios (Bootstrapped)", subtitle = "N = 20; 10 capture occasions",x="Per minute detection probability",y="Power")+
  scale_color_discrete(name="Alpha")
```

This is the important part -> when capture scenarios is low, bootstrapped versions are *much* better
```{r}
dfOtisTest_Rate_Boot%>%
  filter(N == 20 & maxMinute == 5 & !is.na(Type_k) & Type_k == "Power")%>%
  ggplot(aes(x=p_1m, y = Percent, color = as.factor(alpha), label = percentDatasets))+
  geom_point()+
  geom_line()+
  geom_text(hjust=0,vjust=-0.3)+
  #geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  #scale_linetype_manual(name = "Significance level", values = c(5), 
  #                    guide = guide_legend(override.aes = list(color = c("black"))))+
  #facet_wrap(~maxMinute)+
  theme_bw()+
  labs(title = "Power of Conditional Otis Test Across Scenarios (Bootstrapped)", subtitle = "N = 20; 5 capture occasions",x="Per minute detection probability",y="Power")+
  scale_color_discrete(name="Alpha")
```

```{r}
dfOtisTest_Rate_Boot%>%
  filter(N == 20 & maxMinute == 5 & !is.na(Type_k) & Type_k %in% c("Power","Type 1"))%>%
  ggplot(aes(x=p_1m, y = Percent, color = as.factor(alpha), label = percentDatasets))+
  geom_point()+
  geom_line()+
  #geom_text(hjust=0,vjust=-0.3)+
  geom_hline(data = dfOtisTest_Rate_Boot %>% filter(Type_k == "Type 1"),
            aes(yintercept=5,linetype="5 %"),color="black")+
  scale_linetype_manual(name = "Significance level", values = c(5), 
                      guide = guide_legend(override.aes = list(color = c("black"))))+
  facet_wrap(~Type_k)+
  theme_bw()+
  labs(title = "Power of Conditional Otis Test Across Scenarios (Bootstrapped)", subtitle = "N = 20; 5 capture occasions",x="Per minute detection probability",y="Power")+
  scale_color_discrete(name="Alpha")
```




```{r}
dfOtisTest_Rate_Overall_Boot = dfOtisTest_Power_Boot%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  mutate(totalCases = n())%>%
  group_by(N,p_1m,alpha,maxMinute,combinationNumber,totalCases,Type)%>%
  summarise(nType=n())%>%
  ungroup()%>%
  mutate(Percent_Overall=round(nType/totalCases*100,2))
dfOtisTest_Rate_Overall_Boot = left_join(dfOtisTest_Rate_Overall_Boot, dfOtisPercent,by="combinationNumber")
dfOtisTest_Rate_Overall_Boot
```

```{r}
dfOtisTest_Rate_Overall_Boot%>%
  filter(N == 20 & maxMinute == 10 & !is.na(Type) & Type == "Power")%>%
  ggplot(aes(x=p_1m, y = Percent_Overall, color = as.factor(alpha), label = percentDatasets))+
  geom_point()+
  geom_line()+
  geom_text(hjust=0,vjust=-0.3)+
  #geom_hline(aes(yintercept=5,linetype="5 %"),color="black")+
  #scale_linetype_manual(name = "Significance level", values = c(5), 
  #                    guide = guide_legend(override.aes = list(color = c("black"))))+
  #facet_wrap(~maxMinute)+
  theme_bw()+
  labs(title = "Power of Overall Otis Test Across Scenarios (Bootstrapped)", subtitle = "N = 20; 10 capture occasions",x="Per minute detection probability",y="Power")+
  scale_color_discrete(name="Alpha")
```



```{r}
dfOtisTest_Rate_Overall_Boot%>%
  filter(N == 20 & maxMinute == 5 & !is.na(Type) & Type %in% c("Power","Type 1"))%>%
  ggplot(aes(x=p_1m, y = Percent_Overall, color = as.factor(alpha), label = percentDatasets))+
  geom_point()+
  geom_line()+
  #geom_text(hjust=0,vjust=-0.3)+
  geom_hline(data = dfOtisTest_Rate_Overall_Boot %>% filter(Type == "Type 1"),
            aes(yintercept=5,linetype="5 %"),color="black")+
  scale_linetype_manual(name = "Significance level", values = c(5), 
                      guide = guide_legend(override.aes = list(color = c("black"))))+
  facet_wrap(~Type)+
  theme_bw()+
  labs(title = "Power of Overall Otis Test Across Scenarios (Bootstrapped)", subtitle = "N = 20; 5 capture occasions",x="Per minute detection probability",y="Power")+
  scale_color_discrete(name="Alpha")
```



```{r}
dfOtisTest_Rate_Overall_Boot_v2Org = dfOtisTest_Power_Boot%>%
  ungroup()%>%
  select(id, N,p_1m,alpha,maxMinute,combinationNumber, counts, p_k_Otis) %>%
  unique() %>%
  group_by(id, N,p_1m,alpha,maxMinute,combinationNumber)%>%
  filter(!is.na(p_k_Otis))%>%
  summarize(bOverallReject = if_else(any(p_k_Otis < 0.05/n()),"Reject","Fail to Reject"))


dfOtisTest_Rate_Overall_Boot_v2Org = dfOtisTest_Rate_Overall_Boot_v2Org%>%
  mutate(TypeModified = case_when(
    bOverallReject == "Reject" & alpha == 0 ~ "Type 1",
    bOverallReject == "Fail to Reject" & alpha != 0 ~ "Type 2",
    bOverallReject == "Reject"  & alpha !=0 ~ "Power",
    bOverallReject == "Fail to Reject" & alpha == 0 ~ "True"
  )
         )
```

```{r}
dfOtisTest_Rate_Overall_Boot_v2 = dfOtisTest_Rate_Overall_Boot_v2Org%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  mutate(totalCases = n())%>%
  group_by(N,p_1m,alpha,maxMinute,combinationNumber,totalCases,TypeModified)%>%
  summarise(nType=n())%>%
  ungroup()%>%
  mutate(Percent_Modified=round(nType/totalCases*100,2))
dfOtisTest_Rate_Overall_Boot_v2 = left_join(dfOtisTest_Rate_Overall_Boot_v2, dfOtisPercent,by="combinationNumber")
dfOtisTest_Rate_Overall_Boot_v2
```


```{r}
dfOtisTest_Rate_Overall_Boot_v2%>%
  filter(N == 20 & maxMinute == 10 & !is.na(TypeModified) & TypeModified %in% c("Power","Type 1"))%>%
  ggplot(aes(x=p_1m, y = Percent_Modified, color = as.factor(alpha)))+
  geom_point()+
  geom_line()+
  #geom_text(hjust=0,vjust=0)+
  geom_hline(data = dfOtisTest_Rate_Overall_Boot_v2 %>% filter(TypeModified == "Type 1"),aes(yintercept=5,linetype="5 %"),color="black")+
  scale_linetype_manual(name = "Significance level", values = c(5), 
                      guide = guide_legend(override.aes = list(color = c("black"))))+
   scale_color_discrete(name="Alpha")+
  facet_wrap(~TypeModified)+
  theme_bw()+
  labs(title = "Error Rates Across Scenarios With Modified Otis Test", subtitle = "N = 20; 10 capture occasions",x="Per minute detection probability",y="Power")
```

```{r}
dfPowerOtisCombinedBoot= left_join(dfOtisTest_Rate_Boot, dfOtisTest_Rate_Overall_Boot[,c("combinationNumber","Type","Percent_Overall")], by= c("combinationNumber","Type_k"="Type"))%>%
  rename("Conditional Test Boot"="Percent",
         "Full Test Boot" = "Percent_Overall")


dfPowerOtisCombinedBoot= left_join(dfPowerOtisCombinedBoot, dfOtisTest_Rate_Overall_Boot_v2[,c("combinationNumber","TypeModified","Percent_Modified")], by= c("combinationNumber","Type_k"="TypeModified"))%>%
  rename("Modified Test Boot" = "Percent_Modified")
```

```{r}
head(dfPowerOtisCombinedBoot)
```



Side by side comparison shows that the overall test is stronger
```{r}
dfPowerOtisCombinedBoot%>%
  pivot_longer(cols= c("Conditional Test Boot","Full Test Boot","Modified Test Boot"), names_to = "ModelType", values_to ="Percentages")%>%
  filter(N == 20 & maxMinute == 10 & !is.na(Type_k) & Type_k == "Power")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 10 capture occasions",x="Per minute detection probability",y="Power")+
   scale_color_discrete(name="Alpha")
```

```{r}
dfPowerOtisCombinedBoot%>%
  pivot_longer(cols= c("Conditional Test Boot","Full Test Boot","Modified Test Boot"), names_to = "ModelType", values_to ="Percentages")%>%
  filter(N == 20 & maxMinute == 5 & !is.na(Type_k) & Type_k == "Power")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 5 capture occasions",x="Per minute detection probability",y="Power")+
   scale_color_discrete(name="Alpha")
```



## CJS


```{r}
dfCJS_Power_Boot = left_join(dfClosure_CJS, dfCutoff[,c("N","p_1m","maxMinute","CV_LRT")],by=c("N","p_1m","maxMinute"))

dfCJS_Power_Boot = dfCJS_Power_Boot%>%
  filter(paramSimple == "p")%>%
  mutate(Type = case_when(
    p_CJS <= CV_LRT & alpha == 0 ~ "Type 1",
    p_CJS > CV_LRT & alpha != 0 ~ "Type 2",
    p_CJS <= CV_LRT & alpha !=0 ~ "Power",
    p_CJS > CV_LRT & alpha == 0 ~ "True"
  )
         )


```


```{r}
dfCJS_Power_Rate_Boot = dfCJS_Power_Boot%>%
  distinct(id,.keep_all = TRUE)%>%
  group_by(combinationNumber)%>%
  mutate(totalCases = n())%>%
  group_by(N,p_1m,alpha,maxMinute,combinationNumber,totalCases,Type)%>%
  summarise(nType=n())%>%
  ungroup()%>%
  mutate(Percent_CJS_Boot=round(nType/totalCases*100,2))
dfCJS_Power_Rate_Boot
```

```{r}
dfCJS_Power_Rate_Boot%>%
  filter(N == 20 & maxMinute == 5 & !is.na(Type) & Type %in% c("Power","Type 1"))%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percent_CJS_Boot, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percent_CJS_Boot, color = as.factor(alpha)))+
   geom_hline(data = dfCJS_Power_Rate_Boot%>%filter(Type == "Type 1"),aes(yintercept=5,linetype="5 %"),color="black")+
  scale_linetype_manual(name = "Significance level", values = c(5), 
                      guide = guide_legend(override.aes = list(color = c("black"))))+
  scale_color_discrete(name = "Alpha")+
  facet_wrap(~Type)+
  theme_bw()+
  labs(title = "Error Rates Across Scenarios", subtitle = "N = 20; 5 capture occasions")
```


```{r}
dfPowerCombinedBoot= left_join(dfCJS_Power_Rate_Boot, dfPowerOtisCombinedBoot[,c("combinationNumber","Type_k","Conditional Test Boot","Full Test Boot","Modified Test Boot"),], by= c("combinationNumber","Type"="Type_k"))%>%
  rename("LRT Boot" = "Percent_CJS_Boot",
         "Conditional Otis Test Boot" = "Conditional Test Boot",
         "Full Otis Test Boot" = "Full Test Boot",
         "Modified Otis Test Boot"="Modified Test Boot")
```




```{r}
dfPowerCombinedBoot%>%
  pivot_longer(cols= c("Conditional Otis Test Boot","Modified Otis Test Boot","Full Otis Test Boot","LRT Boot"), names_to = "ModelType", values_to ="Percentages")%>%
  filter(N == 20 & maxMinute == 10 & !is.na(Type) & Type == "Power")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  scale_color_discrete(name = "Alpha")+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 10 capture occasions",y="Power",x="Per minute detection probability")
```



# Overall Comparison 

```{r}
dfOverallPower = left_join(dfPowerCombined, dfPowerCombinedBoot[,c("combinationNumber","Conditional Otis Test Boot","LRT Boot","Full Otis Test Boot","Modified Otis Test Boot","Type")], by = c("combinationNumber","Type"))
```

```{r}
head(dfOverallPower)
```




```{r}
dfOverallPower%>%
   filter(N == 20 & maxMinute == 10 & !is.na(Type) & Type == "Power")%>%
  pivot_longer(cols= c("Full Otis Test Boot","Full Otis Test"), names_to = "ModelType", values_to ="Percentages")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  scale_color_discrete(name = "Alpha")+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 10 capture occasions",y="Power",x="Per minute detection probability")

dfOverallPower%>%
   filter(N == 20 & maxMinute == 5 & !is.na(Type) & Type == "Power")%>%
  pivot_longer(cols= c("Full Otis Test Boot","Full Otis Test"), names_to = "ModelType", values_to ="Percentages")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  scale_color_discrete(name = "Alpha")+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 5 capture occasions",y="Power",x="Per minute detection probability")
```



```{r}
dfOverallPower%>%
   filter(N == 20 & maxMinute == 10 & !is.na(Type) & Type == "Power")%>%
  pivot_longer(cols= c("Modified Otis Test Boot","Modified Otis Test"), names_to = "ModelType", values_to ="Percentages")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  scale_color_discrete(name = "Alpha")+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 10 capture occasions",y="Power",x="Per minute detection probability")

dfOverallPower%>%
   filter(N == 20 & maxMinute == 5 & !is.na(Type) & Type == "Power")%>%
  pivot_longer(cols= c("Modified Otis Test Boot","Modified Otis Test"), names_to = "ModelType", values_to ="Percentages")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  scale_color_discrete(name = "Alpha")+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 5 capture occasions",y="Power",x="Per minute detection probability")
```

No power when alpha -- 0.05 or 0.1. When alpha is 0.3 there is also no power... that is weird
```{r}
dfOtisTest_Rate_Overall_v2Org%>%
  filter(N==20 & p_1m == 0.1 & maxMinute == 5 )%>%
  group_by(alpha,TypeModified)%>%
  summarise(n())
```



```{r}
dfOverallPower%>%
   filter(N == 20 & maxMinute == 10 & !is.na(Type) & Type == "Power")%>%
  pivot_longer(cols= c("LRT Boot","LRT"), names_to = "ModelType", values_to ="Percentages")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  scale_color_discrete(name = "Alpha")+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 10 capture occasions",y="Power",x="Per minute detection probability")

dfOverallPower%>%
   filter(N == 20 & maxMinute == 5 & !is.na(Type) & Type == "Power")%>%
  pivot_longer(cols= c("LRT Boot","LRT"), names_to = "ModelType", values_to ="Percentages")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  scale_color_discrete(name = "Alpha")+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 5 capture occasions",y="Power",x="Per minute detection probability")
```




When capture occasions is small, LRT boot works the best when we have high capture probability & high alpha. When p_1m is low, lrt boot seems to give a lil power - but nothing great. 
```{r}
dfOverallPower%>%
   filter(N == 20 & maxMinute == 10 & !is.na(Type) & Type == "Power")%>%
  pivot_longer(cols= c("LRT Boot","LRT","Modified Otis Test Boot","Modified Otis Test","Full Otis Test Boot","Full Otis Test"), names_to = "ModelType", values_to ="Percentages")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType)+
  scale_color_discrete(name = "Alpha")+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 10 capture occasions",y="Power",x="Per minute detection probability")

dfOverallPower%>%
   filter(N == 20 & maxMinute == 5 & !is.na(Type) & Type == "Power")%>%
  pivot_longer(cols= c("LRT Boot","LRT","Modified Otis Test Boot","Modified Otis Test","Full Otis Test Boot","Full Otis Test"), names_to = "ModelType", values_to ="Percentages")%>%
  ggplot()+
  geom_point(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  geom_line(aes(x=p_1m, y = Percentages, color = as.factor(alpha)))+
  facet_wrap(~ModelType,ncol=2)+ # facet grid 4x2
  scale_color_discrete(name = "Alpha")+
  theme_bw()+
  labs(title = "Power Across Scenarios", subtitle = "N = 20; 5 capture occasions",y="Power",x="Per minute detection probability")
```





