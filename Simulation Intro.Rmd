---
title: "Simulation Intro"
author: "Tito"
date: "2023-01-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# To do

- find a way to incorporate c, time and heterogeneity into the generateDetects() function

# Load Functions
```{r}
library(tidyverse)
library(RMark)
source("./Functions/generatePointCount.R")
source("./Functions/runSimulation.R")
```


# Test Functions
```{r}
# set parameters
p_1m = 0.5  
maxMinute = 5
colsUse = paste0("Minute",1:maxMinute)
n_locations = 50
lambda = 15 # get this number from some data?
alpha=0.3

set.seed(1)
locationID = 1:n_locations
#N_i = rpois(n_locations,lambda)
N_i = rnbinom(n_locations, mu = lambda, size= 1)
dfPop = as.data.frame(cbind(locationID, N_i))

# generate counts
dfCount = generateDetects(dfPop$N_i, p_1m,maxMinute,10)

# show data in wide format
dfWide = detectsToCapHist(dfCount)
dfWide = left_join(dfWide, dfPop, by = "locationID")

dfMark = dfWide[,"ch"] # data for testing individual functions
```


Test out some individual functions:

Function to make formula dataframe
```{r}
generateFormula()
```
Test fitting a model using parameters (assuming we have data)

```{r}
# true population
sum(dfPop$N_i)
```

```{r}
fitModel(dfMark,strFormula = "~1",strModel="Closed", nMixtures = 3)
```

Investigate the effect of no interaction:
```{r}
fitModel(dfMark, strFormula = "~c+mixture",strModel="FullHet")
```

vs interaction:
```{r}
fitModel(dfMark, strFormula = "~c*mixture",strModel="FullHet")
```

Results of running a *single* simulation
```{r}
runSingleSimulation(N_i=20,p_1m=0.2,seed=10)
```

# Run Simulation

Test simulation with multiple runs, and only alpha changing (for now) (how do i stop the text file from showing??)
Also when i set a seed, way more biased results -> confused. Or maybe once again it's because of the small Ni I'm using?

Really weird result when nRuns = 10, lstMaxMin = 5, seed = 10,lstNi = 20 -> is it just because I have a small N?
```{r, results = "hide"}
dfSimulation1 = calculateStatistics(nRuns = 10, lstNi = c(20), lstP = c(0.2), lstAlpha = c(0,0.1,0.3),lstMaxMin = c(5),seed=10)
```


```{r}
dfSimulationEst = dfSimulation1[[1]]
dfSimulationParams= dfSimulation1[[2]]%>%
  filter(variable == "N")
dfSimulationParams_P = dfSimulation1[[2]]%>%
  filter(variable != "N")
dfSimulationHist = dfSimulation1[[3]]
```

```{r}
dfSimulationEst
dfSimulationParams
dfSimulationHist
```

```{r}
ggplot(dfSimulationParams)+ 
  geom_boxplot(aes(y = estimate,x = as.factor(combinationNumber)))+ 
  theme_light()
```

```{r}
dfSimulationParams%>%
  filter(estimate != max(estimate))%>%
ggplot()+ 
  geom_boxplot(aes(y = estimate,x = as.factor(combinationNumber)))+ 
  theme_light()
```


Try to see where weird estimates are coming from - doesn't look like ch is being repeated 
in the same order...
Could this just be a bad sample?

Sampled 10 individuals, each detected one time only within 5 occasions. The estimated detection probability is extremely small (thus resulting in the large N). 
```{r}
dfSim = detectsToCapHist(dfSimulationHist %>%
  filter(simulationNumber==12)) # simulationNumber == 1 or 2 shows that the number of sampled individuals isn't always 10 - which is how it should be 
dfSim

fitModel(dfSim[,"ch"])
```

When I increase n from 20 to around 676 (like in examples earlier) I get the following:
```{r}
dfSimulation1 = calculateStatistics(nRuns = 10, lstNi = c(676), lstP = c(0.2), lstAlpha = c(0,0.05,0.1,0.15,0.2,0.25,0.3),lstMaxMin = c(5),seed=10)
dfSimulationEst = dfSimulation1[[1]]
dfSimulationParams= dfSimulation1[[2]]%>%
  filter(variable == "N")
dfSimulationParams_P = dfSimulation1[[2]]%>%
  filter(variable != "N")
dfSimulationHist = dfSimulation1[[3]]
dfSimulationEst
dfSimulationParams
dfSimulationHist
```

Some illustrations:

Median seems to be increasing exponentially as alpha increases.
```{r}
ggplot(dfSimulationParams)+ 
  geom_boxplot(aes(y = estimate,x = as.factor(combinationNumber)))+ 
  theme_light()
```
Somewhat exponential then the rate of increase seems to fall -> I wonder if there's some value of alpha where it peaks and then estimates are stagnant? There has to be since once alpha is a certain value, no more splits can happen (depending on how many individuals we have).
```{r}
dfSimulationParams%>%
  group_by(p_1m,alpha,N,maxMinute,strFormula,combinationNumber)%>%
  summarise(median = median(estimate))%>%
  ggplot()+
  geom_point(aes(x=alpha,y=median))+ 
  geom_line(aes(x=alpha,y=median))+
  theme_light()+ 
  labs(title = "Median Population Estimates")
```


```{r}
dfSimulationParams%>%
  group_by(p_1m,alpha,N,maxMinute,strFormula,combinationNumber)%>%
  summarise(mean = mean(estimate))%>%
  ggplot()+
  geom_point(aes(x=alpha,y=mean))+ 
  geom_line(aes(x=alpha,y=mean))+
  theme_light()+
  labs(title = "Mean Population Estimates")
```

