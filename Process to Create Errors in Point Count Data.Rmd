---
title: "Process to Create Errors in Point Count Data"
author: "Tito"
date: "2022-11-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set Variables
```{r}
library(tidyverse)
library(data.table)
library(matrixStats)
source("./Functions/generatePointCount.R")
```


```{r}
p_1m = 0.5  
maxMinute = 10
colsUse = paste0("Minute",1:maxMinute)
n_locations = 50
lambda = 15 # get this number from some data?
alpha=0.3
```


## Generate Population Data
```{r}
set.seed(1)
locationID = 1:n_locations
#N_i = rpois(n_locations,lambda)
N_i = rnbinom(n_locations, mu = lambda, size= 1)
dfPop = as.data.frame(cbind(locationID, N_i))
```


```{r}
# generate counts
dfCount = generateDetects(dfPop$N_i, p_1m,maxMinute,10)

# show data in wide format
dfWide = detectsToCapHist(dfCount)
dfWide = left_join(dfWide, dfPop, by = "locationID")

# add summarised count data
dfPop = left_join(dfPop, formatData(dfCount, isSummarised = TRUE), by = "locationID")
dfPop$C_i = replace_na(dfPop$C_i,0) # fill NA

# show cumulative data
dfCumulative = formatData(dfCount, isCumulative = TRUE)
dfCumulative = left_join(dfCumulative, dfPop[,c("locationID","N_i")], by = "locationID")


# view
head(dfCount)
head(dfWide)
head(dfPop)
head(dfCumulative)
```


- with respect to dfMove, the 1 represents if movement happened before the current
minute (m) and after the previous minute (m-1)
- If the individual is represented by a number and _1 in dfError, 
it means there was no movement at all in the latent matrix at a period when the original 
individual was observed (so even if movement happened, we did not count the bird
and so cannot say it was double counted)

*We assume each movement = double counting error*
```{r}
dfMove = generateMovement(dfCount,alpha,1) # generate movement
dfSplit = splitDetects(dfMove) # generate splits
dfError = generateErrors(dfCount,alpha,1) # combined function

# view results
print(dfMove[dfMove$locationID==29,])
print(dfError[dfError$locationID==29,])
```


```{r}
# add summarised erronous data to dfPop
dfErrorSummarised = formatData(dfError, isSummarised = TRUE)%>%
  rename(C_i_error = C_i)
dfPop = left_join(dfPop, dfErrorSummarised, by = "locationID")
dfPop$C_i_error = replace_na(dfPop$C_i_error,0) # fill NA

# find erronous cumulative data
dfErrorCumulative = formatData(dfError, isCumulative = TRUE)
dfErrorCumulative = left_join(dfErrorCumulative, dfPop[,c("locationID","N_i")], by = "locationID")


# view
head(dfError)
head(dfPop)
head(dfErrorCumulative)
```

# Graphs

## Compare Overall Distribution

I will compare the distribution of errors to the population distribution and the 
distribution of correct counts. 


Distribution of C_i error is radically different from C_i -> almost looks closer to 
distribution of N_i. Would make sense as we're counting extra individuals and that 
means C_i_error avg will be closer to that of N_i avg.
```{r}
dfPop%>%
  pivot_longer(cols= c("N_i","C_i","C_i_error"),names_to = "Type", values_to= "Values")%>%
  ggplot()+
  geom_density(aes(x=Values, color= Type))+
  theme_minimal()+ 
  labs(title= "Distribution of Population, Point Counts, and Erroneous Point Counts")
```

Facet for easier comparison. Looks like erroneous point count data is somewhat
similar to the distribution of N_i, but the density at certain points is different. The 
next step is to generate distributions for various values of alpha - this is a good 
way to check that error mechanism is working since alpha = 0 should give me the same
distribution as C_i.
```{r}
dfPop%>%
  pivot_longer(cols= c("N_i","C_i","C_i_error"),names_to = "Type", values_to= "Values")%>%
  ggplot()+
  geom_density(aes(x=Values, fill= Type))+
  facet_wrap(~Type)+
  theme_minimal()+ 
  labs(title= "Distribution of Population, Point Counts, and Erroneous Point Counts")
```


## Graph by Time Interval
Erroneous data has less peaks than regular point count data --> makes sense because
detection error means that counts that should be attributed to one individual are now
given to a new individual instead. So instead of an individual being counted 4 times 
they might be counted 1 or 2 times instead.
```{r}
dfErrorCumulative%>%
  pivot_longer(cols = c(colsUse), names_to = "Time", values_to = "Count")%>%
ggplot()+ 
  geom_density(aes(x=Count, fill = Time, color = Time),alpha=0.6)+ 
  facet_wrap(~Time,scales= "free")+ 
  theme_minimal()+
  labs(title= "Cumulative Point Counts by Time Interval for Erroneous Data")
```

```{r}
dfCumulative%>%
  pivot_longer(cols = c(colsUse), names_to = "Time", values_to = "Count")%>%
ggplot()+ 
  geom_density(aes(x=Count, fill = Time, color = Time),alpha=0.6)+ 
  facet_wrap(~Time,scales= "free")+ 
  theme_minimal()+
  labs(title= "Cumulative Point Counts by Time Interval for Regular Data")
```

A list of alpha values are generated and the result of C_i error are shown for each 
value of alpha.
```{r}
lstAlpha = c(0,0.3,0.5,0.7,0.9)
dfAlpha = copy(dfPop[,c("locationID","N_i","C_i")])

# loop to create new C_i_errors based on alpha
for(alpha in lstAlpha){
  dfErrorTemp = generateErrors(dfCount,alpha,1)
  
  # add summarised erronous data to dfPop
  dfErrorTempSummarised = formatData(dfErrorTemp, isSummarised = TRUE)
  newColName = paste0("C_i_error_alpha_",alpha)
  colnames(dfErrorTempSummarised) =c("locationID",newColName)
  
  dfAlpha = left_join(dfAlpha, dfErrorTempSummarised, by = "locationID")
  dfAlpha[,newColName] = replace_na(dfAlpha[,newColName],0) # fill NA
}
```


```{r}
head(dfAlpha)
```

Need to zoom in to see image properly, but graphs look right to me. Ci and C_i_error_alpha_0
have the same distribution (as it should be). We can see that as the probability of errors
increases, the distribution has a flatter peak and looks a bit more similar 
```{r}
dfAlpha%>%
  pivot_longer(cols= everything() ,names_to = "Type", values_to= "Values")%>%
  filter(Type !="locationID")%>%
  ggplot()+
  geom_density(aes(x=Values, fill= Type))+
  facet_wrap(~Type)+
  theme_minimal()+ 
  labs(title= "Distribution of Population, Point Counts, and Erroneous Point Counts")
```

Looking at it unfaceted is interesting because you can see how the peak becomes smaller
and the median seems to shift as well. Maybe a boxplot to illustrate would help.
```{r}
dfAlpha%>%
  pivot_longer(cols= everything() ,names_to = "Type", values_to= "Values")%>%
  filter(!(Type %in% c("N_i","locationID")))%>%
  ggplot()+
  geom_density(aes(x=Values, color= Type))+
  theme_minimal()+ 
  labs(title= "Distribution of Population, Point Counts, and Erroneous Point Counts")
```


I think boxplot highlights this well - we see the shifting median and increasing 
outliers as alpha rises in value.
```{r}
dfAlpha%>%
  pivot_longer(cols= everything() ,names_to = "Type", values_to= "Values")%>%
  filter(!(Type %in% c("C_i_error_alpha_0","locationID")))%>%
  ggplot()+
  geom_boxplot(aes(x=Type,y=Values, color= Type))+
  theme_minimal()+ 
  labs(title= "Distribution of Population, Point Counts, and Erroneous Point Counts")+
  theme(axis.text.x = element_blank())
```



## Probability of Detection 


```{r}
CountByTimeInterval=dfCumulative %>%
  pivot_longer(cols = c(colsUse), names_to = "TimeInterval", values_to = "PointCount")%>%
  group_by(locationID, individual, TimeInterval)%>%
  summarise(TotalCount = sum(PointCount))

CountByTimeInterval$TotalCount= ifelse(CountByTimeInterval$TotalCount>=1,1,0)
CountByTimeInterval=CountByTimeInterval %>%
  group_by(locationID, TimeInterval)%>%
  summarise(FinalCount = sum(TotalCount))
# add total location population
CountByTimeInterval = left_join(CountByTimeInterval, dfPop, by = "locationID")
tail(CountByTimeInterval)


CountByTimeIntervalError=dfErrorCumulative %>%
  pivot_longer(cols = c(colsUse), names_to = "TimeInterval", values_to = "PointCount")%>%
  group_by(locationID, individual, TimeInterval)%>%
  summarise(TotalCount = sum(PointCount))

CountByTimeIntervalError$TotalCount= ifelse(CountByTimeIntervalError$TotalCount>=1,1,0)
CountByTimeIntervalError=CountByTimeIntervalError %>%
  group_by(locationID, TimeInterval)%>%
  summarise(FinalCount = sum(TotalCount))
# add total location population
CountByTimeIntervalError = left_join(CountByTimeIntervalError, dfPop, by = "locationID")
tail(CountByTimeIntervalError)

```

Find the implied p_hat of the data. 
```{r}
CountByTimeInterval = CountByTimeInterval %>%
  mutate(p_hat = FinalCount/N_i)
CountByTimeInterval$TimeInterval = as.numeric(gsub('Minute', '', CountByTimeInterval$TimeInterval))
head(CountByTimeInterval)


CountByTimeIntervalError = CountByTimeIntervalError %>%
  mutate(p_hat = FinalCount/N_i)
CountByTimeIntervalError$TimeInterval = as.numeric(gsub('Minute', '', CountByTimeIntervalError$TimeInterval))
head(CountByTimeIntervalError)
```


```{r}
# find theoretical probability of being detected at least once
summaryPhat = CountByTimeInterval %>%
  group_by(TimeInterval)%>%
  summarise(p_data = mean(p_hat))
summaryPhat$TimeInterval = as.numeric(summaryPhat$TimeInterval)
summaryPhat$p_theoretical = PredictProbability(summaryPhat$TimeInterval, p_1m)


summaryPhatError = CountByTimeIntervalError %>%
  group_by(TimeInterval)%>%
  summarise(p_data = mean(p_hat))
summaryPhatError$TimeInterval = as.numeric(summaryPhatError$TimeInterval)
summaryPhat$p_error = summaryPhatError$p_data

head(summaryPhat)
```



```{r}
summaryPhat %>%
  pivot_longer(cols=c("p_data","p_error","p_theoretical"),names_to = "Type",values_to = "p" )%>%
ggplot()+
  geom_point(aes(x=TimeInterval, y = p,color=Type))+ 
  geom_line(aes(x=TimeInterval, y = p,color= Type))+
  theme_minimal()+ 
  labs(title = "Comparison of Estimated Detection Probability and Theoretical Probability",
       subtitle = "Theoretical probability is the probability of being detected at least once.")
```

