---
title: "Simulate Regular Data"
author: "Tito"
date: "2022-10-16"
output:
  html_document:
    df_print: paged
    toc: true
editor_options:
  markdown:
    wrap: 100
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Packages

```{r}
library(tidyverse)
```

# Q

-   Species to focus on?

# Simulate Base Case

To simulate the data, I will choose around 50 locations, and simulate the total population at each
location. Let $N_i$ represent the population at each location $i$.

I'm not sure whether I want the population to follow a poisson or negative binomial distribution - I
can try both!

## Poisson

```{r}
set.seed(1)
n_locations = 50
lambda = 15 # get this number from some data?
locationID = 1:n_locations
N_i = rpois(n_locations,lambda)

dfPoisson = as.data.frame(cbind(locationID, N_i))
head(dfPoisson)
```

I can think of the counts at each location ($C_i$) as a binomial random variable. However to do
this, I need not only the size ($N_i$) but I need the probability.

Modelling this probability is a little tricky so I want to do some further analysis first.

### Detection Probability with Time as a Covariate

From the paper *A Conceptual Guide to Detection Probability for Point Counts and Other Count-based
Survey Methods* I know that I can model detection probability with time as a covariate. We expect
detection probability to change as the duration of the count increases. To model this we need the
following:

-   m = duration of the count
-   $p_{1m}$ = detection probability for a 1 min count
    -   **Note**: The smallest time interval I will have in my simulation is 1 mins

Create a function to model detection probability by time m

```{r}
PredictProbability = function(m, p1m){
  # the way I think about this function is (for the time example): 
  # p1m is the probability of detecting an individual within a 1 min interval 
  # 1-p1m is the probability of not detecting an individual within a 1 min interval
  # (1-p1m)^m is the probability of not detecting an individual at all within m minutes
  # 1-((1-p1m)^m) is the probability of detecting an individual at least once within 
  # m minutes
  p_m = 1-((1-p1m)**m)
  return(p_m)
}
```

Set my constants and plot. The graph below makes sense to me as higher detection probabilities in 1
min intervals means that probability of counting all individuals in the area increases.

```{r}
# let m vary from 1 min to 90 mins
maxMinute = 90
m = 1:maxMinute
# let p_1m vary from a low detection probability to a higher one with 1 min intervals
p_1m = c(0.05,0.1,0.2)
p_1m_col = rep(p_1m, maxMinute)

# make the dataframe of probabilities
dfProbabilityTime = tibble(m=m)
dfProbabilityTime = dfProbabilityTime %>%
  slice(rep(1:n(), each = length(p_1m)))
dfProbabilityTime = dfProbabilityTime%>%
  mutate(p_1m = p_1m_col, 
         p_m = PredictProbability(m,p_1m))

# show results
head(dfProbabilityTime)
ggplot(dfProbabilityTime)+ 
  geom_line(aes(x=m, y = p_m, color = as.factor(p_1m)))+ 
  theme_minimal()+ 
  labs(title = "Detection Probability at Duration m", 
       x = "M minutes", 
       y = "Detection Probability by Time m", 
       color = "Detection Probability for 1 Minute Count")
  
```

If I know that $p_m$ is then I can find C. Because $p_m = \frac{C}{N}$. So rearranging gives me
$C = Np_m$ and I know N (from Poisson distribution).

```{r}
N1=14
dfProbabilityTime = dfProbabilityTime%>%
  mutate(C = N1*p_m)
ggplot(dfProbabilityTime)+ 
  geom_line(aes(x=m, y = C, color = as.factor(p_1m)))+ 
  theme_minimal()+ 
  labs(title = "Cumulative Count at Duration m", 
       x = "M minutes", 
       y = "Cumulative Count by Time m", 
       color = "Detection Probability for 1 Minute Count")
```

### Detection Probability with Cue's as a Covariate

I can also model detection probability with number of cues as a covariate. I would need the
following:

-   $p_{1d}$ = probability of detecting an average cue
-   s = number of cues
-   $p_{d|s}$ = detection probability given auditory cues

$p_{1d}$ is supposed to represent the four factors: amplitude of the birds, attentiveness of the
observer, auditory acuity of the observer, and masking of bird sounds by other sounds. **Would it be
fair to assume that** $p_{1d}$ increases as m increases? I'm trying to see if there's a way to link
time + presence of cues to figure out what the detection probability should be. Might not make sense
though

For now I'll just say that s = Bm where m is the number of minutes, and B is the rate at which s
increases as m increases.

$p_{d|s} = (1-(1-p_{1d})^s)$

One thing to note is that there is a tradeoff between s and $p_{1d}$. If s is high then yes we have
a chorus of individuals singing, but then it becomes harder to identify a single individual amongst
the noise. So higher s means lower $p_{1d}$.

Also I would argue that $p_{1d}$ is what should vary across site location (not across time). I can
maybe make $p_s$ vary across time since it is the probability of making a cue.

```{r}
# let s vary based on time (and a known cues/min rate)
maxMinute = 15
m = 0:maxMinute
cuesPerMin = 3 
s = m*cuesPerMin # should this be a more complicated relationship?
# let p_1d vary from a low detection  to a higher one
p_1d = c(0.1,0.2,0.3)
p_1d_col = rep(p_1d, length(s))

# make df
dfProbabilityCue = tibble(s=s)
dfProbabilityCue = dfProbabilityCue %>%
  slice(rep(1:n(), each = length(p_1d)))

dfProbabilityCue = dfProbabilityCue%>%
  mutate(p_1d = p_1d_col, 
         p_ds = PredictProbability(s,p_1d))

# show results
head(dfProbabilityCue)
ggplot(dfProbabilityCue)+ 
  geom_line(aes(x=s, y = p_ds, color = as.factor(p_1d)))+ 
  theme_minimal()+ 
  labs(title = "Detection Probability After s Cues", 
       x = "Cumulative Number of Cues (s)", 
       y = "Detection Probability Given Availability", 
       color = "Probability of Detecting an Average Cue")
```

To find overall detection probability, I need to incorporate $p_s$ which is the 
probability that the bird makes a cue. Maybe this is where I can incorporate 
using the time equation we saw earlier. So assume I have a base $p_s$ = probability 
of making a cue in one minute.

In this case, we can calculate the probability of making at least one cue after 
m minutes. 

Overall detection probability is significantly impacted by the probability of making
a cue. I don't think there is a need to increase $p_s$ by time because the linear
relationship between s and m means that as m increases, s already increases. 
```{r}
# case 1: p_1d is constant, and p_s varies from high to low 

maxMinute = 15
m = 0:maxMinute
cuesPerMin = 3 
s = m*cuesPerMin # should this be a more complicated relationship?
p_1d = c(0.1)
p_s = c(0.1,0.3,0.5,0.8)
p_1d_col = rep(p_1d, length(s))
p_s_col = rep(p_s, length(s))

# make df
dfProbabilityCue = tibble(s=s)
dfProbabilityCue$p_1d = p_1d
dfProbabilityCue = dfProbabilityCue %>%
  slice(rep(1:n(), each = length(p_s)))

dfProbabilityCue = dfProbabilityCue%>%
  mutate(p_s = p_s_col, 
         p = PredictProbability(s,p_1d)*p_s)

# show results
head(dfProbabilityCue)
ggplot(dfProbabilityCue)+ 
  geom_line(aes(x=s, y = p, color = as.factor(p_s)))+ 
  theme_minimal()+ 
  labs(title = "Overall Detection Probability After s Cues", 
       x = "Cumulative Number of Cues (s)", 
       y = "Detection Probability", 
       color = "Probability of Making a Cue")
```


Graph below shows why I'm probably wrong about using time to model $p_s$. 
It doesn't make sense that I have these patterned dips. Live & learn.
```{r}
# let s vary based on time (and a known cues/min rate)
maxMinute = 15
m = 0:maxMinute
cuesPerMin = 3 
s = m*cuesPerMin # should this be a more complicated relationship?
# let p_1d vary from a low detection  to a higher one
p_1d = c(0.1,0.2,0.3)
p_s = c(0.2)
p_1d_col = rep(p_1d, length(s))
#p_s_col = rep(p_s, length(s))

# make df
dfProbabilityCue = tibble(s=s)
dfProbabilityCue = dfProbabilityCue %>%
  slice(rep(1:n(), each = length(p_1d)))

dfProbabilityCue = dfProbabilityCue%>%
  mutate(p_1d = p_1d_col,
         p = PredictProbability(s,p_1d)*PredictProbability(m,p_s))

# show results
head(dfProbabilityCue)
ggplot(dfProbabilityCue)+ 
  geom_line(aes(x=s, y = p, color = as.factor(p_1d)))+ 
  theme_minimal()+ 
  labs(title = "Detection Probability After s Cues", 
       x = "Cumulative Number of Cues (s)", 
       y = "Detection Probability", 
       color = "Probability of Detecting an Average Cue")
```

### Probability Conclusion

As such, I conclude that we can model detection probability as a function of cues 
(s) and availability. 


```{r}
# ask: ideally p_1d and p_s should vary across the different locationID's 
# based on if they are easy, medium, or hard areas to survey
# for now my assumptions are that all locations are a medium difficulty area 
# to survey
p_1d = 0.2
p_s = 0.6
maxMinute = 5 # picked a 5 min duration for visualization purposes
m = maxMinute #1:maxMinute
cuesPerMin = 3 
s = rep(m*cuesPerMin, nrow(dfPoisson))
```

```{r}
# produce the detection probabilities after duration m 
dfPoissonProb = dfPoisson%>%
  slice(rep(1:n(), each = length(m)))%>%
  mutate(`Duration of Count`=rep(m, nrow(dfPoisson)),
         Cues = s,
         p = PredictProbability(Cues,p_1d)*p_s)

```


```{r}
set.seed(1)
dfPoissonProb$C = rbinom(nrow(dfPoissonProb), dfPoissonProb$N_i, dfPoissonProb$p)
dfPoissonProb$m = dfPoissonProb$`Duration of Count`
dfPoissonProb$`Duration of Count` = paste("Duration of",dfPoissonProb$`Duration of Count`,"mins")
head(dfPoissonProb)
```


## Negative Binomial

Another case where I model population data as following a negative binomial 
distribution. Similar numbers to the poisson case!
```{r}
set.seed(1)
n_locations = 50
size = 10 # chosen so it has the same mean as the poisson 
prob = 0.4 # get this number from some data?
locationID = 1:n_locations
N_i = rnbinom(n_locations,size,prob)
dfNB = as.data.frame(cbind(locationID, N_i))
head(dfNB)
```
```{r}
# ask: ideally p_1d and p_s should vary across the different locationID's 
# based on if they are easy, medium, or hard areas to survey
# for now my assumptions are that all locations are a medium difficulty area 
# to survey
p_1d = 0.2
p_s = 0.6
maxMinute = 5 # picked a small duration for visualization purposes
m = maxMinute #0:maxMinute
cuesPerMin = 3 
s = rep(m*cuesPerMin, nrow(dfNB))
```

```{r}
# produce the detection probabilities after duration m 
dfNBProb = dfNB%>%
  slice(rep(1:n(), each = length(m)))%>%
  mutate(`Duration of Count`=rep(m, nrow(dfNB)),
         Cues = s,
         p = PredictProbability(Cues,p_1d)*p_s)

```


```{r}
set.seed(1)
dfNBProb$C = rbinom(nrow(dfNBProb), dfNBProb$N_i, dfNBProb$p)
dfNBProb$m = dfNBProb$`Duration of Count`
dfNBProb$`Duration of Count` = paste("Duration of",dfNBProb$`Duration of Count`,"mins")
head(dfNBProb)
```

## Visualizing Data

As duration increases, overall detection probability increases. As overall
detection probability increases, we see the average point count value in the 
distribution become higher, which is what we should expect. 
```{r}
# look at distributions based on duration 
dfPoissonProb%>%
  #filter(`Duration of Count`!="Duration of 0 mins")%>%
ggplot()+ 
  geom_histogram(aes(x=C),bins=15)+ 
  #facet_wrap(~as.factor(`Duration of Count`))+ 
  theme_minimal()+ 
  labs(title = "Distribution of Point Counts as Duration of Survey Varies", 
       subtitle = "Distribution formed using location id values. 
Population totals simulated with poisson distribution.",
       x = "Point Counts",
       y="")
```

Look at the NB case.
```{r}
# look at distributions based on duration 
dfNBProb%>%
  #filter(`Duration of Count`!="Duration of 0 mins")%>%
ggplot()+ 
  geom_histogram(aes(x=C),bins=15)+ 
  #facet_wrap(~as.factor(`Duration of Count`))+ 
  theme_minimal()+ 
  labs(title = "Distribution of Point Counts as Duration of Survey Varies", 
       subtitle = "Distribution formed using location id values. 
Population totals simulated with NB distribution",
       x = "Point Counts",
       y="")
```

## Intervals

I know what the total point counts at each location are. The trick is now to 
figure out how I want to simulate the counts during each interval. 

I am going to need to set the total duration of the count at each location, and 
also the width of each time interval. 

Once I have that, I need a way to figure out how many point counts should be 
at each interval. Once again I'll use binomial, but this time I'll add a limit 
such that the last number generated will ensure that the sum of all the numbers 
generated so far are equal to the total point counts at 
location $i$. I will make a function to generate these counts.


The generate interval counts function will work if we only use the probability of 
detection calculated at the end of the full duration of the count. 
```{r}
generateIntervalCounts = function(duration, intervalWidth,prob,locationCount){
  
  # use rbinom to generate random numbers, and each time a number is generated, 
  # check to make sure that it has not surpassed the sum of the location
  
  # counter for summed values
  total = 0
  # counter for number of iterations
  k = 0
  
  # generate the number of intervals
  nGeneratedNumbers = duration/intervalWidth
  
  # vector to store point counts
  intervalCounts = c()

  while(total<locationCount){
    
    rvBinom  = rbinom(1,locationCount,prob)
    total = total + rvBinom
    k = k+1
    
    # set up trace
    trace = tibble(k = k, 
                 rvBinom = rvBinom, 
                 total = total, 
                 intervalCounts = intervalCounts)
  
    
    # check if total is less than location count
    if(total < locationCount & k < (nGeneratedNumbers-1)){
      # case 1
      intervalCounts[k] = rvBinom
    } else if(total < locationCount & k == (nGeneratedNumbers-1)){
      # case 2
      # if by the second last iteration, we still have not reached locationCount
      # value, set final number equal what is needed
      intervalCounts[k] = rvBinom
      finalValue = locationCount-total
      # update counter and sum
      k=k+1
      total = total + finalValue
      intervalCounts[k] = finalValue
      
      
    } else if(total >= locationCount & k <= (nGeneratedNumbers-1)){
      # case 3
      finalValue = locationCount-(total-rvBinom) # figure out last value 
      intervalCounts[k] = finalValue
      while(k<nGeneratedNumbers){
        # fill in remaining intervals with 0
        fillValue = 0
        k=k+1
        intervalCounts[k] = fillValue
      }
    } else if(duration==1){
      if(rvBinom!=locationCount){
        intervalCounts = locationCount
      } else{
        intervalCounts = rvBinom
      }
    }
  }
  #return(list(interval = intervalCounts,trace = trace)) 
  return(intervalCounts)
  
  
}
```


```{r}
# test - it works!

## test case 2
#set.seed(8)
#generateIntervalCounts(5,1,0.1,3)

## test case 3
#set.seed(8)
#generateIntervalCounts(5,1,0.1,3)

```
### Poisson 
```{r}
maxMinute = 5
intervalWidth = 1
if(maxMinute%%intervalWidth!=0){
  print("Error! Duration of count must be divisible by interval width.")
}
```

```{r}
p_1d = 0.2
p_s = 0.6
m = c(1:maxMinute)*intervalWidth
cuesPerMin = 3 
s = rep(m*cuesPerMin, nrow(dfPoisson))
```

```{r}
# produce the detection probabilities after duration m 
dfPoissonProb = dfPoisson%>%
  slice(rep(1:n(), each = length(m)))%>%
  mutate(`Duration of Count`=rep(m, nrow(dfPoisson)),
         Cues = s,
         p = PredictProbability(Cues,p_1d)*p_s)

```

In a scenario like this, I would use the last vector that appears. I don't think 
the numbers make sense though because detection probability changes as time changes. 
So the detection probability being used should differ at each time interval.
```{r}
set.seed(1)
mapply(generateIntervalCounts, dfPoissonProb$`Duration of Count`[1:5], intervalWidth,
                       dfPoissonProb$p[1:5], dfPoissonProb$N_i[1:5])
```

In the second iteration of the function, I will allow the prob argument to be a 
vector! Same logic of original function still applies however.
```{r}
generateIntervalCountsProb = function(duration, intervalWidth,prob,locationCount){
  
  # use rbinom to generate random numbers, and each time a number is generated, 
  # check to make sure that it has not surpassed the sum of the location
  
  # counter for summed values
  total = 0
  # counter for number of iterations
  k = 0
  
  # generate the number of intervals
  nGeneratedNumbers = duration/intervalWidth
  
  # vector to store point counts
  intervalCounts = c()

  while(total<locationCount){
    
    k=k+1
    pUse = prob[k]
    rvBinom  = rbinom(1,locationCount,pUse)
    total = total + rvBinom
    
    # set up trace
    trace = tibble(k = k, 
                 rvBinom = rvBinom, 
                 total = total, 
                 intervalCounts = intervalCounts)
  
    
    # check if total is less than location count
    if(total < locationCount & k < (nGeneratedNumbers-1)){
      # case 1
      intervalCounts[k] = rvBinom
    } else if(total < locationCount & k == (nGeneratedNumbers-1)){
      # case 2
      # if by the second last iteration, we still have not reached locationCount
      # value, set final number equal what is needed
      intervalCounts[k] = rvBinom
      finalValue = locationCount-total
      # update counter and sum
      k=k+1
      total = total + finalValue
      intervalCounts[k] = finalValue
      
      
    } else if(total >= locationCount & k <= (nGeneratedNumbers-1)){
      # case 3
      finalValue = locationCount-(total-rvBinom) # figure out last value 
      intervalCounts[k] = finalValue
      while(k<nGeneratedNumbers){
        # fill in remaining intervals with 0
        fillValue = 0
        k=k+1
        intervalCounts[k] = fillValue
      }
    } else if(duration==1){
      if(rvBinom!=locationCount){
        intervalCounts = locationCount
      } else{
        intervalCounts = rvBinom
      }
    }
  }
  #return(list(interval = intervalCounts,trace = trace)) 
  return(intervalCounts)
  
  
}
```

Store the probabilities as a list.
```{r}
# store vector of probabilities to a new column 
dfPoissonProb = dfPoissonProb %>%
  group_by(locationID)%>%
  mutate(lstProb = I(list(p)))

dfPoissonInput = dfPoissonProb%>%
  group_by(locationID)%>%
  summarise(locationID = locationID, 
            N_i = N_i,
            `Duration of Count`=max(`Duration of Count`),
            lstProb = lstProb)
dfPoissonInput = dfPoissonInput[!duplicated(dfPoissonInput),]
dfPoissonInput
```
I  use mapply so that my function is vectorized and it runs faster.
```{r}
set.seed(1)
poissonResult = mapply(generateIntervalCountsProb, dfPoissonInput$`Duration of Count`, intervalWidth,
                       dfPoissonInput[,"lstProb"]$lstProb, dfPoissonInput$N_i)
poissonResultFinal = t(poissonResult)
```

```{r}
colnames(poissonResultFinal) = 1:ncol(poissonResultFinal)
dfPoissonInputFinal = cbind(dfPoissonInput, poissonResultFinal)
dfPoissonFinal = merge(dfPoissonProb, dfPoissonInputFinal[,c("locationID",
                                                             colnames(poissonResultFinal))],
                       by=c("locationID"),
                       all.x=TRUE, all.y=TRUE)
head(dfPoissonFinal)
```

### Plot Distribution of interval counts

Interesting that at time interval 4 and 5, majority of the counts are at 0. 
This probably has to do with the fact that if the maximum number of counts 
were reached (based on $Ni$) then the rest of the counts went to 0. 

Maybe this suggests I try a different method (ie randomly split the last 
number between the last two intervals)? Or try varying detection probability 
and see how it affects this distribution? 
**ASK**
```{r}
dfPoissonFinalLong = dfPoissonFinal %>%
  pivot_longer(cols = colnames(poissonResultFinal), names_to = "Interval", 
               values_to = "IntervalValue")

ggplot(dfPoissonFinalLong)+ 
  geom_histogram(aes(x=IntervalValue))+ 
  facet_wrap(~Interval)+ 
  theme_minimal()+ 
  labs(title = "Distribution of Interval Count Values", 
       subtitle = "Across Various Location ID's", 
       x = "Point Counts", 
       y = "")
```



