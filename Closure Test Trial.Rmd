---
title: "Closure Test Trial"
author: "Tito"
date: "2023-06-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Questions

- Review code for otisClosure test
  - Is it right to group statistics by combination number?
- The Otis test only applies to those who have been counted > 1 time because it depends on waiting time between first and last capture
  - Is this not a little ineffective though because we ignore all the data from individuals captured only once? 
- Include qualitative indicators?
- Otis closure test seems to work --> however, how do we know its not rejecting because the underlying model is not correct? When alpha = 0 seems to always not reject... but just want to make sure this is ok? underlying model for H0 is that the data is  
- test for temporary emigration? Is it effective with movement?
- likelihood ratio test of constrained models? - couldn't find any papers doing this sort of test for the CJS model (comparing cjs to M_T with GOF). It was all a sequence of chi square tests.
- general GOF in Rmark is done using the quasi likelihood?
  - Should I try the tests presented in the second paper? 


# Load Capture History
```{r}
library(tidyverse)
library(gtools)
library(data.table)
library(matrixStats)
library(RMark)
library(stringi)
library(rlang)
library(gt)
library(gtExtras)
library(webshot2)
library(knitr)
library(readxl)
```


Load functions from the other R scripts.
```{r}
source("./Functions/generatePointCount.R")
source("./Functions/runSimulationFunctions.R")
```


Load data - don't touch this. Temporarily set the working directory to load the data - will automatically return to current working directory after this chunk runs. The data in this folder is for 100 unique scenarios, with 1000 replicates. The scenarios are shown later on in the code and is called the "params" dataframe. The scenarios used to generate this data can also be found in the "./Functions/runSimulation.R" file. 
```{r}
setwd("./DigitalAllianceOutput/Huggins")
dataFinalTrial = list.files(pattern = ".rds") # load rds files
dataFinalTrial = mixedsort(dataFinalTrial)%>% # use mixedsort so it appears in ascending order 
  map(readRDS)

# initialize empty dataframes to store the data
dfSummarisedFinalH= data.frame()
dfEstimatesFinalH = data.frame()
dfHistoryFinalH = data.frame()
#counter = 1

# loop through the rds files and retrive data 
for (item in dataFinalTrial){
  tempSummarisedResults = item[[1]] # retrieve the summary stats; this df is useless because extreme values have not been removed; ignore
  tempCalculations = item[[2]] # load estimates for each run (should be 200,000 rows - 100,000 estimates of N, and 100,000 estimates of p)
  tempHistory= item[[3]] # load the detection histories used to generate the histories 

  
  # add data points to final dataframe
  dfSummarisedFinalH = rbind(dfSummarisedFinalH,tempSummarisedResults)
  dfEstimatesFinalH = rbind(dfEstimatesFinalH, tempCalculations)
  dfHistoryFinalH = rbind(dfHistoryFinalH, tempHistory)
  #counter = counter + 1
}
```


Some code to add identifiers to the data. Combination number will identify the variables used to create the scenario - or just "scenario". Simulation number will identify the run number - or datapoints actually generated. There are 1000 simulation numbers for every combination number! The variable `params` is created in one of the code chunks below, and shows the variables associated with each combination number.

```{r}
# id is combinationNumber_simulationNumber
dfEstimatesFinalH$rowid = 1:nrow(dfEstimatesFinalH)
dfEstimatesFinalH$id = paste0(dfEstimatesFinalH$combinationNumber,"_",dfEstimatesFinalH$simulationNumber)
dfHistoryFinalH$id = paste0(dfHistoryFinalH$combinationNumber,"_",dfHistoryFinalH$simulationNumber)
dfEstimatesFinalHUnClean = copy(dfEstimatesFinalH)
```

Find outliers
```{r}
dfOutliers = dfEstimatesFinalHUnClean %>%
  filter(variable == "N") %>%
  filter(estimate < 0 | is.na(estimate))
nOutliers = nrow(dfOutliers)
nEstimates = nrow(dfEstimatesFinalHUnClean[which(dfEstimatesFinalHUnClean$variable=="N"),])
print(nOutliers)
print(nEstimates)
print(nOutliers/nEstimates)
dfOutliers
```


The params dataset represents the scenarios/environment used to simulate data for each combination number. Add it to the dataset of final estimates to help calculate summary statistics.
```{r}
# this code is what was used in the ./Functions/runSimulation.R file to generate scenarios to input into the HPC 
nRuns = 1000
lstNi = c(10,20)
lstP = c(0.1,0.2,0.3,0.4,0.5)
#lstAlpha = c(0)
lstAlpha =c(0,0.05,0.1,0.2,0.3)
lstMaxMin = c(5,10)
lstFormula = c("~1")
lstMixtures = c(1)
lstSeed = c("NULL")
strModel = "Huggins"

params = expand.grid(nRuns,lstNi,lstP, lstAlpha, lstMaxMin,lstFormula,lstMixtures,lstSeed,strModel)
colnames(params) = c("nRuns","N","p_1m","alpha","maxMinute","strFormula","lstMixtures","lstSeed","strModel")
params$Scenario = 1:nrow(params)

# merge the params dataframe with the dataframe of estimates to help with calculating summary statistics 
dfEstimatesFinalH = left_join(dfEstimatesFinalH,params,by=c("combinationNumber"="Scenario")) 

# some code to help with calculating summary statistics -> full summary not done yet, but these are intermediate steps needed 
# to determine things like coverage probability, MSE, etc
dfEstimatesFinalH$bCoverage = ifelse(dfEstimatesFinalH$trueValue >= dfEstimatesFinalH$lcl & dfEstimatesFinalH$trueValue <= dfEstimatesFinalH$ucl,1,0)
dfEstimatesFinalH$squaredError = (dfEstimatesFinalH$estimate - dfEstimatesFinalH$trueValues)^2
dfEstimatesFinalH$width = dfEstimatesFinalH$ucl - dfEstimatesFinalH$lcl
  
```


Create a dataframe of summarized variables. This is before outliers have been removed.
```{r}
dfSummarisedFinalH = dfEstimatesFinalH %>%
  filter(variable == "N")%>% # only focus on N? or also use p??
  group_by(p_1m,alpha,N,maxMinute,strFormula,combinationNumber)%>%
  summarise(AvgNhat = mean(estimate,na.rm=TRUE),
            AvgNhatSE= mean(se,na.rm=TRUE),
            sdNhat = sd(estimate,na.rm=TRUE),
            mse = mean(squaredError,na.rm=TRUE),
            bias_SE_Nhat = AvgNhatSE - sdNhat,
            coverage = sum(bCoverage,na.rm=TRUE)/sum(!is.na(estimate)), # changed this from n() to sum(!is.na()) because I have some NA values for my estimates
            avgWidth = mean(width,na.rm=TRUE),
            avgAIC = mean(AIC,na.rm=TRUE))
```

## Find Single Detections


```{r}
colnamesUse= paste0("Minute",1:10) # should say 1:10
dfHistoryFinalH$counts = rowSums(dfHistoryFinalH[,colnamesUse],na.rm=TRUE)
dfHistoryFinalH$bCounts = ifelse(dfHistoryFinalH$counts > 1,">1","=1")

# find out for each id (scenario_simulationNumber) how many individuals were counted only once and how many were recaptured
nMultDetect = dfHistoryFinalH %>%
  filter(counts>1)
nMultDetect = length(unique(nMultDetect$id))
nMultDetect
nMultDetect/nEstimates
```

Doing this just to make sure I'm filtering data right. Should add to 100.
```{r}
nMultDetect = dfHistoryFinalH %>%
  filter(counts>1)

nSingleDetection = dfHistoryFinalH %>%
  filter(counts==1 & !(id %in% nMultDetect$id))

nSingleDetection = length(unique(nSingleDetection$id))
nSingleDetection
nSingleDetection/nEstimates
```
Store for usage later on
```{r}
dfMultDetect = dfHistoryFinalH %>%
  filter(counts>1)

dfSingleDetection = dfHistoryFinalH %>%
  filter(counts==1 & !(id %in% dfMultDetect$id))
```


## Clean Data

Add some important data to the dfHistory dataframe for further analysis
```{r}
dfHistoryFinalH = left_join(dfHistoryFinalH,dfSummarisedFinalH[,1:6], by= "combinationNumber")
```

```{r}
# filter out cases that did not run
dfEstimatesFinalH = dfEstimatesFinalHUnClean %>%
  filter(variable == "N") %>%
  filter(estimate>=0) %>%
  filter(!(id %in% dfSingleDetection$id))

dfEstimatesProbability = dfEstimatesFinalHUnClean %>% # these are probability estimates; not really needed for this project
  filter(variable == "p g1 t1") %>%
  filter(estimate>=0) %>%
  filter(!(id %in% dfSingleDetection$id))
```


```{r}
#add p,alpha,N, etc to help with graphing
dfSummarisedFinalH = dfSummarisedFinalH[!(duplicated(dfSummarisedFinalH$combinationNumber)),]
dfEstimatesFinalH = left_join(dfEstimatesFinalH, dfSummarisedFinalH[,1:6], by= "combinationNumber")
dfEstimatesProbability  =  left_join(dfEstimatesProbability, dfSummarisedFinalH[,1:6], by= "combinationNumber")
```


# Otis Closure Test Cases

I think we should only focus on animals which have been detected > 1 times anyways - that's what the scope of the test is for. But then if that is the case... isn't the point of doing the test to detect movement a little invalid? We have a large number of individuals being miscounted, so the capture history matrix is sparse and we won't have as many individuals to apply this too. The test seems a little ineffective, especially for short time intervals. 


Test several ids's

```{r}
testdf = dfHistoryFinalH%>%
  filter(id %in% c("1_30","80_1"))%>%
  filter(counts>1)
```


```{r}
dfHistoryFinalH%>%
  filter(id %in% c("5_10"))
testCase = otisDetectClosure(dfHistoryFinalH, nID = c("5_10"))
testCase
```


Test a case where there are only single detections 
```{r}
dfHistoryFinalH%>%
  filter(id== "1_80")
otisDetectClosure(dfHistoryFinalH, nID = c("1_80"))
```


# Otis Test on All Data

Now that test cases have worked, I will apply the Otis closure test to all the simulated data. 

```{r}
dfOtisTest = otisDetectClosure(dfHistoryFinalH)
head(dfOtisTest)
```

test calculations
```{r}
dfOtisTest%>%
  filter(id == "5_10")
```


## Graphs

Let's graph some of these p values along with the p_1m and alpha to see if the test is effective


## Average wait time (Qi)

Graph of average wait time across the various scenarios


It is not surprising that avg wait time between first and last capture is longer when maxMinute changes. We see that across all values of p_1m, as the probability of miscounting increases, time between first and last capture is smaller. This would make sense given that if animals are moving and being miscounted, the true capture history is truncated, and so time between first and last capture should be smaller. 
```{r}
dfOtisTest%>%
  filter(N == 10 & maxMinute == 5)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = avgQi, color = as.factor(alpha)))+ 
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()

dfOtisTest%>%
  filter(N == 10 & maxMinute == 10)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = avgQi, color = as.factor(alpha)))+ 
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()
```



```{r}
dfOtisTest%>%
  filter(N == 20 & maxMinute == 5)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = avgQi, color = as.factor(alpha)))+ 
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()

dfOtisTest%>%
  filter(N == 20 & maxMinute == 10)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = avgQi, color = as.factor(alpha)))+ 
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()
```



Not that informative when maxMin = 5. But overall same pattern
```{r}
# if qi <0 it means there was only one detection
dfOtisTest%>%
  filter(N == 10 & maxMinute == 5)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = qi, color = as.factor(alpha)))+ 
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()

dfOtisTest%>%
  filter(N == 10 & maxMinute == 10)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = qi, color = as.factor(alpha)))+ 
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()
```


```{r}
dfOtisTest%>%
  filter(N==20 & maxMinute== 10)%>%
  ggplot()+
  geom_bar(aes(x=qi, fill = as.factor(alpha)), position = "dodge")+
  facet_wrap(~p_1m, scales="free")+
  theme_bw()
```


```{r}
dfOtisTest%>%
  filter(N == 20 & maxMinute == 5 )%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = qi, color = as.factor(alpha)))+ 
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()

dfOtisTest%>%
  filter(N == 20 & maxMinute == 10)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = qi, color = as.factor(alpha)))+ 
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()
```

Average detections per scenario (look @ how sparse detections are) - just another graph to show that I simulated properly - # detetections in each scenario is appx equal to capture probability
```{r}
# dfSparse = dfOtisTest[!duplicated(dfOtisTest[,c("id","percentDetections")]),]
# 
# dfSparse%>%
#   filter(N == 20 & maxMinute == 5)%>%
# ggplot()+
#   geom_boxplot(aes(x= as.factor(p_1m), y = percentDetections, color = as.factor(alpha)))+
#   scale_color_discrete(name="Alpha")+
#   theme_bw()
# 
# dfSparse%>%
#   filter(N == 20 & maxMinute == 10)%>%
# ggplot()+
#   geom_boxplot(aes(x= as.factor(p_1m), y = percentDetections, color = as.factor(alpha)))+
#   scale_color_discrete(name="Alpha")+
#   theme_bw()
```



## Conditional Test Statistic (C_k)

Now I will look at the distribution of the p values associated with the test statistic C_k across various scenarios. 


When N = 10 and only five capture occasions, Otis test does not reject closure on average. Note that low population and low capture occasion means that we use only 1-2 individuals to calculate the conditional test statistic - this does not seem to be a good idea.  
```{r}
dfOtisTest%>%
  filter(N == 10 & maxMinute == 5)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = p_k, color = as.factor(alpha)))+ 
  geom_hline(aes(yintercept=0.05), color = "grey30")+
  scale_color_discrete(name="Alpha")+
  theme_bw()

dfOtisTest%>%
  filter(N == 10 & maxMinute == 10)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = p_k, color = as.factor(alpha)))+ 
   geom_hline(aes(yintercept=0.05), color = "grey30")+
  scale_color_discrete(name="Alpha")+
  theme_bw()
```


Same phenomenon for when N= 20 -> 5 capture occasions is simply too small. Previous work has shown that when p is large, even a small alpha (0.05) will cause significantly biased estimates. However, the Otis test will not capture this and still fails to reject the closure assumption.  
```{r}
dfOtisTest%>%
  filter(N == 20 & maxMinute == 5)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = p_k, color = as.factor(alpha)))+ 
  geom_hline(aes(yintercept=0.05), color= "red")+
  scale_color_discrete(name="Alpha")+
  theme_bw()

dfOtisTest%>%
  filter(N == 20 & maxMinute == 10)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = p_k, color = as.factor(alpha)))+ 
  geom_hline(aes(yintercept=0.05), color= "red")+
  scale_color_discrete(name="Alpha")+
  theme_bw()
```



It's a little hard to see the p values when I include alpha = 0, so I will remove it for more granular analysis.
Looks fine except for the case of N = 20, alpha = 0.3. Seems like the median p value gives the conclusion that closure is not violated... concerning because it clearly should be?
```{r}
dfOtisTest%>%
  filter(N == 20 & maxMinute == 5 & alpha !=0)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = p_k, color = as.factor(alpha)))+ 
  geom_hline(aes(yintercept=0.05), color= "red")+
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()

dfOtisTest%>%
  filter(N == 20 & maxMinute == 10 & alpha !=0)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = p_k, color = as.factor(alpha)))+ 
   geom_hline(aes(yintercept=0.05), color= "red")+
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()
```

I think its weird that the case of p_1m = 0.1 isn't producing lower p values - esp when alpha = 0.3. Esp given that N = 20 and maxMin = 10. Check the data for some of these

```{r}
# alpha = 0.3 and p_1m = 0.1 and maxMin = 10 and N = 20 is scenario 92

dfOtisTest%>%
  filter(combinationNumber==92)%>%
  arrange(desc(fk),desc(p_k))%>%
  head(10)%>%
  select(id, fk, counts, p_k, Ck)
```

has a super long wait time - makes sense that it wasn't rejected...
```{r}
dfOtisTest%>%
  filter(id == "92_344")
```

```{r}
dfOtisTest%>%
  filter(id == "92_990")

dfOtisTest%>%
  filter(id == "92_396")
```

makes sense to see less rejections of closure when time interval is larger - even despite the big alpha because the low detection probability means not that many people are detected anyways. Also this is all based on only 3/4 individuals which is not great and feels like it is subject to a lot of variability. 


you can see that in cases where there are a lot of actual splits in the data, the p val is low (ie 92_897)

when maxmin is five we rarely ever reject... probs because the Eqi is incredibly low.

```{r}
dfOtisTest%>%
  filter(combinationNumber==42)%>%
  arrange(desc(fk),desc(p_k) )%>%
  head(30)%>%
  select(id, fk, counts, p_k, Ck)
```
makes sense why this is not rejected - also no splits happened here
```{r}
dfOtisTest%>%
  filter(id == "42_96")
```


## Overall Test Statistic (C)

The smaller the test statistic C is, the more evidence against closure. Looks like p=0.2 is high enough that we can reject closure consistently across alpha values.
```{r}
dfOtisTest%>%
  filter(N == 20 & maxMinute == 5)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = C, color = as.factor(alpha)))+ 
  geom_hline(yintercept = qnorm(0.05, lower.tail=TRUE),color = "grey20",size=0.8)+
  scale_color_discrete(name="Alpha")+
  theme_bw()

dfOtisTest%>%
  filter(N == 20 & maxMinute == 10)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = C, color = as.factor(alpha)))+ 
  geom_hline(yintercept = qnorm(0.05, lower.tail=TRUE),color = "grey20",size=0.8)+
  scale_color_discrete(name="Alpha")+
  theme_bw()
```


same phenomenon where we only reject when p_1m is around 0.5 - unlikely to be the case irl 
```{r}
dfOtisTest%>%
  filter(N == 20 & maxMinute == 5)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = pOverall, color = as.factor(alpha)))+ 
  geom_hline(yintercept = 0.05,color = "grey20",size=0.8)+
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()

dfOtisTest%>%
  filter(N == 20 & maxMinute == 10)%>%
ggplot()+ 
  geom_boxplot(aes(x= as.factor(p_1m), y = pOverall, color = as.factor(alpha)))+ 
  geom_hline(yintercept = 0.05,color = "grey20",size=0.8)+
  #facet_wrap(~maxMinute)+
  scale_color_discrete(name="Alpha")+
  theme_bw()
```







