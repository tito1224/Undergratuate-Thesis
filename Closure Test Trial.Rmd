---
title: "Closure Test Trial"
author: "Tito"
date: "2023-06-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Capture History
```{r}
library(tidyverse)
library(gtools)
library(data.table)
library(matrixStats)
library(RMark)
library(stringi)
library(rlang)
library(gt)
library(gtExtras)
library(webshot2)
library(knitr)
library(readxl)
```


Load functions from the other R scripts.
```{r}
source("./Functions/generatePointCount.R")
source("./Functions/runSimulationFunctions.R")
```


Load data - don't touch this. Temporarily set the working directory to load the data - will automatically return to current working directory after this chunk runs. The data in this folder is for 100 unique scenarios, with 1000 replicates. The scenarios are shown later on in the code and is called the "params" dataframe. The scenarios used to generate this data can also be found in the "./Functions/runSimulation.R" file. 
```{r}
setwd("./DigitalAllianceOutput/Huggins")
dataFinalTrial = list.files(pattern = ".rds") # load rds files
dataFinalTrial = mixedsort(dataFinalTrial)%>% # use mixedsort so it appears in ascending order 
  map(readRDS)

# initialize empty dataframes to store the data
dfSummarisedFinalH= data.frame()
dfEstimatesFinalH = data.frame()
dfHistoryFinalH = data.frame()
#counter = 1

# loop through the rds files and retrive data 
for (item in dataFinalTrial[c(1,80)]){
  tempSummarisedResults = item[[1]] # retrieve the summary stats; this df is useless because extreme values have not been removed; ignore
  tempCalculations = item[[2]] # load estimates for each run (should be 200,000 rows - 100,000 estimates of N, and 100,000 estimates of p)
  tempHistory= item[[3]] # load the detection histories used to generate the histories 

  
  # add data points to final dataframe
  dfSummarisedFinalH = rbind(dfSummarisedFinalH,tempSummarisedResults)
  dfEstimatesFinalH = rbind(dfEstimatesFinalH, tempCalculations)
  dfHistoryFinalH = rbind(dfHistoryFinalH, tempHistory)
  #counter = counter + 1
}
```


Some code to add identifiers to the data. Combination number will identify the variables used to create the scenario - or just "scenario". Simulation number will identify the run number - or datapoints actually generated. There are 1000 simulation numbers for every combination number! The variable `params` is created in one of the code chunks below, and shows the variables associated with each combination number.

```{r}
# id is combinationNumber_simulationNumber
dfEstimatesFinalH$rowid = 1:nrow(dfEstimatesFinalH)
dfEstimatesFinalH$id = paste0(dfEstimatesFinalH$combinationNumber,"_",dfEstimatesFinalH$simulationNumber)
dfHistoryFinalH$id = paste0(dfHistoryFinalH$combinationNumber,"_",dfHistoryFinalH$simulationNumber)
dfEstimatesFinalHUnClean = copy(dfEstimatesFinalH)
```

Find outliers
```{r}
dfOutliers = dfEstimatesFinalHUnClean %>%
  filter(variable == "N") %>%
  filter(estimate < 0 | is.na(estimate))
nOutliers = nrow(dfOutliers)
nEstimates = nrow(dfEstimatesFinalHUnClean[which(dfEstimatesFinalHUnClean$variable=="N"),])
print(nOutliers)
print(nEstimates)
print(nOutliers/nEstimates)
dfOutliers
```


The params dataset represents the scenarios/environment used to simulate data for each combination number. Add it to the dataset of final estimates to help calculate summary statistics.
```{r}
# this code is what was used in the ./Functions/runSimulation.R file to generate scenarios to input into the HPC 
nRuns = 1000
lstNi = c(10,20)
lstP = c(0.1,0.2,0.3,0.4,0.5)
#lstAlpha = c(0)
lstAlpha =c(0,0.05,0.1,0.2,0.3)
lstMaxMin = c(5,10)
lstFormula = c("~1")
lstMixtures = c(1)
lstSeed = c("NULL")
strModel = "Huggins"

params = expand.grid(nRuns,lstNi,lstP, lstAlpha, lstMaxMin,lstFormula,lstMixtures,lstSeed,strModel)
colnames(params) = c("nRuns","N","p_1m","alpha","maxMinute","strFormula","lstMixtures","lstSeed","strModel")
params$Scenario = 1:nrow(params)

# merge the params dataframe with the dataframe of estimates to help with calculating summary statistics 
dfEstimatesFinalH = left_join(dfEstimatesFinalH,params,by=c("combinationNumber"="Scenario")) 

# some code to help with calculating summary statistics -> full summary not done yet, but these are intermediate steps needed 
# to determine things like coverage probability, MSE, etc
dfEstimatesFinalH$bCoverage = ifelse(dfEstimatesFinalH$trueValue >= dfEstimatesFinalH$lcl & dfEstimatesFinalH$trueValue <= dfEstimatesFinalH$ucl,1,0)
dfEstimatesFinalH$squaredError = (dfEstimatesFinalH$estimate - dfEstimatesFinalH$trueValues)^2
dfEstimatesFinalH$width = dfEstimatesFinalH$ucl - dfEstimatesFinalH$lcl
  
```


Create a dataframe of summarized variables. This is before outliers have been removed.
```{r}
dfSummarisedFinalH = dfEstimatesFinalH %>%
  filter(variable == "N")%>% # only focus on N? or also use p??
  group_by(p_1m,alpha,N,maxMinute,strFormula,combinationNumber)%>%
  summarise(AvgNhat = mean(estimate,na.rm=TRUE),
            AvgNhatSE= mean(se,na.rm=TRUE),
            sdNhat = sd(estimate,na.rm=TRUE),
            mse = mean(squaredError,na.rm=TRUE),
            bias_SE_Nhat = AvgNhatSE - sdNhat,
            coverage = sum(bCoverage,na.rm=TRUE)/sum(!is.na(estimate)), # changed this from n() to sum(!is.na()) because I have some NA values for my estimates
            avgWidth = mean(width,na.rm=TRUE),
            avgAIC = mean(AIC,na.rm=TRUE))
```

## Find Single Detections


```{r}
colnamesUse= paste0("Minute",1:10) # should say 1:10
dfHistoryFinalH$counts = rowSums(dfHistoryFinalH[,colnamesUse],na.rm=TRUE)
dfHistoryFinalH$bCounts = ifelse(dfHistoryFinalH$counts > 1,">1","=1")

# find out for each id (scenario_simulationNumber) how many individuals were counted only once and how many were recaptured
nMultDetect = dfHistoryFinalH %>%
  filter(counts>1)
nMultDetect = length(unique(nMultDetect$id))
nMultDetect
nMultDetect/nEstimates
```

Doing this just to make sure I'm filtering data right. Should add to 100.
```{r}
nMultDetect = dfHistoryFinalH %>%
  filter(counts>1)

nSingleDetection = dfHistoryFinalH %>%
  filter(counts==1 & !(id %in% nMultDetect$id))

nSingleDetection = length(unique(nSingleDetection$id))
nSingleDetection
nSingleDetection/nEstimates
```
Store for usage later on
```{r}
dfMultDetect = dfHistoryFinalH %>%
  filter(counts>1)

dfSingleDetection = dfHistoryFinalH %>%
  filter(counts==1 & !(id %in% dfMultDetect$id))
```


## Clean Data

Add some important data to the dfHistory dataframe for further analysis
```{r}
dfHistoryFinalH = left_join(dfHistoryFinalH,dfSummarisedFinalH[,1:6], by= "combinationNumber")
```

```{r}
# filter out cases that did not run
dfEstimatesFinalH = dfEstimatesFinalHUnClean %>%
  filter(variable == "N") %>%
  filter(estimate>=0) %>%
  filter(!(id %in% dfSingleDetection$id))

dfEstimatesProbability = dfEstimatesFinalHUnClean %>% # these are probability estimates; not really needed for this project
  filter(variable == "p g1 t1") %>%
  filter(estimate>=0) %>%
  filter(!(id %in% dfSingleDetection$id))
```


```{r}
#add p,alpha,N, etc to help with graphing
dfSummarisedFinalH = dfSummarisedFinalH[!(duplicated(dfSummarisedFinalH$combinationNumber)),]
dfEstimatesFinalH = left_join(dfEstimatesFinalH, dfSummarisedFinalH[,1:6], by= "combinationNumber")
dfEstimatesProbability  =  left_join(dfEstimatesProbability, dfSummarisedFinalH[,1:6], by= "combinationNumber")
```


# Otis Closure Test Alpha = 0

Test cases to verify this is working correctly


```{r}
# function to detect time of last occurrence
# bFirst = TRUE means detect the first occurence of capture
detectMin = function(row, bFirst = T){
  # split count history into vector 
  str_ch = strsplit(row[3],"")$ch # make this more dynamic instead of hardcoding row[3]
  
  # grab first occurence of capture history 
  if (bFirst == TRUE){
    val = match(TRUE, str_ch == "1")
  } else { # grab last occurence of capture history 
    if(row[1]=="=1"){ # row[1] is the bCounts column
      val = 0
  } else {
    revRow= rev(str_ch) # reverse the row (last to first)
    nColumns= length(revRow)
    val = nColumns - match(TRUE, revRow=="1")+1 # find the new "first" match (even though it is the last match)
    } 
  }
   
  return(val)
}
```


```{r}
testGreaterOne = which(dfHistoryFinalH$bCounts==">1" & dfHistoryFinalH$combinationNumber==80)
# test cases
dfHistoryFinalH[testGreaterOne[1:5],c("bCounts","counts","ch")]
apply(dfHistoryFinalH[testGreaterOne[1:5],c("bCounts","counts","ch")],1,detectMin, bFirst = FALSE)
#apply(dfHistoryFinalH[testGreaterOne[1:5],c("bCounts","counts","ch")],1,detectMin, bFirst = FALSE)

```

I think we should only focus on animals which have been detected > 1 times anyways - that's what the scope of the test is for. But then if that is the case... isn't the point of doing the test to detect movement a little invalid? We have a large number of individuals being miscounted, so the capture history matrix is sparse and we won't have as many individuals to apply this too. The test seems a little ineffective, especially for short time intervals. 
```{r}
dfCopy = copy(dfHistoryFinalH)
otisDetectClosure = function(df = dfCopy[,c("bCounts","counts","ch","combinationNumber")], nID = NULL){
  # store original dataset in case
  dfBackup = copy(df)
  
  # id is an optional argument to test specific simulations - used for checking validity of calculations
  if(!is.null(nID)){
    df = df %>%
      filter(id %in% nID)
  }
  
  
  # store distinct number of individuals captured for the combination number or simulation (simulation by id is optional)
  #nDistinctIndividuals = nrow(df)
  
  
  
  # filter so that counts > 1 - or else the test doesn't really apply
  df = df%>%
    filter(counts>1)
  
  # handle case where *everyone* is captured only once 
  # use original dataset, but make sure wi/vi/qi are all a weird number
  if(nrow(df)==0){
      df = dfBackup
      vi = 100
      wi = 100
      qi = 100
  } else {
      vi = apply(df,1,detectMin, bFirst=TRUE) # occasion of first capture
      wi = apply(df,1,detectMin, bFirst = FALSE) # wi is the occasion of last capture 
    }
  
  df$vi = as.numeric(vi)
  df$wi = as.numeric(wi)
  df$qi= df$wi - df$vi # waiting time between first and last capture 
  
  # get unique number of capture scenarios
  lstK = unique(df$counts)
  lstCombinationNumber = unique(df$combinationNumber)
  
  # dataframe to store components of the test statistic
  dfComponents =  data.frame() # store estimates
  dfComponents = expand.grid(lstCombinationNumber, lstK)
  colnames(dfComponents)= c("combinationNumber","k")
  nDfComponent = nrow(dfComponents) # get the number of combos for later use 
  
  for(nCombo in lstCombinationNumber){ # should I filter by combination number or simulation number??
    for (k in lstK){
      
      # subset data
      if(!is.null(nID)){
        dfK = df %>%
          filter(id %in% nID & counts == k)
      } else {
       dfK = df%>%
        filter(combinationNumber == nCombo & counts == k) 
      }
      
      # get time interval
      timeInterval = nchar(dfK[1,"ch"])
      fk = nrow(dfK) # number of individuals counted k times
      # get conditional expectation
      EQi = ((k-1)*(timeInterval+1))/(k+1)
      
      print(dfK)
      
      # if no individuals exist that have been counted k times, the test statistic can't be calculated
      # for that scenario
      if(fk==0){
        EQ = NA
        VarQi = NA
        Ck= NA
      } else {
        # calculate test statistic
        EQ = sum(dfK$qi)/fk
        VarQi = (2*(timeInterval-k)*(k-1)*(timeInterval+1))/((k+2)*((k+1)^2)*fk )
        Ck = (EQ-EQi)/sqrt(VarQi) 
      }
      
      # add components to dataframe
      dfComponents[dfComponents$combinationNumber == nCombo & dfComponents$k == k,"distinctIndividuals"] = nDistinctIndividuals
      dfComponents[dfComponents$combinationNumber == nCombo & dfComponents$k == k,"fk"] = fk
      dfComponents[dfComponents$combinationNumber == nCombo & dfComponents$k == k,"EQi"] = EQi
      dfComponents[dfComponents$combinationNumber == nCombo & dfComponents$k == k,"VarQi"] = VarQi
      dfComponents[dfComponents$combinationNumber == nCombo & dfComponents$k == k,"EQ"] = EQ
      dfComponents[dfComponents$combinationNumber == nCombo & dfComponents$k == k,"Ck"] = Ck
      dfComponents[dfComponents$combinationNumber == nCombo & dfComponents$k == k,"avgVi"] = mean(dfK$vi)
      dfComponents[dfComponents$combinationNumber == nCombo & dfComponents$k == k,"avgWi"] = mean(dfK$wi)
      dfComponents[dfComponents$combinationNumber == nCombo & dfComponents$k == k,"avgQi"] = mean(dfK$qi)
      
    }
    
    # find the overall test statistic for a given combination number 
    dfComponents[dfComponents$combinationNumber == nCombo ,"C"] = sum(dfComponents$EQ - dfComponents$Eqi)/sqrt(sum(dfComponents$VarQi))
    print(dfComponents)
  }
  
  
  
  
}
```


```{r}
otisDetectClosure(dfCopy[,c("bCounts","counts","ch","combinationNumber","id")],c("80_1","1_30"))
```


```{r}
dfCopy%>%
  filter(id == "1_80")
```

```{r}
lvar=unique(dfCopy$id)
```


```{r}
params
```

```{r}
head(dfHistoryFinalH)
```


