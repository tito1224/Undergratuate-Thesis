---
title: "Final"
author: "Tito"
date: "2023-02-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Results


```{r}
# in case issues with installation of gExtras:
# https://stackoverflow.com/questions/42807247/installing-package-cannot-open-file-permission-denied
# https://stackoverflow.com/questions/60451908/tidyverse-not-loaded-it-says-namespace-vctrs-0-2-0-is-already-loaded-but

library(tidyverse)
library(gtools)
library(data.table)
library(matrixStats)
library(RMark)
library(stringi)
library(rlang)
library(gt)
library(gtExtras)
library(webshot2)
```


```{r}
setwd("~/Rcode/Undergratuate-Thesis")
source("./Functions/generatePointCount.R")
source("./Functions/runSimulationFunctions.R")
```



```{r}
#setwd("~/Rcode/Undergratuate-Thesis/DigitalAllianceOutput/Trial Five")
setwd("~/Rcode/Undergratuate-Thesis/DigitalAllianceOutput/Final")
dataFinalTrial = list.files(pattern = ".rds")
dataFinalTrial = mixedsort(dataFinalTrial)%>% # use mixedsort so it appears in ascending order 
  map(readRDS)

dfSummarisedFinal= data.frame()
dfEstimatesFinal = data.frame()
dfHistoryFinal = data.frame()
#counter = 1
for (item in dataFinalTrial){
  tempSummarisedResults = item[[1]] # retrieve the summary stats
  tempCalculations = item[[2]]
  tempHistory= item[[3]]
  #tempSummarisedResults$id = counter
  #tempCalculations$id = counter
  #tempHistory$id = counter
  
  
  dfSummarisedFinal = rbind(dfSummarisedFinal,tempSummarisedResults)
  dfEstimatesFinal = rbind(dfEstimatesFinal, tempCalculations)
  dfHistoryFinal = rbind(dfHistoryFinal, tempHistory)
  #counter = counter + 1
}
```


```{r}
#setwd("~/Rcode/Undergratuate-Thesis/DigitalAllianceOutput/Trial Five")
setwd("~/Rcode/Undergratuate-Thesis/DigitalAllianceOutput/Final2")
dataFinalTrial2 = list.files(pattern = ".rds")
dataFinalTrial2 = mixedsort(dataFinalTrial2)%>% # use mixedsort so it appears in ascending order 
  map(readRDS)

for (item in dataFinalTrial2){
  tempSummarisedResults = item[[1]] # retrieve the summary stats
  tempCalculations = item[[2]]
  tempHistory= item[[3]]
  tempCalculations$simulationNumber = tempCalculations$simulationNumber+500 # need to update simulation number since i'm stacking these numbers
  tempHistory$simulationNumber = tempHistory$simulationNumber+ 500
  
  
  dfSummarisedFinal = rbind(dfSummarisedFinal,tempSummarisedResults)
  dfEstimatesFinal = rbind(dfEstimatesFinal, tempCalculations)
  dfHistoryFinal = rbind(dfHistoryFinal, tempHistory)
  #counter = counter + 1
}
```

I ran some extra cases where alpha = 0, so I need to add and update
```{r}
setwd("~/Rcode/Undergratuate-Thesis/DigitalAllianceOutput/Final3")
dataFinalTrial3 = list.files(pattern = ".rds")
dataFinalTrial3 = mixedsort(dataFinalTrial3)%>% # use mixedsort so it appears in ascending order 
  map(readRDS)

for (item in dataFinalTrial3){
  tempSummarisedResults = item[[1]] # retrieve the summary stats
  tempCalculations = item[[2]]
  tempHistory= item[[3]]
  tempCalculations$combinationNumber = tempCalculations$combinationNumber+80 # need to update combination number since i'm stacking these numbers and this is a new scenario since alpha = 0
  tempHistory$combinationNumber = tempHistory$combinationNumber+ 80
  tempSummarisedResults$combinationNumber = tempSummarisedResults$combinationNumber+80
  
  
  dfSummarisedFinal = rbind(dfSummarisedFinal,tempSummarisedResults)
  dfEstimatesFinal = rbind(dfEstimatesFinal, tempCalculations)
  dfHistoryFinal = rbind(dfHistoryFinal, tempHistory)
  #counter = counter + 1
}
```


# Outliers

Find outlier values
```{r}
# id is combinationNumber_simulationNumber
dfEstimatesFinal$rowid = 1:nrow(dfEstimatesFinal)
dfEstimatesFinal$id = paste0(dfEstimatesFinal$combinationNumber,"_",dfEstimatesFinal$simulationNumber)
dfHistoryFinal$id = paste0(dfHistoryFinal$combinationNumber,"_",dfHistoryFinal$simulationNumber)

dfEstimatesFinalUnClean = copy(dfEstimatesFinal)
```

16 cases did not run because no detections were made, which is fair.
```{r}
dfOutliers = dfEstimatesFinalUnClean %>%
  filter(variable == "N") %>%
  filter(estimate < 0 | is.na(estimate))
nOutliers = nrow(dfOutliers)
nEstimates = nrow(dfEstimatesFinalUnClean[which(dfEstimatesFinalUnClean$variable=="N"),])
print(nOutliers)
print(nEstimates)
print(nOutliers/nEstimates)
dfOutliers
```

```{r}
dfSummarisedFinal%>%
  filter(combinationNumber %in% dfOutliers$combinationNumber)
```


## Find Single Detections

This tells us that in 94% of the scenarios generated, at least one individual was detected more than once. 
```{r}
colnamesUse= paste0("Minute",1:10)
dfHistoryFinal$counts = rowSums(dfHistoryFinal[,colnamesUse],na.rm=TRUE)
dfHistoryFinal$bCounts = ifelse(dfHistoryFinal$counts > 1,">1","=1")

# find out for each id (scenario_simulationNumber) how many individuals were counted only once and how many were recaptured
nMultDetect = dfHistoryFinal %>%
  filter(counts>1)
nMultDetect = length(unique(nMultDetect$id))
nMultDetect
nMultDetect/nEstimates
```

Doing this just to make sure I'm filtering data right
```{r}
nMultDetect = dfHistoryFinal %>%
  filter(counts>1)

nSingleDetection = dfHistoryFinal %>%
  filter(counts==1 & !(id %in% nMultDetect$id))

nSingleDetection = length(unique(nSingleDetection$id))
nSingleDetection
nSingleDetection/nEstimates
```

```{r}
dfMultDetect = dfHistoryFinal %>%
  filter(counts>1)

dfSingleDetection = dfHistoryFinal %>%
  filter(counts==1 & !(id %in% dfMultDetect$id))
```

## Likelihood 

Generate likelihood for a scenario where 3 individuals are captured only once across 5 capture occasions with a population of N = 10, to demonstrate why single detections are bad.

```{r}
ll_func = function(p){
  #val = log(factorial(10)/factorial(7)) + 3*log(p) + 47*log(1-p)
  val = (factorial(10)/factorial(7))*(p**3)*((1-p)**47)
  return(log(val))
}

ll_data = tibble(p = seq(0.1,1,0.001),
                 log_likelihood = ll_func(p))

ggplot(ll_data)+
  geom_line(aes(x= p, y = log_likelihood))+ 
  theme_bw()+ 
  labs(title = "Log-Likelihood for 3 Individuals Detected Only Once", subtitle = "N = 10, Capture Occasions = 5", x= "p", y = "Log-Likelihood")
```



```{r}
head(dfEstimatesFinal%>%
  filter(estimate > 10 & estimate < 20 & trueValues == 10))
dfHistoryFinal%>%
  filter(id == "1_8")%>%
  group_by(ch)%>%
  summarise(n())
```

Nice to see shape of the likelihood function for id 1_8
```{r}
ll_func = function(N,M,p,X_w,t){
  #X_w represents number of times different detection histories happened
  # vector of counts
  val = (factorial(N)/factorial(N-M) * product(factorial(X_w)) )*(p**M)*((1-p)**((N*t)-M))
  #return(log(val))
  return(val)
}

ll_data = tibble(N = seq(1,40,0.1),
                 log_likelihood = ll_func(N=N,M=3,p=0.1,X_w=c(6,2,2),t=5))

ggplot(ll_data)+
  geom_line(aes(x= N, y = log_likelihood))+ 
  theme_bw()+ 
  labs(title = "Log-Likelihood for 3 Individuals Detected Only Once", subtitle = "N = 10, Capture Occasions = 5", x= "N", y = "Log-Likelihood")
```





## Incorrect Estimates

Find values where unique encounter histories in given by mark does not match unique encounter histories in data
```{r}
dfMisMatch = dfEstimatesFinalUnClean %>%
  filter(variable == "N") %>%
  filter(MarkEncounters != DataEncounters)
nMisMatch = nrow(dfMisMatch)
print(nMisMatch)
print(nMisMatch/nEstimates)
```


See if I can find incorrect estimates
```{r}
dfMisMatch$Difference = dfMisMatch$MarkEncounters - dfMisMatch$DataEncounters
head(dfMisMatch%>%
       filter(!is.na(estimate))%>%
       arrange(desc(Difference)))%>%
  select(trueValues, variable, estimate, MarkEncounters, DataEncounters, combinationNumber, simulationNumber, Difference)
```


```{r}
test = dfHistoryFinal %>%
  filter(combinationNumber==51 & simulationNumber == 337) 
test
nrow(unique(test[,"ch"])) # unique encounter histories
ch = test[,"ch"]
fitModel(ch)
```


Figure out which cases have only one count - 32% of the mis matching cases have only one detection so we would have removed it anyways
```{r}
nSingleMisMatch = nrow(dfMisMatch %>%
  filter(id %in% dfSingleDetection$id))
print(nSingleMisMatch/nrow(dfMisMatch))

# add for filtering later 
# dfSingleMisMatch = dfMisMatch %>%
#   filter(id %in% dfSingleDetection$id)
```

Figure out which cases have more than one count
```{r}
nMultMisMatch = nrow(dfMisMatch %>%
  filter(id %in% dfMultDetect$id))
nMultMisMatch/nrow(dfMisMatch)

dfMultMisMatch = dfMisMatch %>%
  filter(id %in% dfMultDetect$id)
head(dfMultMisMatch)
```

```{r}
dfEstimatesFinalUnClean%>%
  filter(combinationNumber==1 & simulationNumber == 22)
test = dfHistoryFinal %>%
  filter(combinationNumber==1 & simulationNumber == 22) 
test
nrow(unique(test[,"ch"]))
ch = test[,"ch"]
fitModel(ch)
```


## Exploratory Analysis

```{r}
dfHistoryFinal = left_join(dfHistoryFinal,dfSummarisedFinal[,1:6], by= "combinationNumber")
```


```{r}
head(dfHistoryFinal)
```



```{r}
dfPointCounts = dfHistoryFinal%>%
  group_by(p_1m,alpha,N,maxMinute,id)%>%
  summarise(PointCounts = n()) # find number of people counted at least once
head(dfPointCounts%>%
       arrange(desc(PointCounts)))
```


```{r}
dfPointCounts%>%
  filter(N == 20)%>%
ggplot()+
  geom_boxplot(aes(x=as.factor(p_1m), y = PointCounts, fill = as.factor(alpha)))+
  geom_hline(aes(yintercept = 20),color="darkblue",linetype="dotted")+
  facet_wrap(~maxMinute)+
  theme_bw()+
  labs(title = "Number of Unique Individuals Counted", subtitle = "Facetted by Capture Ocassions. Population N = 20.", x= "Per Minute Detection Probability", y = "Point Counts")+ 
  scale_fill_discrete(name = "Alpha")
```



```{r}
dfPointCounts%>%
  filter(N == 10)%>%
ggplot()+
  geom_boxplot(aes(x=as.factor(p_1m), y = PointCounts, fill = as.factor(alpha)))+
  geom_hline(aes(yintercept = 10),color="darkblue",linetype="dotted")+
  facet_wrap(~maxMinute)+
  theme_bw()+
  labs(title = "Number of Unique Individuals Counted", subtitle = "Facetted by Capture Ocassions. Population N = 10.", x= "Per Minute Detection Probability", y = "Point Counts")+ 
  scale_fill_discrete(name = "Alpha")
```

# Clean Data

```{r}
# filter out cases that did not run
# dfEstimatesFinal = dfEstimatesFinal %>%
#   filter(estimate>=0 & estimate < 200 & !is.na(estimate))

# filter out mismatching unique encounter histories
dfEstimatesFinal = dfEstimatesFinalUnClean %>%
  filter(variable == "N") %>%
  filter(estimate>=0) %>%
  filter(!(id %in% dfSingleDetection$id))#%>%
  #filter(!(id %in% dfMisMatch$id))

dfEstimatesProbability = dfEstimatesFinalUnClean %>%
  filter(variable == "p g1 t1") %>%
  filter(estimate>=0) %>%
  filter(!(id %in% dfSingleDetection$id))
```


Total outliers make up appx 10% of the data if we include mis matches and 6% of the data if we do not include mismatches
```{r}
1 - (nrow(dfEstimatesFinal)/nEstimates)
```

```{r}
#add p,alpha,N, etc
dfSummarisedFinal = dfSummarisedFinal[!(duplicated(dfSummarisedFinal$combinationNumber)),]
dfEstimatesFinal = left_join(dfEstimatesFinal, dfSummarisedFinal[,1:6], by= "combinationNumber")
dfEstimatesProbability  =  left_join(dfEstimatesProbability, dfSummarisedFinal[,1:6], by= "combinationNumber")
```


```{r}
head(dfEstimatesFinal)
```


Make sure there are approximately equal data points across all scenarios. The scenarios with less than 500 data points by a significant amount are the ones with lowest detection probability and high alpha and/or low N. Makes sense because we end up with more cases of single detections here. 
```{r}
dfEstimatesFinal %>%
  filter(variable == "N")%>%
  group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
  summarise(Count= n())%>%
  arrange(Count)
```


# Plots

## Population 20
```{r}
dfEstimatesPop = dfEstimatesFinal %>%
  filter(N==20)
```

Make sure they all have around the same data points - 600+ data points per scenario.
```{r}
dfEstimatesPop %>%
  filter(variable == "N")%>%
  group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
  summarise(Count= n())%>%
  arrange(Count)

```




Looking at how estimates varies as alpha increases when p_1m is fixed and time is fixed.

- When the time interval is 5 mins, and detection probability is low, our estimates are closer to the true value in general; Even when we have a higher alpha, the shift in median estimate isn't as pronounced compared to when time = 10. This makes sense as small occasions means we have less time to make more mistakes, and small detection probability means we have detected less individuals to make mistakes on. 

- when alpha is 0.05, and time = 5, we have pretty accurate results, no matter how high p1m is. 

- With both time = 5 and time = 10, I notice that the IQR when alpha is high becomes smaller. **Why should this be the case?**

- When time = 5, not much of a difference in median estimate for alpha =0.2 when p is 0.2 vs when p = 0.3. The marked difference is in the spread of values

- When time = 10, why are the worst estimates when p = 0.1 and alpha= 0.2? Shouldn't the worst estimates be when p = 0.3 and alpha is 0.2?
  - Actually because we have a small p and high alpha, we have a higher occurence of more single detections, which lower our detection probability and increase the estimate of N. This is why we see more outliers for this scenario. When detection probability increases, then we that IQR decrease

- Another interesting observation is that when time = 10, the median estimate is the same for each value of alpha, no matter what the p value is. But once again, that IQR tightens up as the p increases
- a small probability of errors already skyrockets the estimates --> even when we only have five capture occasions.


- once we reach alpha = 10% for time = 10, no matter what p_1m is, median estimate is basically the same -> only change is that alpha becomes smaller...
```{r}
dfEstimatesPop %>%
ggplot()+
  geom_boxplot(aes(x=as.factor(p_1m), y = estimate, fill = as.factor(alpha)))+ 
  geom_hline(aes(yintercept = trueValues),color = "darkblue",linetype="dotted")+
  labs(title = "Population Estimates Facetted by Time", subtitle = "20 Individuals in Population. 5 and 10 Minute Facet Times.")+
  facet_wrap(~maxMinute)+ 
  scale_fill_discrete(name = "Alpha")+ 
  labs(x= "Per Minute Detection Probability",y = "Estimate")+
  theme_bw()
```



```{r}
dfEstimatesFinal %>%
  filter(N==10) %>%
ggplot()+
  geom_boxplot(aes(x=as.factor(p_1m), y = estimate, fill = as.factor(alpha)))+ 
  geom_hline(aes(yintercept = trueValues),color = "darkblue",linetype="dotted")+
  labs(title = "Population Estimates Facetted by Time", subtitle = "10 Individuals in Population. 5 and 10 Minute Facet Times.")+
  facet_wrap(~maxMinute)+ 
  scale_fill_discrete(name = "Alpha")+ 
  labs(x= "Per Minute Detection Probability",y = "Estimate")+
  theme_bw()
```


How many individuals are detected, once, twice, etc for N = 20 and maxMinute = 10.

```{r}
dfCounts = dfHistoryFinal %>%
  filter(N==20 & maxMinute == 10 & p_1m != 0.3)%>%
  group_by(p_1m,alpha,maxMinute,counts)%>%
  summarise(c =n())%>%
  mutate(total = sum(c),
         percent = round(c/total*100,2))
head(dfCounts)
```

```{r}
ggplot(dfCounts,aes(x=counts, y = percent, color = as.factor(alpha)))+ 
  #geom_point()+
  geom_line()+
  facet_wrap(~p_1m)+
  theme_bw()+ 
  scale_color_discrete(name="Alpha")+
  labs(title = "Distribution of Number of Detections Across Capture Histories", subtitle= "Facetted by Detection Probability. Capture Occasion = 10.", x = "Detections per Individual",y = "% of Individuals Detected ")
```


```{r}
dfCounts = dfHistoryFinal %>%
  filter(N==20 & maxMinute == 5 & p_1m != 0.3)%>%
  group_by(p_1m,alpha,maxMinute,counts)%>%
  summarise(c =n())%>%
  mutate(total = sum(c),
         percent = round(c/total*100,2))

ggplot(dfCounts,aes(x=counts, y = percent, color = as.factor(alpha)))+ 
  geom_line()+
  facet_wrap(~p_1m)+
  theme_bw()+ 
  scale_color_discrete(name="Alpha")+
  labs(title = "Distribution of Number of Detections Across Capture Histories", subtitle= "Facetted by Detection Probability. Capture Occasion = 5.", x = "Detections per Individual",y = "% of Individuals Detected ")
```


```{r}
dfCounts = dfHistoryFinal %>%
  filter(N==20 & maxMinute == 5 & p_1m != 0.3)%>%
  group_by(p_1m,alpha,maxMinute,counts)%>%
  summarise(c = n())%>% # of times someone was counted once, twice, etc
  mutate(total = sum(c))
```


Distribution of detection probabilities
```{r}
dfEstimatesProbability%>%
  filter(alpha == 0.3 & maxMinute == 10)%>%
  ggplot()+ 
  geom_density(aes(x=estimate, color = as.factor(p_1m)))+ 
  theme_bw()
 
```




```{r}
dfCompareEstimates = dfEstimatesFinal%>%
  filter(alpha==0.3  & p_1m %in% c(0.1,0.5) & N == 20)%>%
  group_by(maxMinute,p_1m)%>%
  summarise(`Average Estimate` = round(mean(estimate),1),
            `Standard Deviation`=round(sd(estimate),1))%>%
  rename(`Capture Occasions` = maxMinute)
dfCompareEstimates
# dfCompareEstimates = dfCompareEstimates%>%
#   gt()%>%
#   gt_theme_nytimes()
# gtsave(dfCompareEstimates, "compareEstimates.png")
```


# Tables


```{r}
# boolean for if a value is in the confidence interval or not
dfEstimatesFinal$bCoverage = ifelse(dfEstimatesFinal$trueValues >= dfEstimatesFinal$lcl & dfEstimatesFinal$trueValues <= dfEstimatesFinal$ucl,1,0)

# find cases where bCoverage = 0, and check to make sure it's not a tolerance issue; if trueValue = lcl or ucl, change bCoverage boolean to 1
lclTrue = mapply(function(x, y) {isTRUE(all.equal(x, y))}, dfEstimatesFinal$trueValues, dfEstimatesFinal$lcl)
uclTrue = mapply(function(x, y) {isTRUE(all.equal(x, y))}, dfEstimatesFinal$trueValues, dfEstimatesFinal$ucl)
dfEstimatesFinal[which(lclTrue),"bCoverage"] = 1
dfEstimatesFinal[which(uclTrue),"bCoverage"] = 1
```

```{r}

dfEstimatesFinal$squaredError = (dfEstimatesFinal$estimate - dfEstimatesFinal$trueValues)^2
dfEstimatesFinal$width = dfEstimatesFinal$ucl - dfEstimatesFinal$lcl
  
dfSummaryStats = dfEstimatesFinal %>%
    filter(variable == "N")%>% # only focus on N? or also use p??
    group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
    summarise(AvgNhat = mean(estimate,na.rm=TRUE),
              medianNhat= median(estimate,na.rm=TRUE),
              AvgNhatSE= mean(se,na.rm=TRUE),
              sdNhat = sd(estimate,na.rm=TRUE),
              biasNhat = AvgNhat - N,
              mse = mean(squaredError,na.rm=TRUE),
              bias_SE_Nhat = AvgNhatSE - sdNhat,
              coverage = sum(bCoverage,na.rm=TRUE)/sum(!is.na(estimate)), # changed this from n() to sum(!is.na()) because I have some NA values for my estimates
              avgWidth = mean(width,na.rm=TRUE),
              avgAIC = mean(AIC,na.rm=TRUE),
              relNhatBias = round(biasNhat/N*100,2),
              avgUCL= mean(ucl,na.rm=TRUE),
              avgLCL = mean(lcl, na.rm=TRUE))
dfSummaryStats = dfSummaryStats[!duplicated(dfSummaryStats),]
dfSummaryStats
```




## Plots on Table


The order of the average n values is as expected - as alpha increases, estimates of n should increase. I'm curious about the trend though. Namely, shouldn't the average n always be increasing as we increase p? Or at least kind of stabilize. When time = 5 I see this, but when time = 10, I'm confused about it. 

- **Can I attribute these downward movements to just randomized data points**
```{r}
dfSummaryStats %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()+ 
  labs(title  = "Average Population Estimates", subtitle = "N = 20",x= "Per Minute Detection Probability",y = "Average N Estimate")+
  scale_color_discrete(name = "Alpha")
```



```{r}
# dfSummaryStats %>%
#   filter(N==20 & maxMinute == 10)%>%
# ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
#   geom_point()+
#   geom_line()+
#   geom_ribbon(aes(ymin = AvgNhat - AvgNhatSE, ymax = AvgNhat + AvgNhatSE),alpha=0.1)+
#   facet_wrap(~as.factor(alpha))+
#   theme_bw()+ 
#   labs(title  = "Average Population Estimates and Standard Error", subtitle = "N = 20, 10 Capture Occasions.",x= "1 Min Detection Probability (p_1m)",y = "Average Population Estimate")+
#   scale_color_discrete(name = "Alpha")
dfSummaryStats %>%
  filter(N==20 & maxMinute == 10)%>%
ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  facet_wrap(~as.factor(alpha))+
  theme_bw()+ 
  labs(title  = "Average Population Estimates and Standard Error", subtitle = "N = 20, 10 Capture Occasions.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Alpha")
```



```{r}
dfSummaryStats %>%
  filter(N==20 & maxMinute == 5)%>%
ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  facet_wrap(~as.factor(alpha))+
  theme_bw()+ 
  labs(title  = "Average Population Estimates and Standard Error", subtitle = "N = 20, 5 Capture Occasions.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Alpha")
```


```{r}
dfSummaryStats %>%
  filter(N==10 & maxMinute == 10)%>%
ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  facet_wrap(~as.factor(alpha))+
  theme_bw()+ 
  labs(title  = "Average Population Estimates and Standard Error", subtitle = "N = 10, 10 Capture Occasions.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Alpha")
```


```{r}
dfSummaryStats %>%
  filter(N==10 & maxMinute == 5)%>%
ggplot(aes(x=p_1m, y = AvgNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = avgLCL, ymax = avgUCL),alpha=0.1)+
  facet_wrap(~as.factor(alpha))+
  theme_bw()+ 
  labs(title  = "Average Population Estimates and Standard Error", subtitle = "N = 10, 5 Capture Occasions.",x= "Per Minute Detection Probability",y = "Average Population Estimate")+
  scale_color_discrete(name = "Alpha")
```

```{r}
dfSummaryStats%>%
  filter(p_1m==0.2 & N==20 & maxMinute == 5)
```



```{r}
dfInteract = dfEstimatesFinal%>%
  filter(N=="20" & maxMinute==10)
interaction.plot(x.factor = dfInteract$p_1m,
                 trace.factor = dfInteract$alpha,
                 response = dfInteract$estimate)

dfInteract = dfEstimatesFinal%>%
  filter(N=="20" & maxMinute==5)
interaction.plot(x.factor = dfInteract$p_1m,
                 trace.factor = dfInteract$alpha,
                 response = dfInteract$estimate)

dfInteract = dfEstimatesFinal%>%
  filter(N=="10" & maxMinute==10)
interaction.plot(x.factor = dfInteract$p_1m,
                 trace.factor = dfInteract$alpha,
                 response = dfInteract$estimate)
dfInteract = dfEstimatesFinal%>%
  filter(N=="10" & maxMinute==5)
interaction.plot(x.factor = dfInteract$p_1m,
                 trace.factor = dfInteract$alpha,
                 response = dfInteract$estimate)
```

```{r}
head(dfEstimatesFinal)
```


```{r}
dfInteract = dfEstimatesFinal%>%
  filter(N=="20"& p_1m==0.1)
interaction.plot(x.factor = dfInteract$maxMinute,
                 trace.factor = dfInteract$DataEncounters,
                 response = dfInteract$estimate)
```


Significant interaction between capture occasions and detection probability on mean estimate. 
```{r}
dfInteract = dfEstimatesFinal%>%
  filter(N==20)
interaction.plot(x.factor = dfInteract$maxMinute,
                 trace.factor = dfInteract$p_1m,
                 response = dfInteract$estimate)
```


```{r}
dfEstimatesFinal%>%
  filter(N == 20)%>%
  group_by(maxMinute, p_1m, alpha)%>%
  summarise(avgEstimate = mean(estimate,na.rm=TRUE))%>%
  ggplot()+
  geom_line(aes(x=maxMinute,y=avgEstimate,color=as.factor(p_1m)))+
  facet_wrap(~alpha,scales="free")+
  theme_bw()+
  labs(title = "Interaction Plot Between Capture Occasion and Detection Probability",subtitle = "Facetted by Alpha",x = "Number of Capture Occasions",y = "Mean Population Estimate")+
  scale_color_discrete(name = "Detection Probability")

```

All significant.
```{r}
dfAnova=dfEstimatesFinal%>%
  filter(N == 20 & alpha == 0.1)
aov1 = aov(estimate ~ as.factor(maxMinute)*as.factor(p_1m), data = dfAnova)
summary(aov1)
```


## Why does p=0.2 cause a jump then fall 

```{r}
dfSummaryStats %>%
  filter(N==20 & maxMinute == 5)%>%
ggplot(aes(x=p_1m, y = medianNhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = medianNhat - AvgNhatSE, ymax = medianNhat + AvgNhatSE),alpha=0.1)+
  facet_wrap(~as.factor(alpha))+
  theme_bw()+ 
  labs(title  = "Median Population Estimates", subtitle = "N = 20, 5 Capture Occasions.",x= "1 Min Detection Probability (p_1m)",y = "Average Population Estimate")+
  scale_color_discrete(name = "Alpha")
```

```{r}
dfHistoryFinal%>%
  filter(p_1m==0.2 & N == 20 & maxMinute==5)%>%
  group_by(id,ch)%>%
  mutate(n = n())%>%
  group_by(ch)%>%
  summarise(avg = mean(n))
```

Obviously increasing M increases estimate
```{r}
ll_data = tibble(M = seq(1,10,0.01),
                 log_likelihood = ll_func(N=20,M=M,p=0.2,X_w= c(2,2,6),t=5))

ggplot(ll_data)+
  geom_line(aes(x= M, y = log_likelihood))+ 
  theme_bw()+ 
  labs(title = "Log-Likelihood for 3 Individuals Detected Only Once", subtitle = "N = 10, Capture Occasions = 5", x= "N", y = "Log-Likelihood")
```

```{r}
ll_func = function(N,M,p,X_w,t,n){
  #X_w represents number of times different detection histories happened
  # vector of counts
  # n represents what n. in the paper is; max is M_{t+1} * t
  val = (factorial(N)/factorial(N-M) * product(factorial(X_w)) )*(p**n)*((1-p)**((N*t)-n))
  #return(log(val))
  return(val)
}

n = c(1,20,40)
ll_data = tibble(N = seq(1,30,0.1))%>%
  crossing(n)%>%
  mutate(log_likelihood = ll_func(N=N,M=6,p=0.2,X_w=c(6,2,2),t=5,n=10))

ggplot(ll_data)+
  geom_line(aes(x= N, y = log_likelihood, color = as.factor(n)))+ 
  theme_bw()+ 
  labs(title = "Log-Likelihood for 3 Individuals Detected Only Once", subtitle = "N = 10, Capture Occasions = 5", x= "N", y = "Log-Likelihood")+ 
  facet_wrap(~n)
```


```{r}
M = 1:10
ll_data = tibble(N = seq(1,30,0.1))%>%
  crossing(M)%>%
  mutate(log_likelihood = ll_func(N=N,M=M,p=0.2,X_w=c(6,2,2,4,4,6),t=5,n=10))

ggplot(ll_data)+
  geom_line(aes(x= N, y = log_likelihood ))+ 
  theme_bw()+ 
  labs(title = "Log-Likelihood for 3 Individuals Detected Only Once", subtitle = "N = 10, Capture Occasions = 5", x= "N", y = "Log-Likelihood")+
  facet_wrap(~M,scales = "free")
```

Chaning Xw changes scale, but not the shape of the likelihood!
```{r}
M = 1:10
ll_data = tibble(N = seq(1,30,0.1))%>%
  crossing(M)%>%
  mutate(log_likelihood = ll_func(N=N,M=M,p=0.2,X_w=c(6,2,2),t=5,n=10))

ggplot(ll_data)+
  geom_line(aes(x= N, y = log_likelihood ))+ 
  theme_bw()+ 
  labs(title = "Log-Likelihood for 3 Individuals Detected Only Once", subtitle = "N = 10, Capture Occasions = 5", x= "N", y = "Log-Likelihood")+
  facet_wrap(~M,scales = "free")
```
Looks like what is really changing the likelihood is unique detections/individuals M_t+1. Probs due to the (N-M_{t+1})! part in the likelihood that we would need to take the derivative of. I think interaction between M_{t+1} and 


```{r}
dfEstimatesFinal%>%
  filter(N=="20" & p_1m ==0.1)%>%
ggplot()+
  geom_point(aes(x=DataEncounters, y = estimate, color = as.factor(alpha)))+
  facet_wrap(~maxMinute)
```


## Why SE decrease

Why does SE decrease as p_1m increases? Because we have more data points to work with? Is that also why it decreases so dramatically when alpha = 0.3? Combo of more true individuals detected + more new individuals added to the sample

```{r}
dfSummaryStats %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = AvgNhatSE, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()+ 
  labs(title  = "Standard Error of Population Estimates", subtitle = "N = 20",x= "1 Min Detection Probability (p_1m)",y = "Average SE of Estimate")+
  scale_color_discrete(name = "Alpha")
```

```{r}
dfSummaryStats %>%
  filter(N==10)%>%
ggplot(aes(x=p_1m, y = AvgNhatSE, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()+ 
  labs(title  = "Standard Error of Population Estimates", subtitle = "N = 10",x= "1 Min Detection Probability (p_1m)",y = "Average SE of Estimate")+
  scale_color_discrete(name = "Alpha")
```
## mse

It makes sense that MSE is largest when alpha is largest because we are introducing more individuals into the data than there should be. As probability of detection increases, for a fixed alpha, I assumed mse should be increasing. This is because we are detecting more individuals, which means more opportunities for us to miscount. 

- **So if this is the case, for when alpha = 0.3, why is mse decreasing?** 
```{r}
dfSummaryStats %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = mse, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()+ 
  labs(title  = "Mean Squared Error of Population Estimates", subtitle = "Facetted by Capture Occasions. N = 20. ",x= "Per Minute Detection Probability",y = "Average MSE of Estimates")+
  scale_color_discrete(name = "Alpha")
```
```{r}
dfSummaryStats %>%
  filter(N==20 & p_1m %in% c(0.1,0.5)&alpha==0.05)
```

```{r}
dfSummaryStats %>%
  filter(N==10)%>%
ggplot(aes(x=p_1m, y = mse, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()+ 
  labs(title  = "Mean Squared Error of Population Estimates", subtitle = "Facetted by Capture Occasions. N = 10. ",x= "Per Minute Detection Probability",y = "Average MSE of Estimates")+
  scale_color_discrete(name = "Alpha")
```


## Coverage


Coverage for when min= 5 is as expected; lower coverage as alpha increases due to miscounting. What is interesting however is that even for a small alpha value, given a high enough detection probability, the coverage will go to 0. When time = 10, the rate at which coverage drops to 0 is much faster. 
```{r}
dfSummaryStats %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = coverage, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_light()+ 
  labs(title  = "Coverage Probability", subtitle = "Facetted by Capture Occasions. N = 20.", x= "Per Minute Detection Probability",y = "Coverage Probability")+
  scale_color_discrete(name = "Alpha")+
  theme_bw()
```

to verify that cov probabilities make sense, note that cov probability when p = 0.5 and time = 5 is equivalent to cov probability when p = 0.3 andtime = 10. check prob of only being detected once to confirm that they have the same prob of being detected once?


```{r}
dfSummaryStats %>%
  filter(N==10)%>%
ggplot(aes(x=p_1m, y = coverage, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_light()+ 
  labs(title  = "Coverage Probability", subtitle = "Facetted by Capture Occasions. N = 10.", x= "Per Minute Detection Probability",y = "Coverage Probability")+
  scale_color_discrete(name = "Alpha")+
  theme_bw()
```


### Check Coverage

```{r}
dfEstimatesFinal%>%
  filter(variable == "N")%>% # only focus on N? or also use p??
  group_by(p_1m,alpha,N,maxMinute,combinationNumber)%>%
  summarise(c = sum(!is.na(estimate)),
            cv = sum(bCoverage))

testCov = dfEstimatesFinal%>%
  filter(combinationNumber==99)
testCov
```


```{r}
# i was having issues with testing if the floating point values are the same
## https://stackoverflow.com/questions/9508518/why-are-these-numbers-not-equal 
testCov$trueValues[10]>=testCov$lcl[10]
all.equal(testCov$trueValues[10],testCov$lcl[10])
```


```{r}
dfSummaryStats %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = bias_SE_Nhat, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_bw()
```

```{r}
dfSummaryStats %>%
  filter(N==20)%>%
ggplot(aes(x=p_1m, y = relNhatBias, color = as.factor(alpha)))+ 
  geom_point()+
  geom_line()+
  facet_wrap(~maxMinute)+
  theme_light()+ 
  labs(title  = "Relative Bias", subtitle = "Facetted by Capture Occasions. N = 20.", x= "1 Min Detection Probability (p_1m)",y = "Relative Bias %")+
  scale_color_discrete(name = "Alpha")+
  theme_bw()
```

